{"chunks": [{"id": 0, "text": "\u2022\nThe emergence of early computing technologies mainly consisted of mainframes \nand terminals.\n\u2022\nWith the advent of personal computers (PCs) in the 1980s, the second platform \nemerged, which was characterized by the client/server system, Ethernet, RDBMS, \nand Web applications.\n\u2022\nToday we are using the third platform, which includes cloud computing, big data, \nmobile devices, and socialization technologies. At the core of these technologies \nis cloud computing. Customers use cloud providers' services to allocate IT \nresources. Big data turbocharges data analysis to achieve in-depth insights and \nfor leaders' to make better-informed decisions. Mobile devices enable ubiquitous \naccess to applications and information. Socialization technologies help connect \npeople and ensure better collaboration and information exchanges.\n\u2022\nFor more details, see https://en.wikipedia.org/wiki/Third_platform.\n"}, {"id": 1, "text": "\u2022\nThe cloud platform provides enterprises with a new choice in adapting their IT \narchitectures to receive growing volumes of data and business.\n"}, {"id": 2, "text": "\u2022\nData management:\n\u25ab\nTraditional data management involves relational data and transactions but \nconcurrent analysis throughput is low.\n\u25ab\nModern data management adopts database and big data services to mine \nmassive service data. Intelligent analysis contributes to better operations \nand better-informed decisions for new value.\n\u2022\nInfrastructure:\n\u25ab\nThe resource utilization of physical machine-based deployment is low.\n\u25ab\nVirtualization deployment improves device utilization and simplifies O&M.\n\u25ab\nResource pooling allows the management platform to integrate \nvirtualization silos into resource pools for unified management and sharing.\n\u25ab\nModern infrastructure management is automated and allows for self-\nservice. Teams can collaborate and perform massive operations \nconcurrently, and IT activities are fixed, standardized, and measurable. IT \nthat used to support O&M is now oriented to operations.\n\u2022\nDevelopment:\n\u25ab\nTraditional development methods use mature and reliable technologies. \nServices are stable and developed in advance, but the process is rigid.\n\u25ab\nA modern IT architecture is constructed in a distributed manner. That is, the \narchitecture consists of microservices, and adopts DevOps and a \ndevelopment and test pipeline for quick roll-out and elastic scaling of new \nservices.\n"}, {"id": 3, "text": "\u2022\nHuawei Cloud Healthcare Intelligent Twins breathe new life into the traditional \nhealthcare industry and help improve efficiency.\n\u2022\nAgility and resource scheduling are embodied in the previous page. This page \nfocuses on cloud service enablement. That is, the new capabilities that are \noffered in services.\n"}, {"id": 4, "text": "\u2022\nThe cloud computing software industry has been a national priority since the \n12th Five-Year Plan.\n\u2022\nAccording to the 13th Five-Year Science and Technology Innovation Plan, cloud \ncomputing technologies and applications will be promoted to empower the next \ngeneration of ICT infrastructure.\n\u2022\nThe 2019 Federal Cloud Computing Strategy \u2014 Cloud Smart \u2014 is a long-term, \nhigh-level strategy to drive cloud adoption in Federal agencies.\n\u2022\nShaping Europe's Digital Future stresses the importance of cloud computing to \ndigitization.\n\u2022\nIn the Outline of the 14th Five-Year Plan for National Economic and Social \nDevelopment and Long-Range Objectives Through the Year 2035, the \ndevelopment of StatChina was propelled to new heights and cloud computing \nhas becoming key to that growth. Cloud computing software will embrace new \nopportunities.\n"}, {"id": 5, "text": "\u2022\nBuilding a cloud-based software system is very similar to building a house. If the \nfoundation is not solid, structural problems may damage the integrity and \nfunctionality of the house. When designing a solution for migrating enterprise \napplications to the cloud, if you ignore security, reliability, scalability, \nperformance, and cost optimization, it may be difficult to build a system that \nmeets your expectations and requirements. Considering the following factors in \nthe design will help you build a stable and efficient system:\n\u2022\nSecurity: System security is assessed to protect information, systems, and assets \nwhile unleashing business value.\n\u2022\nAvailability: The system recovers from infrastructure or service faults and \ndynamically obtains resources to meet requirements and reduce service \ninterruption. Single-AZ availability, cross-AZ DR, cross-AZ active-active, and \nremote DR deployment should be considered in the design.\n\u2022\nPerformance: The system uses resources to meet performance requirements, \nincluding compute, network, storage, and data.\n\u2022\nScalability: The system can be scaled out or scaled up according to the number of \nusers or overall workload.\n\u2022\nCost: Avoid or eliminate unnecessary costs or poor resources.\n"}, {"id": 6, "text": "\u2022\nSecure communication network\n\u25ab\nAnti-DDoS is used to defend against DDoS attacks.\n\u25ab\nWeb Application Firewall (WAF) is used to defend against web attacks.\n\u25ab\nSSL certificates are used for communication encryption.\n\u2022\nSecurity zone border\n\u25ab\nThe cloud firewall is deployed between Internet borders and VPCs.\n\u2022\nSecure compute environment\n\u25ab\nHost Security Service (HSS) and Container Guard Service (CGS) are \ndeployed.\n\u25ab\nNetwork ACLs and security groups are used for access control within a VPC.\n\u25ab\nData Security Center (DSC) manages data security throughout the data \nlifecycle.\n\u25ab\nData encryption is enabled for storage by default.\n\u25ab\nDatabase Security Service (DBSS) is deployed for key databases.\n\u2022\nSecurity Management Center\n\u25ab\nThe Situational Awareness (SA) service is used to ensure cloud resource \nsecurity.\n\u25ab\nCloud resources are periodically scanned to detect vulnerabilities.\n\u25ab\nLog Tank Service (LTS), Cloud Trace Service (CTS), and Cloud Eye are used \nto manage cloud resources.\n\u25ab\nCloud Bastion Host (CBH) is used for O&M.\n"}, {"id": 7, "text": "\u2022\nTenants deploy and configure security service products, including security \nconfigurations and management tasks (such as updates and security patches) of \ncloud services, such as virtual networks, virtual hosts, and guest VMs in tenant \nspace, as well as container security management. Tenants are also responsible for \nthe internal security configurations of other cloud services they lease.\n\u2022\nTenants are also responsible for the security management of any application \nsoftware or utility they deploy on Huawei Cloud. Before deploying security \nworkloads in the production environment, tenants should test these workloads to \nprevent adverse effects on their applications and services.\n\u2022\nTenants own and control their data regardless of the Huawei Cloud service they \nuse. Tenants take measures to guarantee data confidentiality, integrity, and \navailability, as well as the identity authentication and authorization for data \naccess. For example, tenants using IAM and DEW need to configure rules to \nproperly keep their own service login accounts, passwords, and keys.\n"}, {"id": 8, "text": "\u2022\nThe longest annual downtime allowed for each SLA level is calculated as follows \n(365 days in a year):\n\u25ab\n1 year = 365 days = 8760 hours\n\u25ab\n99.9 = 8760 x 0.1% = 8760 x 0.001 = 8.76 hours\n\u25ab\n99.99 = 8760 x 0.0001 = 0.876 hours = 0.876 x 60 = 52.6 minutes\n\u25ab\n99.999 = 8760 x 0.00001 = 0.0876 hours = 0.0876 x 60 = 5.26 minutes\n\u2022\nAn annual downtime of 5.26 minutes means 99.999% SLA. A better SLA means \nhigher requirements on the system. As a result, we need to consider whether the \nsystem is capable of meeting the increasing SLA requirements.\n"}, {"id": 9, "text": "\u2022\nThe common cloud system HA design solutions are as follows:\n\u25ab\nThe on-premises HA solution applies to on-premises production centers and \nsingle-AZ scenarios.\n\u25ab\nThe intra-city HA/DR solutions, including an active-active data center\nsolution and an active-passive DR solution, apply to the HA design of intra-\ncity DR centers and dual-AZ scenarios.\n\u25ab\nThe remote HA/DR solutions, including a geo-redundant DR solution and \nan active-passive DR solution, apply to remote DR centers and cross-region \nHA.\n"}, {"id": 10, "text": "\u2022 Prevention of performance bottlenecks\n\u25ab\nPerformance issues are detected and resolved in advance, such as high \nserver CPU/MEM usage, program memory leakage, network congestion of \napplication access links, insufficient database connection pools, application \nprocess suspension, and low cache hit ratio.\n\u2022 Better user experience\n\u25ab\nUser experience is improved by preventing the following problems: web \npage opening failures or slow response, video frame freezing, artifacts, \ndelayed market data update, and disconnection and frame freezing during \ngaming.\n\u2022 Appropriate resource allocation\n\u25ab\nCloud service specifications are appropriately allocated based on \nperformance indicators. Nodes are added to or removed from service \nclusters.\n"}, {"id": 11, "text": "\u2022\nThe performance of cloud applications is affected by many factors, including data \ntransmission and software and hardware. These factors make performance \nevaluation complex.\n\u2022\nCloud application performance can be affected by latency, throughput, IOPS, and \nconcurrency, as well as compute, network, storage, and database resources.\n\u2022\nCompute resources: Large-scale infrastructure is shared, resulting in resource \ncompetition. Therefore, the appropriate distribution of limited resources is \nrequired to deal with load changes.\n\u25ab\nCompute resources affect the latency of applications.\n\u2022\nNetwork resources: The public cloud infrastructure is not located in the enterprise \ndata center. As a result, the WAN must be used, which causes high bandwidth \nand latency. Multi-peer networks, encrypted offloading, and compression are \nfactors that must be considered for architecture design.\n\u25ab\nNetwork resources affect the throughput of applications.\n\u2022\nStorage resources: read and write performance of storage products with different \nperformance characteristics; unmeasurable disk I/O of elastic block storage\n\u25ab\nStorage resources affect the data transmission of applications.\n\u2022\nDatabase resources: If an application uses a database, the database resources \naffect application concurrency.\n\u2022\nThe performance of cloud infrastructure can be unpredictable. Load changes may \naffect available CPU, network, and disk I/O resources. As a result, the \nperformance of applications that work at the same time is unpr"}, {"id": 12, "text": " of cloud infrastructure can be unpredictable. Load changes may \naffect available CPU, network, and disk I/O resources. As a result, the \nperformance of applications that work at the same time is unpredictable.\n"}, {"id": 13, "text": "\u2022\nThe overall design of the architecture system is also important. For example, to \navoid remote data transmission, you can deploy resources near service sites, and \nadopt services such as CDN to reduce access latency.\n"}, {"id": 14, "text": "\u2022\nScalability is a design indicator that represents the computing and processing \ncapabilities of a software system. High scalability indicates that the system can \ncontinue to run properly as the system expands and grows. The processing \ncapabilities of the entire system can be linearly increased with only minimal \nmodifications or hardware changes. In this way, high throughput and low latency \ncan be achieved.\n\u25ab\nHorizontal scaling is a feature that allows the connection of multiple \nsoftware and hardware products. In this way, multiple servers can be \nlogically considered an entity. When a system is scaled out by adding new \nnodes with the same functions, the system can redistribute resources \naccording to the loads of all nodes. The system is scaled out by adding \nmore servers to the load balancing network so that incoming requests can \nbe distributed among all of these networks.\n\u25ab\nVertical scaling is to replace existing IT resources with new ones regardless \nof their capacity. That is, the CPU performance of the server is increased or \nreduced in place. You can add processors, main memory, storage devices, or \nnetwork interfaces to nodes to handle the increasing requests of each \nsystem. The system is scaled up by adding more processors or main \nmemory to host more virtual servers.\n\u2022\nScalability of cloud computing allows users to use more resources as the load \nincreases, and lets developers build scalable architectures. For example, \nmicroservices and containerized archi"}, {"id": 15, "text": "rvers.\n\u2022\nScalability of cloud computing allows users to use more resources as the load \nincreases, and lets developers build scalable architectures. For example, \nmicroservices and containerized architectures encourage independent scaling.\n\u2022\nLatency and throughput are a pair of indicators for scalability. An ideal system \narchitecture should deliver low latency and high throughput. Latency is the \nsystem response time that users can perceive. Shorter response time indicates \nlower latency. Throughput indicates the number of users who can perceive the \nlow latency at the same time.\n"}, {"id": 16, "text": "\u2022\nPay-per-use is preferred when service requirements fluctuate or flexible scale-out \nis needed. It is an ideal billing mode for development and test environments.\n\u2022\nYearly/Monthly is a better option when resource requirements are stable and \nresources are used for a long period of time.\n"}, {"id": 17, "text": "\u2022\nHuawei Cloud provides budget and bill functions and visualized fee management \nto help customers optimize costs.\n\u2022\nA transaction bill includes the billing information of each order and of each \nbilling cycle (a cloud service billing cycle can be hourly, daily, or monthly).\n"}, {"id": 18, "text": "\u2022\nHigh service flexibility \u2013 scalability\n\u2022\nHigh network performance \u2013 performance\n\u2022\nExcellent web experience \u2013 performance\n\u2022\nFast R&D iteration \u2013 scalability\n\u2022\nApplication DR and backup \u2013 availability\n\u2022\nEnhanced security protection \u2013 security\n\u2022\nCost is an important factor to consider when selecting a solution and plays a \ndecisive role in the profitability of an enterprise.\n"}, {"id": 19, "text": "\u2022\nThese are the key challenges that need to be addressed by e-commerce \nplatforms built on on-premises infrastructures. A good cloud-based architecture \ndesign can solve these problems.\n"}, {"id": 20, "text": "\u2022\n[Scalability] Auto Scaling (AS)\n\u2022\n[Performance] Cloud Eye\n\u2022\n[Availability] ELB provides multiple back-end ECS instances to prevent single \npoints of failure (SPOFs).\n"}, {"id": 21, "text": "\u2022\nDynamic content refers to the content obtained through asp, jsp, php, perl, and \ncgi requests, APIs, and dynamic interaction requests (such as post, put, and patch \nrequests).\n\u2022\nStatic content refers to the same content obtained through different access \nrequests, such as images, videos, and file packages on websites. CDN can provide \nacceleration services for static content under acceleration domain names.\n\u2022\nCDN cannot cache dynamic content. As a result, dynamic content cannot be \naccelerated during the acceleration of websites, file download, and on-demand \nservices. Static and dynamic content can be accelerated through whole site \nacceleration.\n"}, {"id": 22, "text": "\u2022\nHigh-security methods and tools are adopted.\n"}, {"id": 23, "text": "\u2022\nOPEX indicates the operating expenses (OPEX) of an enterprise. The calculation is \nperformed as follows: OPEX = Maintenance expense + Marketing expense + \nLabor cost (+ Depreciation). OPEX mainly refers to the cash cost of the current \nperiod.\n\u2022\nCAPEX indicates the capital expenditure, such as fund and fixed assets. For \nexample, the once-off expenditure on network equipment, computers, and \ninstruments is CAPEX, among which network equipment accounts for the largest \nproportion.\n"}, {"id": 24, "text": "\u2022\nAnswer: False. After migrating workloads to the cloud, organizations need to \nadopt cost design. If cloud resources are used without restrictions, the cost will \nfar exceed that of the off-cloud architecture.\n\u2022\nAnswer: A. The five principles of architecture design are security, performance, \ncost, availability, and scalability.\n"}, {"id": 25, "text": "\u2022\nIn the early stages, mainframes and midrange computers provided compute, \nstorage, and network resources. We call this era the \"exclusive computing\" era. \nUnder the leadership of well-known companies such as Intel, x86 chips emerged \nand were used commercially. A large number of data centers emerged as well, \nand the industry started shifting from exclusive computing to general computing, \nthe age of computing 2.0. As the development of network and digital \ntechnologies accelerated, computing was no longer limited to data centers or x86 \nprocessors. Computing services and technologies started diversifying to meet full-\nstack, all-scenario service requirements. We call this era the \"intelligent \ncomputing\" era.\n\u2022\nFull-stack, full-scenario: a variety of development frameworks and languages.\n"}, {"id": 26, "text": "\u2022\nCloud computing has the following characteristics:\n\u25ab\nOn-demand self-service: You can purchase software, servers, and other \nservices by yourself through web portals.\n\u25ab\nResource pooling: Thanks to the virtualization technologies, you can share \nsystems and services in cloud data centers. Regions are physically isolated \nfrom each other.\n\u25ab\nExtreme elasticity: Compute resources can be flexibly scaled as service \ndemand changes. For example, you can purchase more powerful servers to \nhandle increased workloads without having to install new IT systems like \nyou would with an on-premises infrastructure.\n\u25ab\nPay-as-you-go: You only need to pay for what you use by the hour or even \nby the minute.\n\u25ab\nWidespread network access: Cloud computing resources are available over \nthe network and can be accessed by diverse customer platforms. No \nadditional tools are required.\n"}, {"id": 27, "text": "\u2022 High reliability:\n\u25ab\nA broad range of EVS disks: Common I/O, high I/O, general purpose SSD, \nultra-high I/O, and extreme SSD disks are available for customer service \nrequirements.\n\u25ab\nHigh data reliability: Scalable, reliable, and high-throughput virtual block \nstorage is provided in a distributed architecture. This ensures that data can \nbe quickly migrated and restored if any data replica is unavailable, \npreventing data from being lost because of a single hardware fault.\n\u25ab\nBackup and restoration of ECSs and EVS disks: You can configure automatic \nbackup policies for in-service ECSs and EVS disks. You can also configure \npolicies on the management console or use APIs to back up the data of \nECSs and EVS disks at a specified time.\n\u2022\nSecurity assurance:\n\u25ab\nMultiple security services: Web Application Firewall (WAF), Vulnerability \nScan Service (VSS), and other security services provide multi-dimensional \nprotection.\n\u25ab\nSecurity evaluation: The security of cloud environments is evaluated to help \nyou quickly identify security vulnerabilities and threats. Security \nconfiguration check and recommendations reduce or eliminate losses due \nto viruses or online attacks.\n\u25ab\nIntelligent process management: You can customize a whitelist to \nautomatically prohibit the execution of unauthorized programs.\n\u25ab\nVulnerability scanning: Comprehensive scanning services are provided, \nincluding general web vulnerability scans, third-party application \nvulnerability scans, port detection, and fingerpr"}, {"id": 28, "text": "programs.\n\u25ab\nVulnerability scanning: Comprehensive scanning services are provided, \nincluding general web vulnerability scans, third-party application \nvulnerability scans, port detection, and fingerprint identification.\n"}, {"id": 29, "text": "\u2022\nECS works with other cloud services to provide compute, storage, and network \nresources.\n\u25ab\nECSs are deployed in different AZs, so that if one AZ becomes faulty, other \nAZs in the same region will not be affected.\n\u25ab\nCloud Eye lets you keep a close eye on the performance and resource \nutilization of ECSs, ensuring their reliability and availability.\n"}, {"id": 30, "text": "\u2022\nGeneral computing-basic\n\u25ab\nSuitable for scenarios that require moderate CPU performance generally \nbut occasionally burstable high performance while keeping costs low\n\u2022\nGeneral computing:\n\u25ab\nSuitable for websites and web applications, small-scale databases and \ncache servers, and light- and medium-workload enterprise applications with \nstrict requirements on PPS\n\u2022\nGeneral computing-plus\n\u25ab\nSuitable for heavy- and medium-load enterprise applications that have \nhigher requirements on computing and network performance, such as web \napplications, e-commerce platforms, short video platforms, online games, \ninsurance, and finance\n\u2022\nMemory-optimized\n\u25ab\nSuitable for massive parallel processing (MPP) data warehouses, \nMapReduce and Hadoop distributed computing, distributed file systems, \nnetwork file systems, and log or data processing applications\n\u2022\nDisk-intensive\n\u25ab\nSuitable for distributed file systems, network file systems, and log or data \nprocessing applications\n\u2022\nHigh-performance computing\n\u25ab\nComputing and storage systems for genetic engineering, games, \nanimations, and biopharmaceuticals\n\u2022\nThe types displayed in the table were current as of the end of August 2022.\n"}, {"id": 31, "text": "\u2022\nTo select the right ECS type, consider the following factors:\n\u25ab\nService deployment: Deploy ECSs in the region closest to your services to \nreduce network delay and improve the access speed.\n\u25ab\nResource utilization: Make full use of purchased cloud resources. Do not \nbuy more capacity than is needed.\n\u25ab\nSpecification adjustment: In the subsequent content, we'll examine a \nhypothetical startup to look at how to select the right ECS types for \ndifferent development stages (startup, growth, and maturity).\n\u25ab\nCost control: Selecting the right ECS types and specifications help control \ncosts. Evaluate your service scale and budget and scale up ECSs or change \nECS types to meet service demands.\n"}, {"id": 32, "text": "\u2022\nT6 family:\n\u25ab\nThe performance of general-computing basic T6 ECSs is restricted by the \nbenchmark performance and CPU credits.\n\u25ab\nSuitable for scenarios where the CPU usage is low but requires burstable \nCPU power, for example, microservices.\n\u2022\nS6 family:\n\u25ab\nS6 ECSs are equipped with second-generation Intel\u00ae Xeon\u00ae Scalable \nprocessors and Huawei 25GE high-speed intelligent NICs that cost-\neffectively provide high network bandwidth and PPS throughput.\n\u25ab\nSuitable for websites and web applications with high requirements for PPS\n\u2022\nS7 family:\n\u25ab\nS7 ECSs are equipped with third-generation Intel\u00ae Xeon\u00ae Scalable \nprocessors and Huawei 25GE high-speed intelligent NICs that cost-\neffectively provide high network bandwidth and PPS throughput.\n\u2022\nWhat is PPS?\n\u25ab\nPPS, short for packets per second, is the number of network data packets \nthat can be processed by an ECS per second, including the number of sent \nand received packets, including both private and public traffic. The \nmaximum PPS is the maximum number of data packets an ECS can process, \nboth incoming and outgoing per second. \n"}, {"id": 33, "text": "\u2022\nC3 family:\n\u25ab\nC3 ECSs use Intel\u00ae Xeon\u00ae Scalable processors to provide high and stable \ncomputing performance. Working in high-performance networks, the C3 \nECSs deliver higher performance and stability, meeting enterprise-class \napplication requirements.\n\u25ab\nSuitable for small- and medium-sized databases, cache clusters, and search \nclusters that have high requirements on stability.\n\u2022\nC6s family:\n\u25ab\nC6s ECSs use second-generation Intel\u00ae Xeon\u00ae Scalable processors to provide \nhigh performance, high stability, low latency, and cost-effectiveness.\n\u25ab\nSuitable for Internet, gaming, and rendering scenarios, especially those that \nrequire high computing and network stability.\n\u2022\nC7 family:\n\u25ab\nC7 ECSs use third-generation Intel\u00ae Xeon\u00ae Scalable processors to provide \nenhanced compute, security, and stability. A C7 ECS can be configured with \nup to 128 vCPUs and 3,200 MHz memory. C7 ECSs support secure reboot \nand provide secure, trusted cloud environment for applications to run in.\n\u25ab\nSuitable for heavy- and medium-load enterprise applications that demand \nmore compute and network performance, such as web applications, e-\ncommerce platforms, short video platforms, online games, insurance, and \nfinance applications.\n"}, {"id": 34, "text": "\u2022\nM7 family:\n\u25ab\nM7 ECSs use third-generation Intel\u00ae Xeon\u00ae Scalable processors to provide \nenhanced compute, security, and stability. An M7 ECS can be configured \nwith up to 128 vCPUs and 3,200 MHz RAM frequency. M7 ECSs support \nsecure reboot and provide secure, trusted cloud environment for \napplications.\n\u25ab\nSuitable for high-performance data warehouses, in-memory databases, \nMapReduce and Hadoop distributed computing, distributed file systems and \nnetwork file systems, and log or data processing applications.\n\u2022\nD7 family:\n\u25ab\nD7 ECSs are mainly used for massively parallel processing (MPP) data \nwarehouses, MapReduce and Hadoop distributed computing, and big data \ncomputing.\n\u25ab\nSuitable for distributed file systems, network file systems, and log or data \nprocessing applications.\n\u2022\nI7 family:\n\u25ab\nI7 ECSs use high-performance local NVMe SSDs to provide high IOPS and \nlow read/write latency.\n\u25ab\nSuitable for high-performance relational databases, non-relational \ndatabases, and ElasticSearch search.\n"}, {"id": 35, "text": "\u2022\nECSs should be continuously optimized.\n"}, {"id": 36, "text": "\u2022\nBilling modes: yearly/monthly, pay-per-use, and spot price\n\u25abYearly/monthly: You can purchase a yearly/monthly ECS subscription and \nenter your required duration. Yearly/monthly subscriptions are pre-paid \nwith a single, lump sum payment.\n\u25abPay-per-use: You do not need to set a required duration after setting ECS \nconfigurations. The system bills your account based on the service duration.\n\u25abSpot price: Huawei Cloud sells available spare compute resources at a \ndiscount. The price changes in real time depending on market supply and \ndemand.\n\u2022\nRegion and AZ: ECSs in different regions cannot communicate with each other \nover a private network. Select a region closest to your services to ensure low \nnetwork latency and quick access.\n\u2022\nSpecifications: A broad set of ECS types are available for you to choose from. You \ncan choose from existing types and flavors in the list, or enter a flavor or specify \nvCPUs and memory size to search for the flavor suited to your needs.\n\u2022\nImage: An image is a server or disk template that contains an OS or service data \nand necessary application software. IMS provides public, private, Marketplace, \nand shared images.\n\u2022\nSystem disk types: high I/O, general-purpose SSD, ultra-high I/O, and extreme \nSSD. By default, you need to specify the type and size of the system disk.\n"}, {"id": 37, "text": "\u2022\nNetwork settings for an ECS:\n\u25ab\nSubnet: A subnet is a range of IP addresses in your VPC and provides IP \naddress management and DNS resolution functions for ECSs in it. The IP \naddresses of all ECSs in a subnet belong to the subnet.\n\u25ab\nSecurity group: A security group is a collection of access control rules for \nECSs that have the same security protection requirements and that are \nmutually trusted. It helps to enhance ECS security.\n\u25ab\nExtension NIC: optional\n\u2022\nAdvanced settings for an ECS:\n\u25ab\nECS name: You can customize ECS names in compliance with naming rules. \nIf you intend to purchase multiple ECSs at a time, the system automatically \nadds a hyphen followed by a four-digit incremental number to the end of \neach ECS.\n\u25ab\nLogin mode: Key pair allows you to use a key pair for login authentication. \nPassword allows you to use a username and its initial password for login \nauthentication. For Linux ECSs, the initial password is the root password. \nFor Windows ECSs, the initial password is the Administrator password.\n\u25ab\nCloud Backup and Recovery: With CBR, you can back up data for ECSs and \nEVS disks, and use backups to restore the ECSs and EVS disks when \nnecessary.\n\u25ab\nECS group (Optional): An ECS group allows ECSs within the group to be \nautomatically allocated to different hosts. \n\u25ab\nAdvanced options: You can configure other advanced and optional settings.\n"}, {"id": 38, "text": "\u2022\nCost-effectiveness: DeH allows you to bring your own license (BYOL), such as \nlicenses for Microsoft Windows Server, Microsoft SQL Server, and Microsoft \nOffice.\n\u2022\nSecurity: DeH isolates compute resources to prevent your workloads on DeHs \nfrom being affected by those of other tenants.\n\u2022\nCompliance: Physically isolated servers meet the compliance requirements of \nsensitive services.\n\u2022\nFlexibility: You can apply for your DeHs flexibly. Your DeHs will be allocated \nwithin several minutes.\n\u2022\nReliability: DeH provides 99.95% availability.\n"}, {"id": 39, "text": "\u2022\nA DeH is fully dedicated for your own ECSs, ensuring the isolation, security, and \nperformance. You can bring your own license (BYOL) to DeH to reduce the costs \non software licenses and facilitate the independent management of ECSs.\n"}, {"id": 40, "text": "\u2022\nNotes:\n\u25ab\nOnly stopped ECSs can be migrated.\n\u2022\nApplication scenario: If you do not use the ECSs deployed on a DeH or want to \ndelete them after a period of time, you can migrate the ECSs to a public resource \npool.\n"}, {"id": 41, "text": "\u2022\nHigh security and reliability:\n\u25ab\nBMS provides you with dedicated computing resources. You can add servers \nto VPCs and security groups for network isolation and integrate related \ncomponents for server security. BMSs run on a QingTian architecture and \ncan use EVS disks, which can be backed up for restoration. BMS \ninterconnects with Dedicated Storage Service (DSS) to ensure the data \nsecurity and reliability required by enterprise services.\n\u2022\nHigh performance:\n\u25ab\nBMS has no virtualization overhead, so the compute resources are fully \ndedicated to running services. The QingTian they run on, an architecture \nfrom Huawei, is designed with hardware-software synergy in mind. BMS \nsupports high-bandwidth, low-latency storage and networks on the cloud, \nmeeting the deployment density and performance requirements of mission-\ncritical services such as enterprise databases, big data, containers, HPC, and \nAI.\n\u2022\nAgile deployment:\n\u25ab\nThe hardware-based acceleration provided by the QingTian architecture \nenables EVS disks to be used as system disks. The required BMSs can be \nprovisioned within minutes of when you submit your order. You can \nmanage your BMSs throughout their lifecycle from the management \nconsole or using open APIs with SDKs.\n\u2022\nQuick integration:\n\u25ab\nBMSs can easily cooperate with the other cloud resources in a VPC, just like \nECSs do, to run a variety of cloud solutions (such as databases, big data \napplications, containers, HPC, and AI solutions), accelerating cloud \ntran"}, {"id": 42, "text": "ith the other cloud resources in a VPC, just like \nECSs do, to run a variety of cloud solutions (such as databases, big data \napplications, containers, HPC, and AI solutions), accelerating cloud \ntransformation.\n"}, {"id": 43, "text": "\u2022\nVPC:\n\u25ab\nYou can configure security groups, VPNs, IP address segments, and \nbandwidth in a VPC. In this way, you can easily manage and configure \ninternal networks and make secure and quick network changes. You can \nalso customize access rules to control BMS access within a security group \nand across security groups to enhance BMS security.\n\u2022\nEnhanced high-speed network:\n\u25ab\nThe bandwidth is at least 10 Gbit/s.\n\u25ab\nThe number of network planes can be customized and up to 4,000 subnets \nare supported.\n\u25ab\nVMs on a BMS can access the Internet.\n\u2022\nUser-defined VLAN:\n\u25ab\nUser-defined VLAN NICs are deployed in pairs. You can configure NIC bonds \nto ensure high availability. User-defined VLANs in different AZs cannot \ncommunicate with each other.\n"}, {"id": 44, "text": "\u2022\nDatabase:\n\u25ab\nMission-critical database services of governments and financial institutions \nmust be deployed on physical servers with dedicated resources, isolated \nnetworks, and guaranteed performance. BMS meets these requirements by \nproviding high-performance servers dedicated to individual users.\n\u2022\nBig data:\n\u25ab\nInternet services involving big data storage and analysis. BMS uses a \ndecoupled storage and compute solution that combines local storage and \nObject Storage Service (OBS).\n\u2022\nContainer:\n\u25ab\nInternet services requiring load balancing. BMS provides more agile \ncontainer deployment with higher density and lower resource overhead \nthan VMs. Cloud native technologies reduce the cost of cloud \ntransformation.\n\u2022\nHPC/AI:\n\u25ab\nHigh-performance computing applications, such supercomputing and DNA \nsequencing, need to process massive volumes of data. BMS meets this \nrequirement by providing excellent computing performance, stability, and \nreal-time responsiveness.\n"}, {"id": 45, "text": "\u2022\nBased on Huawei TaiShan Arm servers, Cloud Phone integrates multiple highly \ncost-effective GPUs to provide professional graphics processing capabilities.\n\u2022\nCloud phones provide video, audio, and touch SDKs. You can develop applications \nbased on terminals to obtain audios and videos of cloud phones. Alternatively, \nyou can collect touch instructions, for example, touch, slide, or click instructions, \nand execute them on cloud phones.\n"}, {"id": 46, "text": "\u2022\nEasy-to-use:\n\u25ab\nDeployment and O&M of containerized applications can be automated and \nperformed all in one place throughout the application lifecycle.\n\u25ab\nHelm charts are pre-integrated, delivering out-of-the-box usability.\n\u2022\nHigh performance:\n\u25ab\nCCE draws on years of field experience in compute, networking, storage, \nand heterogeneous infrastructure. You can concurrently launch containers \nat scale.\n\u25ab\nThe bare-metal NUMA architecture and high-speed InfiniBand network \ncards yield three- to five-fold improvement in computing performance.\n\u2022\nSecure and reliable:\n\u25ab\nCCE allows you to deploy nodes and workloads in a cluster across AZs. Such \na multi-active architecture ensures service continuity against host faults, \ndata center outages, and natural disasters.\n\u25ab\nClusters are private and completely controlled by users with deeply \nintegrated IAM and Kubernetes RBAC. You can set different RBAC \npermissions for IAM users on the console.\n\u2022\nOpen and compatible:\n\u25ab\nCCE streamlines deployment, resource scheduling, service discovery, and \ndynamic scaling of applications that run in Docker containers.\n\u25ab\nCCE is built on Kubernetes and compatible with Kubernetes native APIs and \nkubectl (a command line tool). CCE provides full support for the most \nrecent Kubernetes and Docker releases.\n"}, {"id": 47, "text": "\u2022\nWhen using FunctionGraph, you do not need to apply for or pre-configure any \ncompute, storage, or network services. Simply upload and run code in supported \nruntimes. FunctionGraph provides and manages underlying compute resources, \nincluding CPUs, memory, and networks. It also supports configuration and \nresource maintenance, code deployment, automatic scaling, load balancing, \nsecure upgrade, and resource monitoring.\n"}, {"id": 48, "text": "\u2022\nConvenient: You can use a public, Marketplace, or private image to create ECSs in \nbatches, simplifying service deployment. You can also share, replicate, or export \nimages between different accounts, regions, or even cloud platforms.\n\u2022\nSecure: To ensure data reliability and durability, multiple copies of image files are \nstored using Object Storage Service (OBS). You can use the envelope encryption \nprovided by Key Management Service (KMS) to encrypt private images.\n\u2022\nFlexible: You can manage the lifecycle of images using the management console \nor APIs as needed. IMS can meet your requirements no matter you want to \nmigrate servers to the cloud, back up server environments, or migrate servers \nbetween different accounts or regions on the cloud.\n\u2022\nUnified: IMS provides a unified platform to simplify image maintenance. Images \ncan be used to deploy and upgrade applications in a unified manner, improving \napplication O&M efficiency and ensuring environment consistency.\n"}, {"id": 49, "text": "\u2022\nA private image can be a system disk image, data disk image, or full-ECS image.\n\u25ab\nA system disk image contains an OS and pre-installed software for various \nservices. You can use a system disk image to create cloud servers and \nmigrate your services to the cloud.\n\u25ab\nA data disk image contains only service data. You can use a data disk image \nto create EVS disks and use them to migrate your service data to the cloud.\n\u25ab\nA full-ECS image contains an OS, pre-installed software, and service data. A \nfull-ECS image is created using differential backups and the creation takes \nless time than creating a system or data disk image of the same size.\n"}, {"id": 50, "text": "\u2022\nBefore you share an image, ensure that:\n\u25ab\nYou have obtained the project ID of the target user.\n\u25ab\nAny sensitive data has been deleted from the image.\n\u2022\nConstraints:\n\u25ab\nYou cannot share private images that have been published in Marketplace.\n\u25ab\nYou can share images only within a given region. To share an image across \nregions, you need to replicate the image to the target region first.\n\u25ab\nA system disk image or data disk image can be shared with a maximum of \n128 users, and a full-ECS image can be shared with a maximum of 10 users.\n\u25ab\nEncrypted images cannot be shared.\n\u25ab\nOnly full-ECS images created from an ECS or a CBR backup can be shared.\n"}, {"id": 51, "text": "\u2022\nWhen you submit a request for creating a full-ECS image from an ECS, the \nsystem will automatically create a backup for the ECS and then use the backup \nto create a full-ECS image.\n\u2022\nThe time required for creating a full-ECS image depends on the disk size, \nnetwork quality, and the number of concurrent tasks.\n\u2022\nThe ECS used to create a full-ECS image must be in Running or Stopped state. To \ncreate a full-ECS image containing a database, use a stopped ECS.\n\u2022\nWhen a full-ECS image is being created, if you detach the system disk from the \nECS or stop, start, or restart the ECS, the image creation will fail.\n\u2022\nIf there are snapshots of the system disk and data disks but the ECS backup \ncreation is not complete, the full-ECS image you create will only be available in \nthe AZ where the source ECS is and can only be used to provision ECSs in this AZ. \nYou cannot provision ECSs in other AZs in the region until the original ECS is fully \nbacked up and the full-ECS image is in the Normal state.\n\u2022\nIf you use a full-ECS image to change an ECS OS, only the system disk data can \nbe written into the ECS. Therefore, if you want to restore or migrate the data disk \ndata of an ECS by using a full-ECS image, you can only use the image to create a \nnew ECS rather than use it to change the ECS OS.\n"}, {"id": 52, "text": "\u2022\nWhen there are more resources available than what is needed, idle resources are \nwasted.\n\u2022\nWhen there are not enough resources available, user experience deteriorates. \nUser churn increases and revenue is lost.\n"}, {"id": 53, "text": "\u2022\nAutomatic scaling helps you automatically meet customer requirements.\n"}, {"id": 54, "text": "\u2022\nThe process of using AS is as follows:\n\u25ab\nFirst, create an AS configuration. Then, create an AS group, and then \nconfigure an AS policy for the AS group you just created based on your \nservice requirements.\n\u2022\nAS advantages:\n\u25ab\nAutomatic scaling: When demand spikes, AS adds ECS instances and \nincreases bandwidth to maintain service quality. When demand decreases, \nAS removes unneeded resources to avoid wasting resources.\n\u25ab\nLower costs: AS can automatically adjust resources for applications. This \nenables you to allocate resources on demand, eliminate waste, and reduce \ncosts.\n\u25ab\nImproved availability: With AS, your applications always have the right \namount of resources at the right time. When working with ELB, AS \nautomatically associates a load balancing listener with any instances newly \nadded to an AS group. Then, ELB automatically distributes access traffic to \nall healthy instances in the AS group through the listener.\n\u25ab\nHigh fault tolerance: AS monitors the statuses of instances in an AS group, \nand replaces any unhealthy instances it detects with new ones.\n"}, {"id": 55, "text": "\u2022\nAS can work with Cloud Eye to make smarter scaling actions.\n\u25ab\nIn the example shown here, when the number of access requests reaches \n1,000, the existing resources cannot handle the demand. More resources \nare needed. When the peak hours pass, idle resources need to be removed \nto avoid waste and reduce costs.\n\u25ab\nAS can work together with Cloud Eye to do this automatically. When Cloud \nEye detects resources reach a threshold you have specified in an AS policy, \nfor example, CPU usage higher than 70%, memory usage higher than 80%, \nor access requests more than 500, AS automatically triggers scaling actions \nto add more resources.\n"}, {"id": 56, "text": "\u2022\nWhen you use AS, you need to create an AS group, create an AS configuration, \nand then configure an AS policy for the AS group.\n\u2022\nThen AS checks whether the condition specified in the AS policy is met, and \ndetermines whether to execute a scaling action based on the results.\n\u2022\nAn AS group consists of a collection of ECS instances and AS policies that have \nsimilar attributes and application scenarios. An AS group is the basis for enabling \nor disabling AS policies and performing scaling actions.\n\u2022\nAn AS configuration defines the specifications of instances to be added to an AS \ngroup. The specifications include the ECS image and system disk size.\n\u2022\nAn AS policy can trigger scaling actions to scale ECS and bandwidth resources for \nan AS group. An AS policy defines the conditions for triggering a scaling action \nand the operation that will be performed. When the condition is met, a scaling \naction is triggered automatically. AS supports alarm-based, scheduled, and \nperiodic scaling policies.\n\u2022\nWhen creating an AS group, you need to configure parameters, such as Max. \nInstances, Min. Instances, Expected Instances, and Load Balancing.\n"}, {"id": 57, "text": "\u2022\nThe instance status changes from Initial to Adding to AS group when either of \nfollowing occurs:\n\u25ab\nYou manually increase the expected number of instances for the AS group \nor AS automatically adds instances to the AS group.\n\u25ab\nYou manually add instances to the AS group.\n\u2022\nThe instance status changes from Enabled to Removing from AS group when any \nof the following occurs:\n\u25ab\nYou manually decrease the expected number of instances for the AS group \nor the system automatically removes instances from the AS group.\n\u25ab\nAS removes unhealthy instances from the AS group.\n\u25ab\nYou manually remove instances from the AS group.\n"}, {"id": 58, "text": "\u2022\nWhen a scale-out or scale-in event occurs in the AS group, the required instances \nare suspended by the lifecycle hook and remain in the wait status until the \ntimeout period ends or you manually call back the instances. You can perform \ncustom operations on the instances when they are in the wait status. For \nexample, you can install or configure software on an instance before it is added \nto the AS group or download log files from an instance before it is removed.\n"}, {"id": 59, "text": "\u2022\nYou can use AS and ELB to confidently deal with changes in service demand. \nWhen the workload goes up or down, AS scales out or in instances to maintain \nsteady performance at the lowest possible cost. ELB can manage incoming \nrequests by optimizing traffic routing to prevent instance overload.\n\u2022\nAfter ELB is enabled for an AS group, AS automatically associates a load \nbalancing listener with any instances newly added to the AS group. Then, ELB \nautomatically distributes traffic to all healthy instances in the AS group through \nthe listener to improve system availability. The instances in the AS group may be \nhosting multiple applications. You can bind different load balancing listeners to \nthe AS group to listen to each of these applications. This further improves your \nservice scalability.\n"}, {"id": 60, "text": "\u2022\n1. Answer: ABCD\n\u2022\n2. Answer: ABC\n"}, {"id": 61, "text": "\u2022\nDiscussion 1:\n\u25ab\nCloud servers are managed by cloud service providers.\n\u25ab\nOn-premises self-built servers are managed by users.\n\u2022\nDiscussion 2:\n\u25ab\nSecurity: dynamic and static data security, network security, and access \ncontrol\n\u25ab\nCost: server selection and purchase model\n\u25ab\nReliability: cluster deployment\n\u25ab\nPerformance: meet service requirements and reserve redundancy\n\u25ab\nScalability: use Auto Scaling (AS) to dynamically adjust compute resources\n"}, {"id": 62, "text": "\u2022\nNetwork functions and resources are hosted on public or private cloud platforms, \nmanaged by the platforms or by service providers, and provided on demand.\n\u2022\nUsers and applications with high mobility require the flexibility and scale of cloud \nnetworks for assured performance, security, and easier management.\n\u2022\nCloud networks also improve IT efficiency and save money for offices, schools, \nhome office, healthcare, and public spaces.\n"}, {"id": 63, "text": "\u2022\nThere are the following types of network services:\n\u2022\nCloud networks:\n\u25ab\nGeneral networks and security policies: VPCs, security groups, and network \nACLs\n\u25ab\nCommunications within a given region on the cloud: VPC Endpoint and VPC \nPeering\n\u25ab\nCross-region communications on the cloud: Direct Connect, Cloud Connect, \nand VPN\n\u2022\nCloud network access: EIP, NAT Gateway, ELB, and DNS\n"}, {"id": 64, "text": "\u2022\nIP address planning:\n\u25ab\nEnsure that the VPC CIDR block does not overlap with the enterprise private \nnetwork. If there are multiple VPCs in different regions, the VPC CIDR \nblocks cannot overlap.\n\u25ab\nSelect a VPC CIDR block based on expected service growth.\n\u25ab\nDo not allocate all subnets and IP addresses at once. You should reserve \nspace for future capacity expansion.\n\u2022\nPrivate CIDR blocks:\n\u25ab\nSelect private CIDR blocks for VPCs and subnets, which are used for private \ncommunications. If a public CIDR block is configured, conflicts may occur \nduring internet access.\n\u25ab\n10.0.0.0-10.255.255.255 (10/8 prefix)\n\u25ab\n172.16.0.0-172.31.255.255 (172.16/12 prefix)\n\u25ab\n192.168.0.0-192.168.255.255 (192.168/16 prefix)\n"}, {"id": 65, "text": "\u2022\nSimilar to security group rules, network ACL rules are used to determine whether \ndata packets can enter or leave a subnet.\n"}, {"id": 66, "text": "\u2022\nSecurity group 1: The first rule allows Web 1 server to communicate with other \nweb servers that may be added later for capacity expansion. The second rule \nallows internet access to Web 1 server. The third rule allows all outbound traffic.\n\u2022\nSecurity group 2: The first rule allows the App server to communicate with other \nApp servers that may be added later for capacity expansion. The second rule \nallows Web 1 server to access the App server. The third rule allows all outbound \ntraffic.\n\u2022\nNetwork ACL 1: The first rule denies the access from the test subnet. The second \nrule allows all inbound access, excepting the access from the test subnet denied \nby the first rule. The third rule allows all outbound traffic.\n\u2022\nNetwork ACL 2: The first rule denies the access from the production subnet. The \nsecond rule allows all inbound access, excepting the access from the production \nsubnet denied by the first rule. The third rule allows all outbound traffic.\n"}, {"id": 67, "text": "\u2022\nIf two VPCs connected by a VPC peering connection overlap with each other, \nthere will be route conflicts and the VPC peering connection may not be usable.\n\u2022\nIf two VPCs connected by a VPC peering connection have overlapping CIDR \nblocks, the connection can only enable communications between non-\noverlapping subnets in the VPCs. If subnets in the two VPCs of a VPC peering \nconnection overlap with each other, the connection may not take effect, so \nensure that there are no overlapping subnets.\n\u2022\nIf there are three VPCs, A, B, and C, and VPC A is peered with both VPC B and \nVPC C, but VPC B and VPC C overlap with each other, you cannot configure \nroutes with the same destinations for VPC A.\n\u2022\nYou cannot have more than one VPC peering connection between the same two \nVPCs at the same time.\n\u2022\nVPC peering does not support transitive peering relationships. For example, if VPC \nA is connected to both VPC B and VPC C, but VPC B and VPC C are not \nconnected, VPC B and VPC C cannot communicate with each other through VPC \nA. You need to create a VPC peering connection between VPC B and VPC C.\n\u2022\nA VPC peering connection between VPCs in different regions will not be usable.\n"}, {"id": 68, "text": "\u2022\nIf you request a VPC peering connection with a VPC of another account, the \nconnection takes effect only after the peer account accept the request. If you \nrequest a VPC peering connection with a VPC of your own, the system \nautomatically accepts the request and activates the connection.\n"}, {"id": 69, "text": "VPCEP provides two types of resources: VPC endpoint services and VPC endpoints.\n\u2022\nVPC endpoint services refer to cloud services or your private services that can be \nconfigured in VPCEP to provide services to users. For example, you can create an \napplication in a VPC and configure it as a VPC endpoint service that VPCEP \nsupports. \n\u2022\nVPC endpoints are channels for connecting VPCs to VPC endpoint services. You \ncan create an application in your VPC and configure it as a VPC endpoint service. \nA VPC endpoint can be created in another VPC in the same region and then used \nas a channel to access the VPC endpoint service.\n"}, {"id": 70, "text": "\u2022\nFunction:\n\u25ab\nA VPC peering connection enables traffic between two VPCs so that \ninstances in the subnets of the two VPCs can communicate with each other \nas if they were in the same network.\n\u2022\nAccess scenario:\n\u25ab\nVPC peering connections, in most cases, are used to connect subnets of two \nVPCs belong to the same tenant.\n\u25ab\nVPCEP makes services available to tenants on a cloud platform.\n"}, {"id": 71, "text": "\u2022\nDirect Connect enables communication between the on-premises data center and \nVPC 1.\n\u2022\nWith VPC endpoint 1, the user's on-premises data center can access ELB in VPC 1.\n\u2022\nWith VPC endpoint 2, the user's on-premises data center can access Elastic Cloud \nServers (ECSs) in VPC 2.\n\u2022\nWith VPC endpoint 3, the user's on-premises data center can access Domain \nName Service (DNS) over the intranet.\n\u2022\nWith VPC endpoint 4, the user's on-premises data center can access Object \nStorage Service (OBS) over the intranet.\n"}, {"id": 72, "text": "\u2022\nHigh data security\n\u25ab\nHuawei hardware uses IKE and IPsec to encrypt data to provide carrier-class \nreliability and ensure a stable VPN connection.\n\u2022\nSeamless scale-out\n\u25ab\nWith VPN, you can connect your local data center to your VPC and quickly \nextend services at the data center to the cloud, thereby forming a hybrid \ncloud architecture.\n"}, {"id": 73, "text": "\u2022\nThe connection is a dedicated network connection between your premises and a \nDirect Connect location over a line you lease from a carrier. You can create a \nstandard connection by yourself or request a hosted connection from a partner. \nAfter you are certified as a partner, you can also create an operations connection.\n\u25ab\nA standard or operations connection has a dedicated port for your exclusive \nuse and can be associated with multiple virtual interfaces.\n\u25ab\nA hosted connection allows you to share a port with others. Partners with \noperations connections can provision hosted connections and allocate \nVLANs and bandwidths for those connections. Only one virtual interface \ncan be created for each hosted connection.\n\u2022\nThe virtual gateway is a logical gateway for accessing VPCs. Each VPC can have \nonly one virtual gateway associated, but multiple connections can use the same \nvirtual gateway to access one VPC.\n\u2022\nThe virtual interface links a connection with one or more virtual gateways, each \nof which is associated with a VPC, so that your on-premises network can \ncommunicate with all these VPCs.\n"}, {"id": 74, "text": "\u2022\nVPN\n\u25ab\nIPsec VPN safeguards data transfer.\n\u25ab\nEase of use and instant availability\n\u2022\nDirect Connect\n\u25ab\nHighly private with dedicated connections linking on-premises and cloud \nnetworks\n\u25ab\nLow and stable latency, low jitter level, and excellent performance\n"}, {"id": 75, "text": "Prerequisites:\n\u25ab\nSingle-mode 1 GE, 10 GE, 40 GE, or 100GE optical modules must be used to \nconnect to Huawei Cloud access devices.\n\u25ab\nAuto-negotiation for the port has been disabled. The port speed and full-\nduplex mode have been manually configured.\n\u25ab\n802.1Q VLAN encapsulation is supported on your on-premises network.\n\u25ab\nYour device supports Border Gateway Protocol (BGP) and does not use \nAutonomous System Number (ASN) 64512, which is used by Huawei Cloud.\n"}, {"id": 76, "text": "Constraints:\n\u25ab\nA cloud connection cannot be created between VPCs that have overlapping \nCIDR blocks, or network communications will fail. In addition, IP addresses \nof network instances that will be loaded to a cloud connection cannot \noverlap.\n\u25ab\nIf you load a VPC to a cloud connection created using the same account, \nyou cannot enter loopback addresses, multicast addresses, or broadcast \naddresses for the custom CIDR block.\n\u25ab\nIf a NAT gateway has been created for any VPC you have loaded to a cloud \nconnection, a custom CIDR block needs to be added and set to 0.0.0.0/0.\n"}, {"id": 77, "text": "\u2022\nShared bandwidth:\n\u25ab\nShared bandwidth allows ECSs, BMSs, and load balancers that are bound \nwith EIPs from the same region to share the same bandwidth.\n\u25ab\nWhen you host a large number of applications on the cloud, if each EIP \nuses an independent bandwidth, a lot of bandwidths are required, which \nsignificantly increases bandwidth costs. If all EIPs share the same \nbandwidth, you can lower bandwidth cost and easily perform O&M.\n"}, {"id": 78, "text": "\u2022\nDynamic BGP:\n\u25ab\nDynamic BGP provides automatic failover and chooses the best path based \non real-time network conditions and preset policies.\n\u2022\nStatic BGP:\n\u25ab\nStatic routes are configured manually and must be manually reconfigured \nanytime the network topology or link status changes.\n\u2022\nComparison in assurance:\n\u2022\nDynamic BGP:\n\u25ab\nWhen a fault occurs on a carrier's link, dynamic BGP will quickly select \nanother path to take over services, ensuring service availability.\n\u25ab\nCurrently, carriers in China that support dynamic BGP routing include China \nTelecom, China Mobile, China Unicom, China Education and Research \nNetwork (CERNET), National Radio and Television Administration, and Dr. \nPeng Group.\n\u2022\nStatic BGP:\n\u25ab\nWhen changes occur on a network that uses static BGP, the manual \nconfiguration takes some time and high availability cannot be guaranteed.\n"}, {"id": 79, "text": "\u2022\nDedicated load balancers give you exclusive access to their resources, so the \nperformance of a dedicated load balancer is not affected by other load balancers. \nIn addition, there are a wide range of specifications available for selection.\n\u2022\nShared load balancers are deployed in clusters, where all the load balancers \nshare resources. With a shared load balancer, the performance of one load \nbalancer can be affected by other load balancers.\n"}, {"id": 80, "text": "\u2022\nELB periodically sends heartbeat messages to associated backend servers to \ncheck their health to ensure that traffic is distributed only to healthy backend \nservers. This can improve the availability of applications.\n"}, {"id": 81, "text": "\u2022\nThe maximum stickiness duration at Layer 7 is 24 hours.\n\u2022\nThe maximum stickiness duration at Layer 4 is one hour.\n"}, {"id": 82, "text": "\u2022\nApplications with predictable peaks and troughs in traffic\n\u25ab\nFor an application that has predictable peaks and troughs in traffic volumes, \nELB works with Auto Scaling to add or remove backend servers to keep up \nwith the changing demand. ELB routes requests to the required number of \nbackend servers to handle the load of your application based on the load \nbalancing algorithm and health check you set. One example is flash sales, \nduring which application traffic spikes in a short period. ELB can work with \nAuto Scaling to run only the required number of backend servers, helping \nto minimize IT costs.\n"}, {"id": 83, "text": "\u2022\nCross-AZ load balancing:\n\u25ab\nFor services that require high availability, ELB can distribute traffic across \nAZs. If an AZ becomes faulty, ELB distributes the traffic to backend servers \nin other AZs that are running properly.\n\u25ab\nELB is ideal for banking, policing, and large application systems that require \nhigh availability.\n"}, {"id": 84, "text": "\u2022\nFlexible deployment\n\u25ab\nA public NAT gateway can be shared across subnets and AZs, so that even \nif an AZ fails, the public NAT gateway can still run normally in another AZ. \nThe type and EIP of a public NAT gateway can be changed at any time.\n\u2022\nEase of use\n\u25ab\nMultiple types of public NAT gateways are available. Public NAT gateway \nconfiguration is simple, the O&M is easy, and they can be provisioned \nquickly. Once provisioned, they are stable and reliable.\n\u2022\nCost-effectiveness\n\u25ab\nWith a public NAT gateway, when you send data through a private IP \naddress or provide services accessible from the Internet, the public NAT \ngateway translates the private IP address to a public IP address. You no \nlonger need to configure one EIP for each server, which saves money on \nEIPs and bandwidth.\n"}, {"id": 85, "text": "\u2022\nTransit subnet: A transit subnet is where a transit IP address resides.\n\u2022\nTransit VPC: A transit VPC is where a transit subnet resides.\n\u2022\nEasier network planning\n\u25ab\nThe private NAT gateway allows for communication between overlapping \nCIDR blocks. This frees customers from the time-consuming and stressful \nnetwork replanning, so that customers can retain their original network \nwhile migrating workloads to the cloud.\n\u2022\nStrong security\n\u25ab\nPrivate NAT gateways help organizations meet industry regulatory \nrequirements by mapping private IP addresses to specified IP addresses for \naccess.\n\u2022\nEasy O&M\n\u25ab\nA private NAT gateway can map the CIDR block of each department to the \nsame VPC CIDR block, which simplifies the management of complex \nnetworks.\n\u2022\nZero conflicts\n\u25ab\nThanks to IP address mapping, the private NAT gateways allow for \ncommunication between overlapping CIDR blocks.\n"}, {"id": 86, "text": "\u2022\nAn SNAT connection consists of a source IP address, source port, destination IP \naddress, destination port, and transport layer protocol. The source IP address \nrefers to the EIP, and the source port refers to the EIP port. These five elements \nidentify a connection as a unique session.\n\u2022\nThroughput specifies the total bandwidth of EIPs in a DNAT rule. For example, a \npublic NAT gateway has two DNAT rules. If the EIP bandwidth in the first rule is \n10 Mbit/s and that in the second rule is 5 Mbit/s, the throughput of the public \nNAT gateway is 15 Mbit/s.\n\u2022\nEach public NAT gateway supports up to 20 Gbit/s of bandwidth.\n\u2022\nCommon scenarios and recommended NAT gateway types:\n\u25ab\nSmall or medium: scenarios where there are a small number of destination \naddresses and connections, such as upload, download, and Internet access.\n\u25ab\nLarge or extra large: scenarios where there are a large number of \ndestination addresses or ports and connections, such as crawlers and client \npush.\n\u2022\nThe maximum number of SNAT connections varies depending on the NAT \ngateway type. The details are as follows:\n\u25ab\nSmall: 10,000\n\u25ab\nMedium: 50,000\n\u25ab\nLarge: 200,000\n\u25ab\nUltra-large: 1,000,000\n"}, {"id": 87, "text": "\u2022\nSmooth service migration\n\u25ab\nYou can migrate an in-use website domain name to the DNS service. To \nensure that your website services are not interrupted during the migration, \nwe will create a public zone and add DNS record sets for your website in \nadvance.\n"}, {"id": 88, "text": "\u2022\nPublic domain name resolution: maps domain names to public IP addresses so \nthat your users can access your website or web applications over the Internet. A \npublic zone contains information about how a domain name and its subdomains \nare translated into IP addresses for routing traffic over the Internet.\n\u2022\nPrivate domain name resolution: Translates private domain names into private IP \naddresses to facilitate access to cloud resources within VPCs. A private zone \ncontains information about how to map a domain name (such as ecs.com) and \nits subdomains used within one or more VPCs to private IP addresses (such as \n192.168.1.1). With private domain names, your ECSs can communicate with each \nother within the VPCs without having to connect to the Internet. These ECSs can \nalso access cloud services, such as OBS and Simple Message Notification (SMN), \nover a private network.\n\u2022\nReverse resolution: DNS obtains a domain name based on an IP address. Reverse \nresolution, or reverse DNS lookup, is typically used to affirm the credibility of \nemail servers.\n\u2022\nIntelligent resolution: returns different resolution results for the same domain \nname based on the carrier networks or geographic locations of user IP addresses. \nFor example, if the visitor is a China Unicom user, the DNS server will return an \nIP address of China Unicom. With this function, you can improve DNS resolution \nefficiency and speed up cross-network access. You can also create more fine-\ngrained resolution lines bas"}, {"id": 89, "text": "ill return an \nIP address of China Unicom. With this function, you can improve DNS resolution \nefficiency and speed up cross-network access. You can also create more fine-\ngrained resolution lines based on source IP addresses.\n"}, {"id": 90, "text": "\u2022\nABC\n\u2022\nA\n"}, {"id": 91, "text": "\u2022\nDiscussion 1:\n\u25ab\nCloud network services are managed by cloud service providers.\n\u25ab\nTraditional network services are managed by users.\n\u2022\nDiscussion 2:\n\u25ab\nTo ensure network security, configure outbound and inbound rules and \nallow traffic only on specific ports.\n\u25ab\nTo reduce costs, delete servers that are not working in a backend server \ngroup for load balancing immediately.\n\u25ab\nTo ensure reliability, perform health check to ensure that backend servers \nare healthy and are of the same type.\n\u25ab\nTo ensure performance, use the monitoring function to maximize the load \ncapability of ECSs.\n\u25ab\nTo ensure scalability, use the Layer 4 and Layer 7 forwarding capabilities of \nload balancers to prevent network access congestion.\n"}, {"id": 92, "text": "\u2022\nIn the IBM mainframe era, file, network, and storage capabilities are all \nencapsulated into one environment. In the x86 era, data in databases is stored in \nx86 servers. In the virtualization era, data is stored in VMs using the distributed \ntechnology. Services are migrating to the cloud, data is stored on the cloud, and \nall-IP network protocols become a major trend.\n"}, {"id": 93, "text": "\u2022\nBased on the server type, storage can be classified into closed storage and open \nstorage. Open storage can be then classified as built-in storage and external \nstorage. External storage can be further classified as Direct-Attached Storage \n(DAS), Network-Attached Storage (NAS), and Storage Area Network (SAN) \nbased on the connection method and transmission protocol.\n\u2022\nDAS: Although DAS is old, it is still suitable for scenarios where the data volume \nis small and the requirement for access speed is not high.\n\u2022\nNAS: NAS is suitable for file servers to store unstructured data. Although their \naccess speed is limited by the Ethernet, NAS can be flexibly deployed at low costs.\n\u2022\nSAN: SAN is suitable for large-scale applications or database systems. But SAN is \ncostly and complex.\n\u2022\nBlock storage: Block storage breaks up data into blocks and then stores those \nblocks as separate pieces, each with a unique identifier. Those blocks of data can \nbe placed wherever it is most efficient. That means each block can be configured \n(or partitioned) to work with different operating systems.\n\u2022\nFile storage: File storage is also referred to as file-level or file-based storage. File \nstorage data is stored as single pieces of data in folders.\n\u2022\nObject storage: Object storage, which is also known as object-based storage, \nbreaks data files up into pieces called objects. It then stores those objects in a \nsingle repository, which can be spread out across multiple networked systems.\n"}, {"id": 94, "text": "\u2022\nRecovery Point Objective (RPO): the maximum tolerable amount of lost data\n\u2022\nRecovery Time Objective (RTO): the maximum tolerable service downtime, from \nthe time when a disaster happened to the time when services were recovered\n"}, {"id": 95, "text": "\u2022\nPrecautions:\n\u25ab\nThe maximum number of disks that can be attached to a cloud server \nvaries with server specifications.\n\u25ab\nWhen attaching a disk, ensure that the server and disk reside in the same \nAZ. Or, the attachment will fail.\n\u25ab\nA backup of a disk will be created in the same AZ of the disk.\n"}, {"id": 96, "text": "\u2022\nRecommended use:\n\u25ab\nHigh I/O disks are recommended to be used as system disks.\n\u25ab\nSSD-based disks are recommended to be used as data disks.\n"}, {"id": 97, "text": "\u2022\nIf the security administrator is the first one to use the encryption function, the \nprocedure is as follows:\n\u25ab\nGrants KMS access rights to EVS. After KMS access rights have been \ngranted, the system automatically creates a Default Master Key (DMK) and \nnames it evs/default. The DMK can be used for encryption.\n\u25ab\nNote: EVS encryption relies on KMS. When the encryption function is used \nfor the first time ever, KMS access rights need to be granted to EVS. After \nKMS access rights have been granted, all the users in this region can use \nthe encryption function, and KMS access rights do not need to be granted \nagain.\n\u25ab\nSelects a key. Users can select one of the following keys: the DMK, \nevs/default.\n\u25ab\nCMKs, including existing CMKs or new CMKs.\n\u25ab\nAfter the security administrator has used the encryption function, all the \nusers in region B can directly use the encryption function.\n\u2022\nIf user E (common user) is the first one to use the encryption function, the \nprocedure is as follows:\n\u25ab\nUses the encryption function, and the system responds a message showing \nthat KMS access rights have not been granted to EVS.\n\u25ab\nContacts the security administrator to request KMS access rights to EVS.\n\u2022\nAfter KMS access rights have been granted to EVS, user E as well as all the users \nin region B can directly use the encryption function and do not need to contact \nthe security administrator to request KMS access rights to EVS again.\n"}, {"id": 98, "text": "\u2022\nExpanding capacity on the management console:\n\u25ab\nChoose an appropriate expansion method based on the disk status. View \nthe disk status. If the disk status is In-use, the disk has been attached to a \nserver. Check whether the disk can be expanded in the In-use state based \non the constraints. If so, directly expand the disk capacity. If not, detach the \ndisk and then expand the disk capacity. If the disk status is Available, the \ndisk has not been attached to any server. You can directly expand the disk \ncapacity.\n"}, {"id": 99, "text": "\u2022\nRoutine data backup\n\u25ab\nYou can create snapshots for disks regularly and use snapshots to recover \nyour data in case that data is lost or inconsistent due to misoperations, \nviruses, or attacks.\n\u2022\nRapid data restoration\n\u25ab\nYou can create a snapshot or multiple snapshots before an application \nsoftware upgrade or a service data migration. If an exception occurs during \nthe upgrade or migration, service data can be rapidly restored to the state \nwhen the snapshot was created.\n\u2022\nMulti-service quick deployment\n\u25ab\nYou can use a snapshot to create multiple disks containing the same initial \ndata, and these disks can be used as data resources for various services.\n"}, {"id": 100, "text": "\u2022\nA bucket is a container for storing objects in OBS. OBS offers a flat structure \nbased on buckets and objects. This structure enables all objects to be stored at \nthe same logical layer, rather than being stored hierarchically. Each bucket has its \nown properties, such as the storage class, access control, and region. You can \ncreate buckets with required storage classes and access control in different \nregions and further configure advanced settings, to meet storage requirements in \na wide range of scenarios.\n\u2022\nOBS provides massive storage for files of any format, catering to the needs of \ncommon users, websites, enterprises, and developers. Neither the entire OBS \nsystem nor any single bucket has limitations on the storage capacity or the \nnumber of objects/files that can be stored. As a web service, OBS supports APIs \nover HTTP and HTTPS. You can easily access and manage data stored in OBS \nanytime, anywhere through OBS Console or OBS tools. In addition, OBS SDKs and \nAPIs make it easy to manage data stored in OBS and to develop upper-layer \napplications.\n"}, {"id": 101, "text": "\u2022\nStandard:\n\u25ab\nThe Standard storage class is appropriate for a wide range of application \nscenarios, including big data analytics, mobile applications, hot videos, and \nsocial images.\n\u2022\nInfrequent Access:\n\u25ab\nThe Infrequent Access storage class can be used for file synchronization and \nsharing, enterprise backups, and many other scenarios. It has the same \ndurability, low latency, and high throughput as the Standard storage class, \nwith a lower cost, but its availability is slightly lower than the Standard \nstorage class.\n\u2022\nArchive:\n\u25ab\nThe Archive storage class is ideal for scenarios such as data archive and \nlong-term backups. It is secure and durable and delivers the lowest cost \namong the three storage classes. The OBS Archive storage class can be \nused to replace tape libraries. To save money, it may take hours to restore \nthe archived data.\n"}, {"id": 102, "text": "\u2022\nYou can choose multi-AZ storage or single-AZ storage as your redundancy policy \nbased on your business needs. The multi-AZ storage stores data in multiple AZs \nto deliver up to 99.9999999999% of data durability and up to 99.995% of service \ncontinuity, far higher than those of a conventional architecture.\n\u2022\nThe 12 nines of durability means that the average annual loss rate of objects is \nexpected to be 0.0000000001%. For example, if you store 100 million objects in \nOBS, only one object may be lost every 10,000 years.\n\u2022\nThe availability can be considered as service continuity. The 99.995% availability \nmeans that if you keep accessing OBS for 100,000 minutes (about 69 days), you \ncan expect less than 5 minutes of unavailability.\n"}, {"id": 103, "text": "\u2022\nLet's look at what these actions mean:\n\u2022\nStorage class transition: An object is transitioned from one storage class to \nanother.\n\u2022\nDelete upon expiration: After an object has expired, it is deleted by OBS.\n\u2022\nThe following rules are also important:\n\u25ab\nThere is no limit on the number of lifecycle rules in a bucket, but the total \nsize of XML descriptions about all lifecycle rules in a bucket cannot exceed \n20 KB.\n\u25ab\nThe minimum storage duration of Archive storage is 90 days. After an \nobject is transitioned to the Archive storage class, if it stays in this storage \nclass for less than 90 days, you still need to pay for a full 90 days.\n\u2022\nThere are some restrictions on storage class transition using lifecycle rules:\n\u25ab\nLifecycle rules can transition objects only from the Standard storage class to \nInfrequent Access storage class, or from the Standard or Infrequent Access \nstorage class to Archive storage class.\n\u25ab\nIf you want to change the storage class back from Infrequent Access to \nStandard, or from Archive to Standard or Infrequent Access, you must \nmanually transition the storage class. In addition, to change the storage \nclass of an archived object, you need to manually restore the object first.\n"}, {"id": 104, "text": "\u2022\nYou can configure a rule to replicate only objects with a specified prefix or to \nreplicate all objects in a bucket. Replicated objects in the destination bucket are \ncopies of those in the source bucket. Objects in both buckets have the same \nnames, metadata, content, sizes, last modification time, creators, version IDs, \nuser-defined metadata, and ACLs. By default, a source object and its copy have \nthe same storage class, but you can also specify a different storage class for an \nobject copy if you want.\n\u2022\nThe content that is replicated includes:\n\u25ab\nNewly uploaded objects (excluding those in the Archive storage class).\n\u25ab\nUpdated objects, for example, the object content is updated or the copied \nACL is updated.\n\u25ab\nHistorical objects in a bucket if the function of synchronizing existing \nobjects is enabled (excluding those in the Archive storage class).\n"}, {"id": 105, "text": "\u2022\nOn this slide, the bucket stores two objects: object 1 and object 2. As versioning \nhas been enabled for this bucket, the current version of object 1 is version 3. By \nquerying the historical records, you can find that version 1 and version 2 are the \nnoncurrent versions of object 1. In addition, the current version of object 2 has \nbeen deleted because there is a delete marker. By querying the historical records, \nyou can find that version 1 is the noncurrent version of object 2.\n"}, {"id": 106, "text": "\u2022\nServer-side encryption with KMS-managed keys (SSE-KMS)\n\u25ab\nWith this method, you need to create a key using Key Management Service \n(KMS) or use the default key provided by KMS. The KMS key is then used \nfor server-side encryption when you upload objects to OBS.\n\u2022\nServer-side encryption with customer-provided keys (SSE-C)\n\u25ab\nFor this method, the customer-provided keys and their MD5 values are used \nfor server-side encryption.\n"}, {"id": 107, "text": "\u2022\nEvents supported by OBS are listed as follows:\n\u2022\nOBS provides APIs such as PUT, POST, and COPY for uploading objects. You can \nconfigure event types corresponding to these APIs. Then, when you use such an \nAPI to upload an object, you will receive a notification. You can also configure the \nObjectCreated:* event type to obtain all object upload notifications.\n\u25ab\nObjectCreated:* (all upload operations)\n\u25ab\nObjectCreated:Put (uploading an object)\n\u25ab\nObjectCreated:Post (uploading an object with a browser)\n\u25ab\nObjectCreated:Copy (copying an object)\n\u25ab\nObjectCreated:CompleteMultipartUpload (merging parts)\n\u2022\nBy configuring the ObjectRemoved event type, you can receive a notification \nwhen one or more objects are removed from a bucket.\n\u2022\nBy configuring the ObjectRemoved:Delete event type, you can receive a \nnotification when an object is deleted or an object version is permanently deleted. \nBy configuring the ObjectRemoved:DeleteMarkerCreated event type, you can \nreceive a notification when a delete marker is added to an object. You can also \nuse ObjectRemoved:* to receive a notification each time an object is deleted.\n\u25ab\nObjectRemoved:* (all delete operations)\n\u25ab\nObjectRemoved:Delete (deleting an object)\n\u25ab\nObjectRemoved:DeleteMarkerCreated (adding a delete marker to an object)\n"}, {"id": 108, "text": "\u2022\nThe OBS big data solution is designed for a variety of scenarios, including storage \nand analysis of massive amounts of data, query of historical data details, analysis \nof a large number of behavior logs, and analysis and statistics of public \ntransactions.\n\u2022\nTypical scenarios of storage and analysis of massive amounts of data include:\n\u25ab\nStorage for petabytes of data, batch data analysis, and response for data \ndetail queries in seconds\n\u2022\nTypical scenarios of query of historical data details include:\n\u25ab\nTransaction audit, device energy consumption analysis, trail playback, \ndriving behavior analysis, and fine-grained monitoring\n\u2022\nTypical scenarios of analysis of a large number of behavior logs include:\n\u25ab\nAnalysis of learning habits and operation logs, as well as analysis and query \nof system operation logs\n\u2022\nTypical scenarios of analysis and statistics of public transactions include:\n\u25ab\nCrime tracking, associated case queries, traffic congestion analysis, and \nscenic spot popularity statistics\n"}, {"id": 109, "text": "\u2022\nEVS: Raw disk spaces are mapped entirely to hosts or VMs. You can format the \ndisk with any file system and use it.\n\u2022\nSFS: Like a shared folder, for example, a remote shared directory in Windows, the \nfile system already exists, and you can directly store data to the file system.\n\u2022\nOBS: Each piece of data corresponds to a unique ID. Object storage does not \nhave the directory structure similar to file storage. Data is stored in a flat \nstructure, and you can locate data by object ID.\n"}, {"id": 110, "text": "\u2022\nVarious specifications:\n\u25ab\nHigh I/O storage is suitable for scenarios that require high performance, \nhigh read/write speed, and real-time data storage.\n\u25ab\nUltra-high I/O storage is excellent for read/write-intensive scenarios that \nrequire extremely high performance and read/write speed, and low latency.\n\u2022\nElastic scalability:\n\u25ab\nOn-demand capacity expansion: Storage pools can be expanded based on \nservice requirements.\n\u25ab\nLinear performance scaling: DSS disks can be expanded while services are \nrunning, and linear performance increase can be achieved.\n\u2022\nSecurity and reliability:\n\u25ab\nThree-copy redundancy ensures 99.9999999% data durability.\n\u25ab\nBoth system disks and data disks can be encrypted for improved data \nsecurity.\n\u2022\nBackup and restore:\n\u25ab\nCBR allows you to create backups for DSS disks and restore the disk data \nusing backups. Backups can be created for a DSS disk, maximizing data \nsecurity and integrity and ensuring service security.\n"}, {"id": 111, "text": "\u2022\nEnterprise customers: IDC hosting customers, securities settlement companies, \nand more.\n\u2022\nCustomers use EVS shared storage and DSS dedicated storage for their services. \nEVS provides storage for enterprise OA, development and testing, and databases. \nDSS provides storage for the mission-critical services running on BMSs.\n"}, {"id": 112, "text": "\u2022\nCDN facilitates whole network access across carriers and regions. Websites \ncannot be accessed due to various factors, such as regional ISP limitation and \negress bandwidth limitation. CDN can cover global lines. It cooperates with \ncarriers to deploy Internet Data Center resources and edge nodes on networks of \nbackbone node providers. CDN helps customers make the most of bandwidth \nresources and balance origin server traffic.\n\u2022\nLoad balancing and distributed storage of CDN enhance website security and \nreliability to cope with most Internet attacks. The anti-attack system can also \nprotect websites from malicious attacks.\n\u2022\nCDN supports remote backups. When a server is faulty, the system switches \nservices to other adjacent healthy server nodes. The reliability is close to 100%, \nand websites never breaks down.\n\u2022\nWith CDN, customers can delivery content to global users without worrying \nabout server investments, subsequent hosting and O&M, image synchronization \nbetween servers, or O&M personnel. CDN helps customers save human, energy, \nand financial resources.\n\u2022\nCDN enables customers to stay focused on their core services. CDN vendors \ndeliver one-stop services, including content delivery, cloud storage, big data, and \nvideo cloud services. In addition, CDN vendors provide 24/7 O&M and monitoring \nto ensure network connectivity at any time.\n"}, {"id": 113, "text": "\u2022\nHuawei Cloud CDN caches origin content on edge nodes across the globe. When \na user accesses the content, the user does not need to retrieve it from the origin \nserver. Based on a group of preset policies (including content types, geological \nlocations, and network loads), CDN provides the user with the IP address of a \nCDN node that responds the fastest, enabling the user to obtain the requested \ncontent faster than would have otherwise been possible.\n\u2022\nHuawei Cloud CDN has over 2,000 edge nodes in the Chinese mainland and over \n800 edge nodes outside the Chinese mainland. The network-wide bandwidth is at \nleast 150 Tbit/s. Edge nodes are deployed on networks of top carriers in China \nsuch as China Telecom, China Unicom, China Mobile, and China Education and \nResearch Network (CERNET), as well as many small- and medium-sized carriers. \nUp to now, Huawei Cloud CDN covers more than 130 countries and regions, \nconnecting to over 1,600 carrier networks. CDN precisely schedules user requests \nto the most appropriate node for efficient and reliable acceleration.\n"}, {"id": 114, "text": "\u2022\nDynamic data: web program\n\u2022\nStatic data: image, video, and audio\n"}, {"id": 115, "text": "\u2022\nCDN is widely used in security-sensitive communications on the World Wide Web, \nsuch as online payment.\n"}, {"id": 116, "text": "\u2022\nRange information specifies the positions of the first and last bytes for the data \nto be returned. For example, Range: bytes=0-100 indicates that the first 101 bytes \nof the file are required.\n\u2022\nRange-based retrieval shortens the distribution time of large files, improves \nretrieval efficiency, and reduces content retrieval consumption.\n"}, {"id": 117, "text": "\u2022\nBackups:\n\u25ab\nA backup is a copy of a particular chunk of data and is usually stored \nelsewhere so that it can be used to restore the original data in the event of \ndata loss.\n\u2022\nVaults:\n\u25ab\nCBR uses vaults to store backups. Before creating a backup, you need to \ncreate at least one vault and associate the resource you want to back up \nwith the vault. Then generated resource backups are stored in the \nassociated vault.\n\u25ab\nVaults can be either backup vaults or replication vaults. Backup vaults store \nresource backups, whereas replication vaults store replicas of backups.\n\u25ab\nThe backups of different types of resources must be stored in different \ntypes of vaults.\n\u2022\nPolicies: consist of backup policies and replication policies.\n\u25ab\nBackup policies: To perform automatic backups, configure a backup policy \nby setting the execution times of backup tasks, the backup frequency, and \nretention rules, and then apply the policy to a vault.\n\u25ab\nReplication policies: To automatically replicate backups or vaults, configure \na replication policy by setting the execution times of replication tasks, the \nreplication frequency, and retention rules, and then apply the policy to a \nvault. Backup replicas must be stored in replication vaults.\n"}, {"id": 118, "text": "\u2022\nThe following are the types of CBR backups:\n\u25ab\nCloud disk backup. This type of backup provides snapshot-based data \nprotection for EVS disks.\n\u25ab\nCloud server backup. This type of backup uses the consistency snapshot \ntechnology for disks to protect data of ECSs and BMSs. The backups of \nservers without deployed databases are common server backups, and those \nof servers with deployed databases are application-consistent backups.\n\u25ab\nSFS Turbo backup. This type of backup protects data of SFS Turbo file \nsystems.\n\u25ab\nHybrid cloud backup. This type of backup protects data of on-premises \nOceanStor Dorado storage systems and VMware VMs by storing their \nbackups on the cloud. You can manage the backups on CBR Console.\n"}, {"id": 119, "text": "\u2022\nTwo backup options can also be used together if needed. For example, users can \nassociate all servers or file systems with a vault and then apply a backup policy \nto the vault for periodic backups, and manually perform one-time backups for \nthe most important servers or file systems to further ensure data security.\n"}, {"id": 120, "text": "\u2022\nRPO = 0: With the accumulation of 8+ years of Huawei-developed technology, \nthe storage-layer, synchronous replication ensures zero data loss.\n\u2022\nMinute-level RTO: If a disaster occurs, a failover can be completed within \nminutes.\n\u2022\nOnline DR drill: DR drills can be performed at any time when needed to verify the \nfeasibility and effectiveness of the DR solution.\n\u2022\nThree-step DR: Cloud DR can be completed in only three steps: creating a \nprotection group, creating protected instances, and enabling protection.\n\u2022\nOne-click DR switchover: SDRS supports one-click DR switchover. After a \nswitchover is complete, services can be quickly recovered by manually starting \nthe ECSs.\n\u2022\nWorkload-level protection: SDRS supports DR protection by workload, that is, \nadding ECSs running the same workload to the same protection group.\n\u2022\nNo additional plug-ins: The deployment is simply. No additional plug-in is \nrequired on DR site ECSs.\n\u2022\nNo fees for DR site ECSs: If everything is working normally, ECSs at the DR site \nare not started and are not charged.\n\u2022\nTCO reduced by 60%: SDRS saves the DR TCO by 60% compared with the \ntraditional DR solutions, reducing the costs in hardware devices, power supply, \nO&M, and more.\n\u2022\nAutomatic network migration: After a switchover is complete, the IP address, \nMAC address, and EIP of an ECS are automatically migrated, freeing you from \nreconfigure them again.\n"}, {"id": 121, "text": "\u2022\nABCD\n\u2022\nAB\n"}, {"id": 122, "text": "\u2022\nDiscussion 1:\n\u25ab\nCloud vendor offers management.\n\u25ab\nSelf-built storage is managed by users.\n\u2022\nDiscussion 2:\n\u25ab\nSecurity: Static data is protected using encryption, and dynamic data is \nprotected using HTTPS access. If data is stored in OBS, data in the bucket \ncan be protected by bucket policy.\n\u25ab\nCost: Based on the data usage, select an appropriate type of storage to \nreduce storage costs.\n\u25ab\nReliability: Determine whether storage redundancy is required based on \nservice requirements. If redundancy is not used, cross-region replication can \nbe used to back up data.\n\u25ab\nPerformance: In large service volume scenarios, considering the high I/Os, \nSSD-based disk types are recommended.\n\u25ab\nScalability: Determine appropriate storage classes based on the usage of \ndata to be stored. Capacity expansion and reduction methods may vary \nwith storage classes.\n"}, {"id": 123, "text": "\u2022\nEnterprises are facing explosive data growth and ever more diverse types of data \napplications. Large-scale cloud transformation has been changing traditional \nbusiness models.\n"}, {"id": 124, "text": "\u2022\nCompared with traditional databases, cloud databases have the following \nadvantages:\n\u25ab\nEase-of-use: Each cloud database is provided as a cloud service. You can \neasily create and run it on the cloud.\n\u25ab\nScalable: Each cloud database service is an open-source database with \ndecoupled storage and compute for more flexible scaling.\n\u25ab\nCost-effective: Compared with a traditional database, using a cloud \ndatabase service saves money on software and hardware, and pay-per-use \nbilling helps you reduce the total cost of ownership (TCO).\n"}, {"id": 125, "text": "\u2022\nA relational database organizes data using a relational model. A relational model \nis a two-dimensional table model, and a relational database is a data \norganization consisting of two-dimensional tables and their relationships.\n\u2022\nA non-relational database is a non-relational, distributed data storage system \nthat does not comply with ACID properties.\n\u2022\nTypical products:\n\u25ab\nRelational databases: SQL Server, MySQL, and PostgreSQL\n\u25ab\nNon-relational databases: Redis, Memcached, and MongoDB\n"}, {"id": 126, "text": "\u2022\nDatabases are classified as either relational databases or non-relational \ndatabases.\n\u2022\nHuawei Cloud relational database services include RDS for MySQL, RDS for \nPostgreSQL, RDS for SQL Server, GaussDB(for openGauss), and GaussDB(for \nMySQL).\n\u2022\nHuawei Cloud non-relational database services include GaussDB(for Mongo), \nGaussDB(for Cassandra), GaussDB(for Redis), GaussDB(for Influx), DDS, DCS.\n\u2022\nDatabase ecosystem services include DDM, DRS, and UGO.\n\u2022\nDistributed Database Middleware (DDM) breaks through the capacity and \nperformance bottlenecks that plague traditional databases and addresses \ndistributed scaling issues so you can handle highly concurrent access to massive \nvolumes of data.\n\u2022\nDDM uses decoupled storage and compute. It provides functions such as \ndatabase and table sharding, read/write splitting, elastic scaling, and sustainable \nO&M. Management of instance nodes has no impact on your workloads. You can \nperform O&M on your databases and read and write data from and to them on \nthe DDM console, just like as operating a single-node MySQL database.\n\u2022\nAdvantages: automatic database and table sharding, read/write splitting, and \nelastic scaling\n"}, {"id": 127, "text": "\u2022\nScenarios:\n\u25ab\nSmall systems and peripheral applications: 100,000 QPS, small-scale OLTP, \nand tens to hundreds of GB of data\n\u25ab\nEnterprise-class applications: millions of queries per second, medium-scale \nOLTP, and terabytes, or even dozens of terabytes of data\n\u25ab\nMission-critical and high-concurrency systems: ultra-large OLTP, \nOLTP/OLAP, cloud-native distributed, dozens of terabytes of data\n"}, {"id": 128, "text": "\u2022\nSecurity\n\u25ab\nRunning a DB instance in a VPC improves security. You can configure \nsubnets and security groups to control access to DB instances.\n\u2022\nAccess control\n\u25ab\nWhen you create an RDS DB instance, an account is automatically created. \nTo separate permissions, you can create IAM users and assign permissions \nto them as needed.\n\u2022\nTransmission encryption\n\u25ab\nYou can download the Certificate Agency (CA) certificate from the console \nand upload it when connecting to a database for authentication.\n\u2022\nStorage encryption\n\u25ab\nRDS encrypts data before storing it. Encryption keys are managed by Key \nManagement Service (KMS).\n\u2022\nData deletion\n\u25ab\nAutomated backup data and the data stored in the disks associated with \nyour instance can be securely deleted. You can restore a deleted DB \ninstance from a manual backup or rebuild the DB instance in the recycle bin \nduring the retention period.\n"}, {"id": 129, "text": "\u2022\nRDS for MySQL\n\u25ab\nIt uses a stable architecture and supports a wide range of web applications.\nIt is cost-effective and often preferred by small and medium enterprises.\n\u25ab\nA web-based console is available for you to monitor performance metrics \nso if there is an issue, you can identify it and take appropriate measures as \nsoon as possible.\n\u25ab\nYou can flexibly scale resources to meet business needs and pay for only \nwhat you use.\n\u2022\nRDS for PostgreSQL\n\u25ab\nRDS for PostgreSQL supports the postgis plugin and provides excellent \nspatial performance.\n\u25ab\nRDS for PostgreSQL is a cost-effective solution suitable for many business \nscenarios. You can flexibly scale resources based on your needs and pay for \nonly what you use.\n\u2022\nRDS for SQL Server\n\u25ab\nRDS for SQL Server is reliable, scalable, inexpensive, and easy to manage. It \nsupports high availability for your applications with automatic database \nfailover that completes within several seconds. It also provides multiple \noptions for backing up your data.\n"}, {"id": 130, "text": "\u2022\nDatabase engine versions: MySQL 5.6, 5.7, and 8.0\n\u2022\nData security: Multiple security policies protect databases and data privacy.\n\u2022\nDatabase reliability: Three-copy data storage ensures up to 9 nines of database \ndata reliability and up to 11 nines of backup data reliability.\n\u2022\nHigh availability (intra-city disaster recovery): Primary/standby DB instances are \ndeployed within an AZ or across AZs, ensuring service availability over 99.95%.\n\u2022\nInstance access: Multiple access methods are supported. You can use floating IP \naddresses, public IP addresses, or VPNs.\n\u2022\nInstance management: You can add, delete, modify, query, and reboot your DB \ninstance on the console.\n\u2022\nElastic scaling: Horizontal scaling: Read replicas can be created (up to five for \neach instance) or deleted. Vertical scaling: DB instance classes can be modified \nand storage space can be scaled up to 10 TB.\n\u2022\nBackup and restoration: \n\u25ab\nFor backup, there are automated backup, manual backup, full backup, and \nincremental backup. Backups can be added, deleted, queried, or replicated.\n\u25ab\nFor restoration, data can be restored to any point in time within the backup \nretention period, or to a new or an original DB instance. The backup \nretention period is up to 732 days.\n"}, {"id": 131, "text": "\u2022\nWhen creating a DB instance, you can select Primary/Standby as the instance \ntype. If a primary instance fails, RDS automatically switches to the standby \ninstance. If the standby instance also fails, a primary/standby instance in another \nAZ will automatically take over the workloads.\n\u2022\nEach RDS DB instance supports up to five read replicas and can scale out with \nDistributed Database Middleware (DDM) to further increase capacity. Write \nrequests are routed to the primary instance and read requests are routed to read \nreplicas.\n\u2022\nThe primary and standby DB instances share the same virtual IP address (VIP) for \ncommunication with external systems. The DB instance associated with the VIP is \nthe primary instance. If the primary instance is unavailable, RDS automatically \nassociates the VIP with the standby instance and promotes it to be the new \nprimary instance. Associating the VIP with the standby instance can be completed \nin seconds. There is no downtime. The switchover is imperceptible to users.\n\u2022\nConstraints: You can create read replicas only after purchasing a DB instance.\n"}, {"id": 132, "text": "\u2022\nAfter read replicas are created and read/write splitting is enabled for your DB \ninstance, RDS will distinguish between read and write requests. Write requests \nare routed to the primary instance. Read requests are distributed to the read \nreplicas.\n"}, {"id": 133, "text": "\u2022\nThe automated backup retention period (1-732) is configurable.\n"}, {"id": 134, "text": "\u2022\nDB engine versions: 9.5, 9.6, 10.0, 11, and 12\n\u2022\nSecurity: Multiple security policies protect databases and data privacy.\n\u2022\nData migration: There is online and offline migration to the cloud, to on-\npremises, and across clouds.\n\u2022\nHA: Data is automatically synchronized from a primary DB instance to a standby \nDB instance. If the primary DB instance fails, workloads are quickly and \nautomatically switched over to the standby DB instance.\n\u2022\nMonitoring: Key performance metrics of RDS DB instances are monitored. These \nmetrics include the CPU usage, memory usage, storage space usage, I/O activity, \ndatabase connections, QPS, TPS, buffer pools, and read/write activities.\n\u2022\nHorizontal scaling: Read replicas (up to five for each instance) can be created or \ndeleted. Vertical scaling: DB instance classes can be modified and storage space \ncan be scaled without downtime.\n\u2022\nBackup and recovery: RDS supports automated and manual backups along with \npoint-in-time recovery (PITR).\n"}, {"id": 135, "text": "\u2022\nRDS for PostgreSQL supports cross-AZ HA. If the primary instance fails, the fault \ndetection module attempts to start it three times. If the primary instance still \ncannot be started, a failover is automatically performed and completed within \nseconds. The standby instance is promoted to primary and read replicas are \nautomatically associated with the new primary instance.\n\u2022\nRDS provides data backup and restoration. You can set an automated backup \npolicy to back up your data daily. Automated backups can be retained for up to \n732 days. An incremental backup is performed every 5 minutes for data \nconsistency.\n\u2022\nIf data is lost or deleted by mistake, you can restore the database to any point in \ntime.\n\u2022\nBackup files are stored in OBS. OBS has no capacity upper limit and provides \n99.999999999% data reliability.\n"}, {"id": 136, "text": "\u2022\nNote: Both of PostgreSQL and MySQL can be used in most scenarios.\n\u25ab\nWhen you are choosing a database, database use and design habits need to \nbe considered. For example, some gaming and Internet companies just use \ndatabases to store data. Both PostgreSQL and MySQL are fine for this. But \nif many of your system's functions depend on more varied database\nfeatures, PostgreSQL is recommended. It is a stable and reliable open-\nsource database that is a good choice for many companies.\n\u25ab\nIf your current DB system only processes transactions, choose a database \nusing the same engine. If your database requires both transaction and \nanalytic processing, PostgreSQL is recommended because it provides \nexcellent analytical performance.\n\u25ab\nIf many stored procedures are used, PostgreSQL is recommended. Use \nwhatever your company is already used.\n\u25ab\nIf your application has to access heterogeneous databases, PostgreSQL is \nrecommended because it provides foreign data wrappers, which allows \nusers to access heterogeneous data using SQL statements.\n\u25ab\nPostgreSQL is recommended for complex data types, such as complex \narrays, spatial data, network data, JSON data, XML data, and certain \ncustom types.\n\u2022\nPostgreSQL is recommended if your application requires geographic, spatial, \nimage, time series, multi-dimensional data, access to heterogeneous DB, machine \nlearning, text retrieval, or word segmentation and you do not want another \ndedicated database.\n"}, {"id": 137, "text": "\u2022\nShared DFV storage:\n\u25ab\nGaussDB(for MySQL) provides a shared storage pool. When adding a read \nnode, you only need to add one compute node, and no additional storage is \nrequired. If there are more read-only nodes, more storage costs will be \nsaved.\n\u2022\nActive-active architecture:\n\u25ab\nGaussDB(for MySQL) does not support standby instances. All read replicas \nare active, offloading read traffic from the primary node and improving \nresource utilization.\n\u2022\nA \"logs as data\" architecture:\n\u25ab\nGaussDB(for MySQL) does not use page flushing or double writes. All \nupdate operations are recorded in logs to save bandwidth.\n"}, {"id": 138, "text": "\u2022\nIn TPC-H testing, if a DB instance (with 32 vCPUs and 256 GB of memory) \nhandles 100 GB of data, its performance is improved by 8x when handling of 16-\nthread concurrency requests.\n"}, {"id": 139, "text": "\u2022\nLinear expansion of GaussDB(for MySQL) read and write performance:\n\u2022\nYou do not need to re-divide storage for the new nodes because GaussDB(for \nMySQL) uses DFV distributed storage. The new nodes can share the same storage \nas the existing nodes.\n"}, {"id": 140, "text": "\u2022\nWhen data is restored, GaussDB(for MySQL) can provide services before the \nrestoration is complete. In contrast, traditional databases need to wait for all \ndata to be fully restored before they can provide services again.\n"}, {"id": 141, "text": "\u2022\nHigh security:\n\u25ab\nGaussDB(for openGauss) provides security equal to that of top commercial \ndatabases using dynamic data masking, transparent data encryption (TDS), \nrow-level access control, and encrypted computing. This feature meets the \ncore data security requirements of enterprises and finance institutions.\n\u2022\nComprehensive tools\n\u25ab\nGaussDB(for openGauss) can be deployed in the Huawei Cloud and Huawei \nCloud Stack for commercial use. It can also work with ecosystem tools such \nas DAS, UGO and DRS to make database development, O&M, tuning, \nmonitoring, and migration easier.\n\u2022\nIn-house, full-stack development\n\u25ab\nDeveloped based on Kunpeng ecosystem, GaussDB(for openGauss) \nperformance is continuously optimized to meet ever-increasing demands \nacross a wide range of scenarios.\n\u2022\nOpen-source ecosystem\n\u25ab\nThe primary/standby version is available for you to download from the \nopenGauss community.\n"}, {"id": 142, "text": "\u2022\nETCD: Editable Text Configuration Daemon. It ensures data consistency.\n\u2022\nCMS: It manages cluster and controls primary/standby switchover to ensure high \navailability.\n"}, {"id": 143, "text": "\u2022\nDatabase type and versions: compatible with MongoDB 4.0 and 4.2.\n\u2022\nData security: Multiple security policies protect databases and data privacy.\n\u2022\nData reliability: Three-copy data storage ensures up to 9 nines of database data \nreliability and up to 12 nines of backup data durability.\n\u2022\nHigh availability (intra-city disaster recovery): Cluster and replica set instances \ncan be deployed within an AZ or across three AZs, ensuring service availability \nover 99.95%.\n\u2022\nDB instance monitoring: DDS monitors key performance metrics of DB instance \nOSs and DB engines, including CPU usage, memory usage, storage space usage, \nI/O activity, and database connections.\n\u2022\nElastic scaling: \n\u25ab\nHorizontal scaling: Shards can be created (up to 32 for each instance) or \ndeleted. You can also create 7-node replica sets and read replicas. \n\u25ab\nVertical scaling: DB instance classes can be modified and storage space can \nbe scaled up to 32 * 2 TB.\n\u2022\nBackup and restoration: \n\u25ab\nBackup: Multiple backup methods are available, such as automated backup, \nmanual backup, full backup, and incremental backup. Backup files can be \nadded, deleted, queried, or replicated. \n\u25ab\nRestoration: Data can be restored to any point in time within the backup \nretention period and can be backed up to the original DB instance or to a \nnew one. The backup retention period is up to 732 days.\n"}, {"id": 144, "text": "\u2022\nGaming:\n\u2022\nPlayer information, such as player items and bonus points, is stored in DDS \ndatabases. During peak hours, DDS cluster instances can handle large amounts \nof concurrent requests. DDS clusters and replica sets provide high availability to \nensure games are stable in high-concurrency scenarios.\n\u2022\nIn addition, DDS is compatible with MongoDB and provides a no-schema mode, \nwhich means you do not have to change the table structure when the game play \nmode changes. DDS can easily meet many flexible gaming requirements. You can \nstore structured data with fixed patterns in RDS, data with flexible patterns in \nDDS, and hot data in GaussDB(for Redis) to speed up data access and reduce \ndata storage costs.\n"}, {"id": 145, "text": "\u2022\nCassandra APIs for:\n\u25ab\nWide-column data model\n\u25ab\nUltra-high write performance, making GaussDB NoSQL a huge fit for IoT\nand financial fraud detection scenarios\n\u2022\nMongoDB APIs for:\n\u25ab\nDocument-oriented data model\n\u25ab\nOutstanding read/write performance, low latency, and high reliability\n\u2022\nRedis APIs for:\n\u25ab\nRedis databases with decoupled storage and compute\n\u25ab\nHigh reliability, scalability, and cost-effectiveness\n\u2022\nInfluxDB APIs for:\n\u25ab\nEfficiently handling time series data\n\u25ab\nHigh write performance and compression ratio\n"}, {"id": 146, "text": "GaussDB(for Redis) has the following features:\n\u2022\nHigh cost-effectiveness\n\u25ab\nThanks to shared storage, GaussDB(for Redis) is able to inexpensively \nprocess massive amounts of data.\n\u25ab\nAll data is stored in disks with cold and hot data separated. Hot data can \nbe read from the cache directly, making programs run fast.\n\u2022\nHitless scaling\n\u25ab\nRocksDB is customized to allow storage to be scaled up in seconds.\n\u25ab\nScaling is fast and smooth because no data needs to be migrated.\n\u25ab\nProxies ensure that upper-layer applications are not affected by data \nsharding in the storage layer.\n\u2022\nCold and hot data separation\n\u25ab\nHot data is loaded to the memory and cold data is stored persistently, so \nthere is no need to use an extra MySQL database.\n\u25ab\nCold and hot data is automatically exchanged, making coding easier than \nbefore.\n"}, {"id": 147, "text": "GaussDB(for Redis) is Redis-compatible and can store a large amount of data \ninexpensively and reliably, so it is a great fit for persistent storage scenarios.\n"}, {"id": 148, "text": "Gaming:\nGaussDB(for Mongo) is compatible with MongoDB and allows you to keep \ntrack of gaming data like equipment or points earned. Adding compute nodes \nis so easy, making GaussDB(for Mongo) an excellent choice for high-\nconcurrency scenarios often involved in online gaming.\n"}, {"id": 149, "text": "\u2022\nDatabase Migration Method:\n\u2022\nIn most cases, you can migrate databases using both UGO and DRS. When \nmigrating databases from on-premises or other clouds to Huawei Cloud, you can \nuse UGO to analyze the source databases and migrate the databases based on \nthe actual scenario and the suggestions provided by UGO. You can also use the \nfull + incremental migration provided by DRS to migrate data from one database \nto another.\n"}, {"id": 150, "text": "\u2022\nEasy to use\n\u25ab\nTraditional migration requires professional technical personnel and \nmigration procedures are complex.\n\u2022\nFast setup\n\u25ab\nTraditional migration takes several days, weeks, or even months to set up.\n\u2022\nLow costs\n\u25ab\nTraditional migration is expensive and there is no pay-per-use pricing.\n\u2022\nSecure\n\u25ab\nTraditional migration involves downtime and if there is a migration failure, \ndata may be lost.\n"}, {"id": 151, "text": "\u2022\nReal-time migration can be performed over different networks, such as public \nnetworks, VPCs, VPNs, and Direct Connect. With these network connections, \nmigration can be performed between different cloud platforms, from on-\npremises databases to cloud databases, or between cloud databases across \nregions.\n"}, {"id": 152, "text": "\u2022\nUGO has been deployed commercially only in CN South-Guangzhou and AP-\nSingapore.\n"}, {"id": 153, "text": "\u2022\nCore Features 1: Source Database Profiling\n\u25ab\nSource database profiling uses a massive collection of actual service \nscenarios as samples and key database metrics as features for training to \npresent a picture of the source database. Source database profiling provides \na basis for a fast follow-up precision analysis of source database' s \napplication scenarios and user habits.\n"}, {"id": 154, "text": "\u2022\nCore Features 2: Recommended Specifications for Target Databases\n\u25ab\nUGO recommends different types of target databases by priority based on \nsource database profiling, compatibility, performance, object complexity, \nand application scenarios. UGO also intelligently recommends specifications \nand estimates costs of the target databases.\n"}, {"id": 155, "text": "\u2022\nCore Features 3: Target Database Compatibility Analysis\n\u25ab\nUGO analyzes the compatibility of up to 14 core object types between \nsource and target databases based on source database profile and on a \nhigh syntax conversion rate. UGO delivers an excellent conversion rate \nbased on hundreds of millions of samples.\n"}, {"id": 156, "text": "\u2022\nCore Features 4: Workload evaluation\n\u25ab\nThe cost of labor for a typical database migration is used as a baseline, and \nthen the workloads involved in automatic database migration are added in. \nAdditionally, UGO evaluates the migration workloads based on the amount \nof code involved, the conversion rate, and how hard it will be to modify \nincompatible objects.\n"}, {"id": 157, "text": "\u2022\nCore Features 5: Database schema migration\n\u25ab\nAfter evaluating the source database, UGO allows users to filter the objects \nto be migrated, and then verifies and migrates the objects. Failed objects \nare modified and the process is repeated until all objects pass.\n"}, {"id": 158, "text": "\u2022\nABC\n\u2022\nABCD\n"}, {"id": 159, "text": "\u2022\nDiscussion 1:\n\u25ab\nCloud databases are managed by cloud vendors.\n\u25ab\nOn-premises databases are managed by users.\n\u2022\nDiscussion 2:\n\u25ab\nSecurity: Do not open external network access to databases. Open only \ninternal ports. Use cloud security services to prevent malicious attacks and \nperform data DR drills.\n\u25ab\nCosts: Evaluate the costs and performance of different engines. When the \nnumber of access requests is small, reduce the cluster scale.\n\u25ab\nReliability: Preferentially select primary/standby instances.\n\u25ab\nPerformance: Select a proper DB engine based on data types. Design the \nassociation between tables. Use caches to improve the access speed.\n\u25ab\nScalability: Select cloud-native databases to facilitate future data expansion. \nConsider intra-region and cross-region data backup.\n"}, {"id": 160, "text": "\u2022\nCSA: Cloud Security Alliance\n"}, {"id": 161, "text": "\u2022\nConsole: a visualized management platform, where you can apply configurations \nin a centralized manner and view the defense status and scan results of servers in \na region.\n\u2022\nThe HSS cloud protection center receives these configuration information and \ndetection tasks and then forwards them to the Agent installed on the server. \nAgents block attacks based on security policies and scan all servers every early \nmorning; monitor the security status of servers; and report the collected server \ninformation (including non-compliant configurations, insecure configurations, \nintrusion traces, software list, port list, and process list) to the cloud protection \ncenter.\n\u2022\nThe cloud protection center presents analysis results as reports on the console.\n\u2022\nOther functions:\n\u25ab\nWeb Tamper Protection (WTP): WTP can detect and prevent tampering of \nfiles in specified directories in real time, including web pages, documents, \nand images, and quickly restore them using valid backup files.\n\u25ab\nAdvanced defense: application recognition service (ARS), file integrity \nmonitoring, and ransomware protection.\n\u25ab\nUnified multi-cloud management: HSS can manage hundreds of thousands \nof servers running mainstream OSs, such as Linux and Windows, no matter \nwhat cloud they are deployed and which architectures (x86 or Arm) they \nare using.\n"}, {"id": 162, "text": "\u2022\nHSS provides comprehensive and effective security solutions for 230,000 \ncompanies and individual users in government, Internet, and education industries \nin and outside China. HSS provides comprehensive risk prevention and real-time \nprotection capabilities, periodically generating security reports to meet DJCP \nrequirements. HSS implements comprehensive protection by providing \nprevention, detection, and operations functions.\n"}, {"id": 163, "text": "\u2022\nWith CGS, you can detect and eliminate risks in your containers and images \nthroughout their lifecycles, including building, distributing, and running.\n"}, {"id": 164, "text": "\u2022\nCBH also enables collaborative O&M tasks and batch management of servers \nand databases.\n"}, {"id": 165, "text": "\u2022\nHow an administrator creates access policies:\n\u25ab\nAdding resources to CBH: The administrator adds resources to be managed. \nThey can add a wide range of resources, such as servers, network devices, \nsecurity devices, applications, and databases. CBH allows users to edit \nresource details, including the system type, department name, resource \nname, resource address, protocol type, and applications.\n\u25ab\nCreating user accounts: The administrator creates user accounts. A user \naccount is a unique account that is used by a specific O&M engineer to log \nin to the CBH system. Each user account maps to a real O&M individual.\n\u25ab\nAdding resource accounts to CBH: The administrator adds resource \naccounts to the CBH system. A resource account is used to log in to a \nspecific resource managed in CBH. Each resource account has its own \nusername and a password. Resource accounts can be used for automatic, \nmanual, or semi-automatic logins. A regular resource account can be \nescalated to a privileged account, or even given sudo privileges. Beyond \nthat, passwords of resource accounts can be updated by CBH periodically.\n\u25ab\nCreating access control policies: The administrator creates access policies \nbased on combinations of time ranges, primary accounts, resource \naccounts, and permissions.\n\u25ab\nFull-process behavior audit: CBH automatically logs the administrator's \nbehavior, including how they manage resources, system users, and policies, \nfor monitoring and audits.\n"}, {"id": 166, "text": "\u2022\nCustomers can use IAM accounts to control who can access a CBH system. The \nadministrator of a CBH system can create users in the CBH system and assign \nrole-based permissions. This figure shows account permissions assigned to \ndifferent O&M personnel. Only the administrator has the permissions needed to \nmanage roles in the CBH system.\n\u2022\nStrict compliance audit:\n\u25ab\nCBH gives the customers the ability to establish a sound O&M audit system, \nmaking it easier for them to comply with regulatory requirements no \nmatter what industry they are in and no matter how strict the requirements \nare. CBH provides a single point of entry for cloud resource management \nthat enables customers to centrally manage accounts and resources, grant \npermissions by department, configure multi-level review for operations on \nmission-critical assets, and require double approvals for sensitive \noperations.\n\u2022\nEfficient and stable O&M\n\u25ab\nDuring remote O&M, CBH hides the actual IP addresses so the details of \nremotely managed assets can be kept secure. CBH provides comprehensive \nO&M logs that let customers can effectively monitor and audit the \noperations of O&M personnel both inside and outside of their \norganizations, reducing network security incidents and keeping service \nsystems stable.\n\u2022\nManagement of a large number of assets and personnel:\n\u25ab\nCBH provides a system to securely manage a large number of O&M \naccounts and a wide range of resources. It also allows O&M personnel to \naccess resources usin"}, {"id": 167, "text": "a large number of assets and personnel:\n\u25ab\nCBH provides a system to securely manage a large number of O&M \naccounts and a wide range of resources. It also allows O&M personnel to \naccess resources using single sign-on (SSO) tools, improving the O&M \nefficiency. CBH uses fine-grained permissions control so that all operations \non a managed resource are recorded and operations of all O&M staff are \nauditable. Any O&M incidents are traceable, making it easier to locate the \noperators.\n"}, {"id": 168, "text": "\u2022\nThe solid line indicates the access traffic.\n\u2022\nDemilitarized Zone (DMZ) is a special network area different from the external \nnetwork or internal network. Generally, the DMZ houses public servers that do \nnot contain confidential information, such as web servers, email servers, or FTP \nservers. Users from the external network can only access the services in the DMZ, \nbut cannot access the information on the internal network. So, the information \non the internal network cannot be impacted even if the servers in the DMZ were \nattacked.\n"}, {"id": 169, "text": "\u2022\nIn an SQL injection attack, an attacker tricks the database server into executing \nunauthorized queries. Attackers use exploits or logic flaws in application code to \nbypass security controls. They manipulate the database server behind a web \napplication, tricking the system into doing what they want by executing specially \nconstructed SQL statements.\n\u2022\nCross-Site Scripting (XSS) is a common type of web security vulnerability. \nAttackers can exploit XSS vulnerabilities to inject malicious scripts into web pages \nthat are provided for other users. In most types of attacks, there are only two \nparties involved: the attacker and the site they attack, but in an XSS attack, web \nclients, and web applications are also involved, so website visitors also suffer. XSS \nattacks are designed to steal cookies stored on a client or sensitive information \nused by other websites to identify a client. \n\u2022\nIn command injection attacks, attackers construct and submit special command \nstrings to embedded or web applications as these applications typically do not \ncheck data submitted by users very strictly. After receiving the constructed \ncommands, applications are tricked into executing external programs or \nlaunching OS attacks so that attackers can steal data or network resources.\n\u2022\nIn a Trojan attack, attackers upload a Trojan to a legitimate website. When a \nuser visits the website, the Trojan is downloaded and executed automatically. The \nuser's computer is attacked and even manipulated "}, {"id": 170, "text": " attack, attackers upload a Trojan to a legitimate website. When a \nuser visits the website, the Trojan is downloaded and executed automatically. The \nuser's computer is attacked and even manipulated by the attacker.\n\u2022\nChallenge Collapsar (CC) attacks are web attacks against web servers or \napplications. In CC attacks, attackers send a large amount of standard GET/POST \nrequests to target system to exhaust web servers or applications. For example, \nattackers can send requests to URIs of databases or other resources to make the \nservers unable to respond to normal requests.\n"}, {"id": 171, "text": "\u2022\nA zero-day vulnerability is a vulnerability in a system or device that has been \ndisclosed but has not been patched yet. No one except the one who discovered \nthe vulnerability is aware of it. This person may exploit the vulnerability to \nlaunch attacks, and such attacks are often unpredictable and destructive.\n"}, {"id": 172, "text": "\u2022\nThe solid line indicates the access traffic.\n\u2022\nDemilitarized Zone (DMZ) is a special network area different from the external \nnetwork or internal network. Generally, the DMZ houses the public servers that \ndo not contain confidential information, for instance, web servers, email servers, \nor FTP servers. Users from the external network can only access the services in \nthe DMZ, but cannot access the information on the internal network. So, the \ninformation on the internal network cannot be impacted even if the servers in \nthe DMZ were attacked.\n"}, {"id": 173, "text": "The global average total cost of data breaches increased by 10% from 2020 to \n2021.\n"}, {"id": 174, "text": "\u2022\nApplication scenarios:\n\u25ab\nData identification: Scanning massive amounts of data, DSC can \nautomatically identify sensitive data and analyze how it is being used. It \nalso scans structured data in RDS and unstructured in OBS, and classifies \nthe data by risk level for further security handling.\n\u25ab\nBehavior analysis: DSC analyzes user behavior, using deep learning to \nestablish a user behavior library. Any behavior uncovered in the library is \ndeemed abnormal and an alarm will be reported in real time. You can then \ntrace user behaviors and correlate the events with the users to identify who \nperformed the risky operations. DSC also detects data breaches and \ngenerates alarms so that you can take immediate protective actions.\n\u25ab\nData masking: The DSC data masking engine leverages a wide range of \npreset and user-defined masking algorithms. It then masks structured and \nunstructured data for storage.\n\u25ab\nCompliance: DSC provides dozens of templates that can be used to check \nfor compliance with regulations and standards such as GDPR, PCI DSS, and \nHIPAA. DSC checks your data protection measures against multiple rules \nstored in templates, and generates reports to propose corrective measures.\n"}, {"id": 175, "text": "\u2022\nPublic certificate can be used by a web server to identify websites that use Secure \nSockets Layer (SSL)/Transport Layer Security (TLS), and to establish encrypted \nconnections to these websites.\n\u25ab\nA public certificate is issued by a public CA to authenticate resources on the \nInternet.\n\u25ab\nA public certificate is trusted by applications and browsers by default, \nbecause the corresponding CA root certificates have been stored in the \ntrusted area of the browser and OS.\n\u25ab\nA public certificate complies with security standards specified by browser \nand operating system vendors and provides operation visibility.\n\u2022\nPrivate certificates identify and protect resources such as applications, services, \ndevices, and users in an organization.\n\u25ab\nA private certificate is issued by a private CA and is used for authenticating \ninternal resources of an organization.\n\u25ab\nServers, websites, clients, devices, and VPN users\n\u25ab\nResources in a private network\n\u25ab\nUntrusted by default: You need to install private certificates in the trusted \nzone on the client.\n\u25ab\nAdvantages:\n\u25aa\nIt can be used to identify any resource.\n\u25aa\nUsers can define issuance rules for verification and naming.\n\u25aa\nIt is not restricted by public CA certificate/agency rules.\n"}, {"id": 176, "text": "\u2022\nCurrently, the SSL certificates issued by international certificate authorities are \nvalid for one year. In CCM, users can configure a rotation schedule for a private \ncertificate based on its expiration date.\n\u2022\nSSL certificate management:\n\u25ab\nBuilding a trusted website. SSL certificates can authenticate websites to \neffectively prevent the websites from being forged.\n\u25ab\nSSL certificates can also authenticate cloud and mobile applications. For \nexample, a wide range of cloud applications, such as customer relationship \nmanagement (CRM), office automation (OA), and enterprise resource \nplanning (ERP) applications, can be authenticated to prevent unauthorized \naccess.\n\u25ab\nSSL certificates can encrypt transmission between websites, applications, \nand clients. This effectively ensures data integrity and prevents data in \ntransit from being stolen or tampered with.\n\u2022\nPrivate Certificate Authority (PCA)\n\u25ab\nEnterprises can use PCA to establish a unified certificate management \nsystem and manage certificates throughout the entire lifecycle. The system \nintegrates continuous monitoring and automation to reduce the risk of \nimproper certificate management.\n\u25ab\nTelematics Service providers (TSPs) use PCA to issue certificates to vehicular \nterminal, thus providing security capabilities such as authentication and \nencryption during vehicle-vehicle, vehicle-cloud, and vehicle-road \ninteractions.\n\u25ab\nInternet of Things (IoT) platforms can use PCA to issue certificates to IoT\ndevices for identity a"}, {"id": 177, "text": "thentication and \nencryption during vehicle-vehicle, vehicle-cloud, and vehicle-road \ninteractions.\n\u25ab\nInternet of Things (IoT) platforms can use PCA to issue certificates to IoT\ndevices for identity authentication, ensuring that only authenticated devices \nare connected.\n"}, {"id": 178, "text": "\u2022\nDedicated HSM: A customer can use dedicated HSMs to meet strict compliance \nrequirements (large-scale high concurrency services, such as payment services).\n\u2022\nKMS is used for cloud service encryption (integrated in cloud services), data disk \nencryption, and small-size data encryption.\n\u2022\nKPS is used for server login.\n\u2022\nCSMS is used for password and token storage.\n"}, {"id": 179, "text": "\u2022\nIf you have purchased an instance, you can use Dedicated HSM to initialize and \nmanage the instance. You can fully control the generation and storage of keys, as \nwell as access authentication for keys.\n"}, {"id": 180, "text": "\u2022\nKMS is integrated with a range of Huawei Cloud services. Customers can create \nkeys on the KMS console or import external keys to encrypt data stored in more \nthan 45 cloud services, such as RDS, ECS, OBS, SFS, DDS and EVS.\n"}, {"id": 181, "text": "\u2022\nA pair of public and private keys are used in the encryption method commonly \nknown as the asymmetric encryption method. The key pair, consisting of a public \nkey and a private key, is generated based on an algorithm. The public key is open \nwhile the private key is not. A public key can be used to encrypt a session key, \nverify a digital signature, or encrypt data that can be decrypted using a private \nkey. The public and private key pair is unique across the whole world. If one key \nis used to encrypt a piece of data, the other key must be used to decrypt the \ndata. If you use either key to encrypt a piece of data, the encrypted data can only \nbe decrypted using the other key or the decryption fails.\n\u2022\nRDS/WKS password management is enhanced. Users do not need to record their \npasswords. Strong passwords are randomly generated, blocking credential \nstuffing attacks. A key pair can be dynamically bound to an ECS. Users can switch \nto the key pair login mode in one click and avoid using weak passwords.\n\u2022\nPrivate keys and passwords are not statically stored on the user side, reducing \nthe risk of private key and password leakage. KMS and KPS manage private keys \nand regularly rotate keys in a unified manner, reducing the attack time window. \nPrivate keys and passwords are encrypted by KMS/KPS on the cloud and then \nsecurely stored. They are dynamically obtained after IAM/MFA authentication. \nThey are easy to use and can be accessed anytime, anywhere. Users can use IAM \ncrede"}, {"id": 182, "text": "ted by KMS/KPS on the cloud and then \nsecurely stored. They are dynamically obtained after IAM/MFA authentication. \nThey are easy to use and can be accessed anytime, anywhere. Users can use IAM \ncredentials and MFA to obtain private keys and passwords anywhere to access \nresources.\n"}, {"id": 183, "text": "\u2022\nUsers or applications can use CSMS to create, retrieve, update, and delete \ncredentials in a unified manner throughout the credential lifecycle. CSMS can \nhelp you eliminate risks that stem from insecure practices such as hardcoding, \nplaintext configuration, and inadequate permission control.\n"}, {"id": 184, "text": "\u2022\nIn this figure, DEW modules include KPS and KMS.\n"}, {"id": 185, "text": "\u2022\nVerizon is the largest wireless carrier in the United States, with over 140 million \nsubscribers.\n"}, {"id": 186, "text": "\u2022\nIn digital transformation, companies face stringent security compliance \nrequirements. Complying with security requirements is a huge responsibility, and \nnon-compliance may result in severe penalties. Security compliance is the first \nand most important thing that enterprises are concerned with in cloud migration. \nCompliance standards determine the security level companies need to be able to \ncomply with on the cloud.\n"}, {"id": 187, "text": "\u2022\nA project can contain different resources. You can attach policies to different user \ngroups to grant permissions for accessing specific resources. In the figure, user A \nis granted access to all resources in project A and to specific resources in project \nB. User B is granted access to specific resources in project B and all resources in \nproject C.\n"}, {"id": 188, "text": "\u2022\nAK: An access key ID is a unique ID associated with an SK. An AK is used together \nwith an SK to cryptographically sign requests.\n\u2022\nSK: A secret access key is used in conjunction with an AK to sign requests \ncryptographically. It identifies a request sender and prevents the request from \nbeing modified.\n"}, {"id": 189, "text": "\u2022\nIAM users do not have their own resources and cannot pay for the resources they \nuse. The account assigns permissions to IAM users and pays for the use of the \nresources.\n\u2022\nYou can assign permissions to IAM users through user groups. By default, new \nIAM users do not have any permissions assigned. To assign permissions to new \nusers, add them to one or more groups, and assign permissions to these groups. \nThe users then inherit permissions from the groups they belong to, and they can \nperform operations on cloud services based on the assigned permissions.\n"}, {"id": 190, "text": "\u2022\nAuthorization policies:\n\u25ab\nSystem-defined policies: maintained by Huawei Cloud\n\u25ab\nCustom policies: maintained by users\n"}, {"id": 191, "text": "\u2022\nAccount delegation: You can delegate permissions to other Huawei Cloud \naccounts only. You cannot delegate permissions to federated accounts or IAM \nusers.\n\u2022\nCloud service delegation: Huawei Cloud services interwork with each other. Some \ncloud services depend on other services. You can create an agency to delegate a \ncloud service to call other services on your behalf. For example, if Container \nGuard Service (CGS) needs to scan container images, you need to delegate \nSoftWare Repository for Container (SWR) permissions to CGS.\n"}, {"id": 192, "text": "\u2022\nOpenID Connect (OIDC): a standard identity authentication protocol that runs on \ntop of the OAuth 2.0 protocol.\n\u2022\nSecurity Assertion Markup Language (SAML): Security Proposition Markup \nLanguage. It is an XML-based open-standard for transferring identity data \nbetween two parties: an identity provider (IdP) and a service provider (SP).\n\u2022\nIdentity provider (IdP): collects and stores user identity information, such as \nusernames and passwords, and authenticates users during login. For identity \nfederation between an enterprise and Huawei Cloud, the IdP refers to the identity \nauthentication system of the enterprise.\n\u2022\nIdentity federation process:\n\u25ab\nCreate an IdP and establish a trust relationship.\n\u25aa\nOIDC-based IdP: Create OAuth 2.0 credentials in the enterprise IdP\nand create an IdP in Huawei Cloud to establish a trust relationship \nbetween the enterprise and Huawei Cloud.\n\u25aa\nSAML-based IdP: Exchange the metadata files (SAML 2.0-compliant \ninterface files that contain interface addresses and certificate \ninformation) of the enterprise IdP and Huawei Cloud. Then, create an \nIdP in Huawei Cloud to establish a trust relationship between the \nenterprise and Huawei Cloud.\n\u25ab\nConfigure identity conversion rules: Map the users, user groups, and their \npermissions in the enterprise IdP to Huawei Cloud.\n\u25ab\nConfigure a login link: Configure a login link in the enterprise management \nsystem to allow users to access Huawei Cloud using SSO.\n"}, {"id": 193, "text": "\u2022\nAfter data is collected, it is batch processed by the big data platform and then \nanalyzed by the big data operations center. Analysis results are reported to SA so \nthat SA can take appropriate protective actions such as event analysis and alarm \nreporting.\n"}, {"id": 194, "text": "\u2022\nAsset management: As enterprises migrate more workloads to the cloud, more \ncloud assets are used, and there are frequent changes made to those assets. This \nmeans more security risks on the cloud.\n\u25ab\nSA gives customers a comprehensive view of the security status of assets on \nthe cloud. SA monitors the security status of all assets in the cloud in real \ntime and visualizes vulnerabilities, threats, and attacks on servers, making it \neasier for customers to handle risks.\n\u2022\nThreat event alarms: Security threats to clouds never stop, and a variety of new \nthreats are emerging every day.\n\u25ab\nBy collecting network-wide traffic data and security device logs, SA can \ndetect and monitor security risks on the cloud in real time, display statistics \non security events in real time, and aggregate event data from other \nsecurity services. SA uses preset security policies to effectively defend \nagainst common brute-force attacks, web attacks, Trojans, and zombie \nbots, greatly improving defense and O&M efficiency.\n\u2022\nVulnerability notifications: Service security is of top priority during cloud \nmigrations. To prevent vulnerabilities from being exploited, we need to find and \nfix as many vulnerabilities as possible.\n\u25ab\nApart from reporting latest vulnerabilities based on emergency security \nnotices issued on Huawei Cloud, SA periodically scans OSs, software, and \nwebsites for vulnerabilities by working with linked security services, making \nit easier for customers to centrally manage server "}, {"id": 195, "text": "ces issued on Huawei Cloud, SA periodically scans OSs, software, and \nwebsites for vulnerabilities by working with linked security services, making \nit easier for customers to centrally manage server and website \nvulnerabilities. SA also provides mitigation suggestions. With centralized \nvulnerability management on the cloud, SA helps customers quickly identify \nkey risks and vulnerable assets and harden their service system.\n\u2022\nSA can scan for unsafe settings of cloud services, report scan results by category, \ngenerate alarms for unsafe settings, and provide hardening suggestions and \n"}, {"id": 196, "text": "guidelines.\n"}, {"id": 197, "text": "\u2022\nMTD collects logs from IAM, DNS, CTS, OBS, and VPC and uses an AI engine, \nthreat intelligence, and detection policies to continuously detect potential threats, \nmalicious activities, and unauthorized behavior, such as brute-force cracking, \npenetration attacks, and mining attacks.\n\u2022\nInbound bandwidth is the bandwidth consumed when data is transferred from \nthe Internet to Huawei Cloud. For example, when resources are downloaded \nfrom the Internet to ECSs, that consumes inbound bandwidth.\n\u2022\nOutbound bandwidth is the bandwidth consumed when data is transferred from \nHuawei Cloud to the Internet. For example, when ECSs provide services accessible \nfrom the Internet and external users download resources from the ECSs, that \nconsumes outbound bandwidth.\n"}, {"id": 198, "text": "\u2022\nMTD uses advanced detection technologies, such as threat intelligence, AI \ndetection engine, and correlation models, to scan IAM, CTS, VPC, and DNS service \nlogs for cracking attacks. Additionally, MTD tracks and audits network behaviors, \nand identifies traffic changes of network devices and servers for an abnormal \nnumber of connections. MTD reports alarms to SA and collaborates with other \nsecurity services for overall situation monitoring. System security issues can be \ndetected in a timely manner.\n\u2022\nMTD identifies threats to IAM accounts and vulnerabilities to DNS and looks for \nintrusions by checking CTS logs. These security risks cannot or can barely be \ndetected by other security services. When risks increase, multi-factor verification \nor biometric recognition is required by MTD for using an IAM account.\n"}, {"id": 199, "text": "\u2022\n1. Answer: False. \n\u25ab\nSecurity compliance is the foundation of businesses on the cloud. If \ncompanies do not meet compliance requirements, they may be punished \nfinancially.\n\u2022\n2. Answer: A. \n\u25ab\nKMS is integrated into cloud services.\n"}, {"id": 200, "text": "\u2022\nDiscussion 1:\n\u25ab\nThe five security dimensions should be considered.\n\u25ab\nApplication scenarios and features of Huawei Cloud security products \nshould be considered.\n\u2022\nDiscussion 2:\n\u25ab\nSecurity organizations and personnel: Internal security and personnel \nsecurity awareness of key positions, core services, and confidential services \nof the company\n\u25ab\nInfrastructure security: Isolated deployment of service planes (secure traffic \ndistribution) and water-proof, electricity, and equipment room security of \non-premises resources\n\u25ab\nEngineering security: Secure coding and review and approval processes of \nthird-party software\n\u25ab\nO&M security: Business continuity planning and testing (periodical testing \nof DR and infrastructure HA)\n"}, {"id": 201, "text": "\u2022\nServer-based phase: With hardware devices as the center, service applications are \ncustomized based on devices, OSs, and virtualization software. Device installation \nand commissioning, and application deployment and O&M are performed \nmanually, so the automation level is low and unified device and application \nmanagement capabilities are unavailable. With the emergence of virtualization \nsoftware, resource utilization and container scaling flexibility are improved. \nHowever, the infrastructure is still separate from software and O&M is still \ncomplex.\n\u2022\nCloud-based phase: Devices that are separately distributed in traditional mode \nare unified to form resource pools. A unified virtualization software platform \nautomatically manages resources of upper-layer service software to enhance \napplication universality. However, vendors strengthen virtualization software \nplatforms with different commercial capabilities which cannot be shared among \nvendors, so applications cannot be built in a fully standardized mode, and \napplication deployment is still resource-centric.\n\u2022\nCloud native phase: Enterprise digital transformation is now shifting to cloud \nnative. Agile application delivery, rapid scaling, smooth migration, and hitless DR \nare under the spotlight. Therefore, enterprises start to consider how to integrate \nthe infrastructure with their service platforms to run service applications in a \nunified manner by taking advantages of standard app running, monitoring, and \ngovern"}, {"id": 202, "text": "art to consider how to integrate \nthe infrastructure with their service platforms to run service applications in a \nunified manner by taking advantages of standard app running, monitoring, and \ngovernance capabilities to implement application automation.\n"}, {"id": 203, "text": "\u2022\nIn July 21, 2015, Cloud Native Computing Foundation (CNCF) was founded by \nGoogle, Huawei, and other enterprises, marking the shift of cloud native from a \ntechnical concept to an open source implementation.  Huawei Cloud is the only \nCNCF founding member from Asia and the only platinum member from China.\n\u2022\nCNCF is committed to fostering and maintaining a vendor-neutral open source \necosystem. We democratize state-of-the-art patterns to make these innovations \naccessible for everyone.\n\u2022\nCNCF is committed to fostering and maintaining a vendor-neutral open source \necosystem and aims to make cloud native technologies available to the public. \nProviding a clearer, more understandable definition on cloud native, CNCF lays a \nfoundation for the wide adoption of cloud native in a variety of industries. As \nsurveyed by CNCF, more than 80% of users have used or plan to use the \nmicroservice architecture for service development and deployment. Users' \nawareness and use of cloud native technologies are at a new height, and the \ntechnology ecosystem is experiencing rapid changes.\n"}, {"id": 204, "text": "\u2022\nStarting from the basic container engine, the cloud native open source project \ncontinuously expands the application field and improves the adaptation capability \nto various scenarios. From Docker (an early open source container engine), to \nKubernetes, Swarm, and Mesos (for efficient container orchestration), to Istio (for \nmicroservice governance using service meshes), kubeEdge (for edge scenarios), \nK3s (a lightweight Kubernetes distribution), and Volcano (for high-performance \nheterogeneous computing), these projects have accelerated the convergence of \ncloud native and industries and promoted innovation in various industries.\n\u2022\nIn 2020, CAICT compiled the Cloud Native Development White Paper 2020 after \nin-depth survey and analysis on cloud native technologies and the industry in \nChina. In 2020, Huawei Cloud first proposed the concept of Cloud Native 2.0, \naiming to help every enterprise become a cloud native enterprise.\n"}, {"id": 205, "text": "\u2022\nAt the early stage of enterprise digital transformation, services were migrated \nfrom on premises to the cloud and deployed and run on the cloud. This is called \n\"On Cloud\". In this mode, the cloud-based resource pool simplifies service \ndeployment, O&M, and capacity expansion in the IDC era. However, monolithic \napplications with their siloed architectures may lead to many application-level \nproblems. The benefits of the cloud are still mostly limited to resource \nprovisioning.\n\u2022\nIn Cloud Native 1.0, technologies focus on the infrastructure layer and the \nmonolithic architecture is resource-centric. The application ecosystem is simple. \nCloud native technologies are mainly used in Internet companies.\n\u2022\nAs digital transformation thrives, enterprises need to build and develop services in \nthe cloud and integrate legacy capabilities with the new ones. In Cloud Native 2.0, \n\"born in cloud\" means using cloud native technologies, architectures, and services \nto build applications. \"Grow in cloud\" means these new apps run and expand fully \non the cloud to build digital, intelligent services.\n\u2022\nFrom \"On Cloud\" to \"In Cloud\": New enterprise applications are built on cloud \nnative technologies. Applications, data, and AI are managed in the cloud \nthroughout their lifecycle. Existing applications are organically coordinated with \nnew ones.\n\u2022\nNew Cloud Native Enterprises: Cloud Native 2.0 is a new phase for intelligent \nupgrade of enterprises. Legacy capabilities co-exist and work wel"}, {"id": 206, "text": "lications are organically coordinated with \nnew ones.\n\u2022\nNew Cloud Native Enterprises: Cloud Native 2.0 is a new phase for intelligent \nupgrade of enterprises. Legacy capabilities co-exist and work well with new ones \nto achieve efficient resource utilization, agile applications, service intelligence, and \nsecure, trustworthy services.\n"}, {"id": 207, "text": "\u2022\nIn Cloud Native 2.0,\n\u25ab\ncloud native technologies shift from resource-centric to application-centric. \nCloud native infrastructure can be aware of application features, and \napplications can use cloud native infrastructure more intelligently and \nefficiently.\n\u25ab\nThe multi-cloud architecture allows cloud-native applications to be \ndistributed. Clouds can collaborate with devices, edges, and clouds \nthemselves in multiple scenarios.\n\u25ab\nCloud Native 2.0 is an open system that allows organic collaboration and co-\nexistence between new and legacy applications.\n\u25ab\nCloud Native 2.0 features full stack, where cloud native is extended to fields \nsuch as application, big data, database, and AI.\n"}, {"id": 208, "text": "\u2022\nHardware layer: Introducing the cloud-infrastructure-aware hardware PCI card \n(SDI/Qingtian offloading card), self-developed universal CPU (Kunpeng), and \nheterogeneous NPU (Ascend), through a series of hardware offloading and in-\ndepth software-hardware synergy oriented to homogeneous and heterogeneous \ncompute, Huawei builds the most cost-effective computing power platform that \nworks with containers and VMs.\n\u2022\nOS layer: In addition to standard OS functions, this layer distributes resources. \nPhysical server resources are divided into multiple VMs and containers. The \nEulerOS supports upper-layer intelligent resource scheduling and flexible \ncomputing. Hardware passthrough minimizes the overheads of storage and \nnetwork virtualization.\n\u2022\nElastic resource layer: This layer integrates resources. For example, for cloud \nnative compute, especially Kubernetes container clusters, their extended tasks, \nand Alkaid intelligent scheduling system, streamline cloud-edge and regionless \nscheduling, as well as cloud native capabilities such as network virtualization, \ndistributed storage, disaster recovery, and high reliability.\n\u2022\nApplication and data enablement layer: This layer covers blockchain, cloud \nsecurity enablement, AI ModelArts (inclusive AI development platform), and cloud \nnative distributed middleware, edge, database, big data, video, and IoT.\n\u2022\nApplication lifecycle management: includes DevSecOps (service development \npipeline), cloud native service governance and orche"}, {"id": 209, "text": "tive distributed middleware, edge, database, big data, video, and IoT.\n\u2022\nApplication lifecycle management: includes DevSecOps (service development \npipeline), cloud native service governance and orchestration, CMDB (for tenants \nto deploy services), and monitoring and O&M services.\n\u2022\nMulti-tenant framework: provides cloud services with multi-tenant authentication \nand permission management (identity authentication for cloud service and \nresource access, and access permission management for cloud service objects), \ncloud native operations and billing, API openness, and cloud native console.\n"}, {"id": 210, "text": "\u2022\nDocker is the first system that allows containers to be portable in different \nmachines. It simplifies the packaging of both the application and the application \nlibraries and dependencies. Even the OS file system can be packaged into a simple \nportable package, which can be used on any other machine that runs Docker. \nDocker proposed OCI to set up container engine technology standards followed \nby many companies.\n\u2022\nContainers have the following advantages over VMs:\n\u25ab\nHigher system resource utilization: With no overhead for virtualizing \nhardware and running a complete OS, containers outperform VMs in \napplication execution speed and memory loss.\n\u25ab\nFaster startup: Traditional VMs usually take minutes to start an application. \nHowever, Docker containerized applications run directly on the host kernel \nwith no need to boot the OS, so they can start within seconds.\n\u25ab\nConsistent running environments: A common problem in development is the \nconsistency of application running environments. Due to inconsistent \ndevelopment, testing, and production environments, some bugs cannot be \ndiscovered prior to rollout. A Docker container image provides a complete \nruntime to ensure consistency in application running environments.\n\u25ab\nEasier migration: Docker ensures the consistency in execution environment, \nso migrating applications becomes much easier. Docker can run on many \nplatforms, and no matter on physical machines or virtual ones, its running \nresults remains the same.\n\u25ab\nEasier main"}, {"id": 211, "text": "environment, \nso migrating applications becomes much easier. Docker can run on many \nplatforms, and no matter on physical machines or virtual ones, its running \nresults remains the same.\n\u25ab\nEasier maintenance and extension: Tiered storage and images in Docker \nfacilitate the reuse of applications and simplify application maintenance and \nupdate. In addition, Docker collaborates with open source project teams to \nmaintain a large number of high-quality official images. You can directly use \nthem in the production environment or form new images based on them, \ngreatly reducing the image production cost of applications.\n"}, {"id": 212, "text": "\u2022\nContainer technology was first invented by Linux developers. Docker has \npopularized containers by making the technology accessible through an open \nsource tool and reusable images.\n\u2022\nNamespaces isolate the running environments, that is, each container is an \nindependent process.\n\u2022\nCgroups isolate running resources and make them exclusive for each container. \nYou can specify the amount of resources for each container. \n\u2022\nThe union filesystem is a filesystem service that Docker uses to layer images. \nContainer images allow for standard container running, but container images are \nnot containers. A container image is a series of layered read-only files managed \nby the storage driver. When a container image runs as a container, a writable \nlayer, that is, a container layer, is added to the top of the image. All modifications \nto a running container are actually modifications to the container read/write \nlayer. Such modifications, such as writing a new file and modifying an existing file \nare only applied to the container layer.\n\u2022\nContainers share the host kernel and do not need to boot an OS or virtualize \nresources. Therefore, they are more lightweight and have low resource overheads. \nIn addition, a container image packages an application and its runtime \nenvironment. These highly portable and standardized packages allow for large-\nscale application scaling and management.\n\u2022\nDocker Daemon is a background system process in the Docker architecture.\n\u2022\nContainerd is an intermedi"}, {"id": 213, "text": "ly portable and standardized packages allow for large-\nscale application scaling and management.\n\u2022\nDocker Daemon is a background system process in the Docker architecture.\n\u2022\nContainerd is an intermediate communication component between dockerd and \nrunc. Docker manages and operates containers through containerd.\n\u2022\nContainerd-shim is a carrier for running containers. Each time a container is \nstarted, a new containerd-shim process is created.\n\u2022\nRunC is a command-line tool used to run the OCI applications.\n"}, {"id": 214, "text": "\u2022\nKata Containers is an open source container project initiated by Intel, Huawei, \nand Red Hat. It runs container management tools on bare metal servers and \nprovides strong, secure workload isolation. Kata containers are as lightweight and \nfast as containers and as secure as VMs.\n"}, {"id": 215, "text": "\u2022\nThe name Kubernetes originates from Greek, meaning helmsman or pilot. K8s as \nan abbreviation results from counting the eight letters between the \"K\" and the \n\"s\". Kubernetes is open-sourced by Google from its internal cluster management \nsystem Borg after Google's own service attributes are removed. Kubernetes is a \nrecognized de facto standard in the container orchestration field. Almost all \ncontainer technologies of public cloud vendors are built on Kubernetes.\n\u2022\nIn the standard architecture of Kubernetes, a cluster is a complete set of \nKubernetes products. Most enterprises encapsulate the management plane on \nclusters for cluster-level management.\n\u2022\nFor application developers, Kubernetes can be regarded as a cluster operating \nsystem. Kubernetes provides service discovery, scaling, load balancing, self-\nhealing, and even leader election, freeing developers from infrastructure-related \nconfigurations.\n\u2022\nWith Kubernetes, applications can be automatically deployed, restarted, migrated, \nand scaled based on the application status. Kubernetes can be compatible with \ndifferent infrastructures (public/private cloud) using plug-ins. Kubernetes also \nprovides flexible resource isolation for different teams to set up running \nenvironments quickly.\n\u2022\nA master node in the cluster manages the entire container cluster. In HA \nscenarios with etcd used, there are at least three master nodes in a cluster. There \nare many worker nodes in a cluster, which are used to run containerized \n"}, {"id": 216, "text": "ages the entire container cluster. In HA \nscenarios with etcd used, there are at least three master nodes in a cluster. There \nare many worker nodes in a cluster, which are used to run containerized \napplications. The master node installs kubelet on each worker node as the agent \nfor managing the node.\n"}, {"id": 217, "text": "\u2022\nMaster node:\n\u25ab\nAPI server: functions as a transit station for component communication, \nreceives external requests, and writes information to etcd.\n\u25ab\nController manager: performs cluster-level component replication, node \ntracing, and node fault fixing.\n\u25ab\nScheduler: schedules containers to nodes by conditions (such as available \nresources and node affinity).\n\u25ab\netcd: serves as a distributed data storage component that stores cluster \nconfigurations and object status.\n\u2022\nWorker node:\n\u25ab\nkubelet: communicates with the container runtime, interacts with the API \nserver, and manages containers on the node. The cAdvisor monitors \nresources and containers on nodes in real time and collects performance \ndata.\n\u25ab\nkube-proxy: serves as an access proxy between application components.\n\u25ab\nContainer runtime: runs container software, such as Docker, containerd, CRI-\nO, and Kubernetes CRI.\n\u2022\nkubectl is a command line tool for Kubernetes clusters. You can install kubectl on \nany machine and run kubectl commands to operate your Kubernetes cluster.\n\u2022\nWhen using Kubernetes, users call the API server on the master node to use \nrequired resource objects such as applications and Services in the declarative APIs. \nThe master node controller and scheduler create resources on the node based on \nthe user definition and monitor the status at any time, ensuring that the \nresources meet requirements. Unified access to containerized applications on \nnodes can be achieved through kube-proxy.\n"}, {"id": 218, "text": "\u2022\nThe minimum unit of Kubernetes orchestration is pod. The idea comes from pea \npod. A pod can contain many containers, just as a pea pod can contain many \npeas.\n\u2022\nIn most cases, containers are used to carry microservices (small and single \nservices). During microservice design, it is recommended that one process be \nborne by one application. If the bearer is a container, one process is borne by one \ncontainer. However, to manage microservices, you need to install service \nmonitoring software or data reading software. That is, multiple software, or \nprocesses, need to be installed in a container. This undermines the principle of \none container for one process. To comply with the microservice design principles, \nKubernetes designed pods. Generally, a pod contains multiple containers, \nincluding one application container (used to provide services) and multiple \nsidecar containers (used to monitor the application container or manage data). \nFor example, a pod contains three containers: web container, monitoring \ncontainer, and log reading container. The web container only runs web software, \nand port 80 is exposed externally. The monitoring software of the web container, \nrunning in the monitoring container, monitors the web service through \n127.0.0.1:80, because containers in the pod share the IP address. The log reading \ncontainer reads files in the corresponding path and report the files to the log \nmanagement platform, because containers in the pod share the data storage \nvo"}, {"id": 219, "text": "od share the IP address. The log reading \ncontainer reads files in the corresponding path and report the files to the log \nmanagement platform, because containers in the pod share the data storage \nvolumes.\n\u2022\nContainer Runtime Interface (CRI) defines the interfaces of container and image \nservices. The lifecycle of a container is separated from that of an image, two \nservices need to be defined. CRI is responsible for the communication between \nkubelet and containers.\n"}, {"id": 220, "text": "\u2022\nProbe types:\n\u25ab\nLiveness probe: checks whether the container is running. If the check fails, \nkubelet kills the container and restarts the container based on the restart \npolicy.\n\u25ab\nReadiness probe: checks whether the container is ready to handle requests. If \nthe check fails, the endpoint controller deletes the IP address of the pod \nfrom the endpoint of all services that match the pod.\n\u25ab\nStartup probe: checks whether the containerized application is started. Other \nprobes are disabled until the startup probe is successfully detected. If the \ncheck fails, kubelet kills the container and restarts the container based on \nthe restart policy.\n\u2022\nApplication scenarios of DaemonSets\n\u25ab\nDaemonSet for clusters on the node.\n\u25ab\nDaemonSet for log collection on the node.\n\u25ab\nDaemonSet for node monitoring.\n\u2022\nKubernetes supports node-level and pod-level affinity and anti-affinity. You can \nconfigure custom rules to achieve affinity and anti-affinity scheduling. For \nexample, you can deploy frontend pods and backend pods together, deploy the \nsame type of applications on a specific node, or deploy different applications on \ndifferent nodes.\n"}, {"id": 221, "text": "\u2022\nDeployment: Controllers create and manage pods for Kubernetes and provide \nreplica management, rolling upgrade, and self-healing.\n\u2022\nDeployment: the most commonly used controller. When a Deployment is created, \na ReplicaSet is automatically created. A Deployment can manage multiple \nReplicaSets and use them to manage pods.\n\u2022\nAll pods under a Deployment have the same characteristics except for the name \nand IP address. If required, a Deployment can use the pod template to create a \nnew pod. If not required, the Deployment can delete any one of the pods. \nGenerally, a pod contains one container or several containers that are closely \nrelated. A ReplicaSet contains multiple identical pods. A Deployment contains one \nor more different ReplicaSets.\n\u2022\nStatefulSets provide a fixed identifier for each pod. A fixed suffix ranging from 0 \nto N is added to the pod name. After pods are rescheduled, the pod name and \nhost name remain unchanged. StatefulSets provide a fixed access domain name \nfor each pod through the headless Service (described in following sections). \nStatefulSets create PersistentVolumeClaims (PVCs) with fixed identifiers to ensure \nthat pods can access the same persistent data after being rescheduled.\n\u2022\nJobs and cron jobs allow you to run short lived, one-off tasks in batch. They \nensure the task pods run to completion.\n\u25ab\nJob: a resource object used by Kubernetes to control batch tasks. Jobs are \ndifferent from long-term servo workloads (such as Deployments and \nState"}, {"id": 222, "text": "ch. They \nensure the task pods run to completion.\n\u25ab\nJob: a resource object used by Kubernetes to control batch tasks. Jobs are \ndifferent from long-term servo workloads (such as Deployments and \nStatefulSets). The former is started and terminated at specific times, while \nthe latter runs unceasingly unless being terminated. The pods managed by a \njob automatically exit after successfully completing the job based on user \nconfigurations.\n\u25ab\nCronJob: runs a job periodically on a specified schedule. A cron job object is \nsimilar to a line of a crontab file in Linux.\n"}, {"id": 223, "text": "\u2022\nSimilar to a ConfigMap, a secret stores data in key-value pairs. The difference is \nthat the value must be encoded using Base64.\n\u2022\nsecret\n\u25ab\nA secret provides better security in the process of creating, viewing, and \nediting pods.\n\u25ab\nThe system takes extra precautions for secret objects, for example, \npreventing it from being written to a location on the disk.\n\u25ab\nOnly the secret requested by the pod is visible in its container. One pod \ncannot access the secret of another pod.\n"}, {"id": 224, "text": "\u2022\nBridges between different nodes can be implemented in multiple modes. \nHowever, in a cluster, the pod IP address must be unique. Therefore, cross-node \nbridges use different CIDR blocks to prevent duplicate pod IP addresses.\n\u2022\nCommunication between containers: Containers in a pod share the same network \nnamespace, which is provided by IaaS. Each pod has its own IP address and has \nno conflicts with each other.\n\u2022\nPods and nodes in the cluster can directly communicate with each other using the \nIP address. This communication does not require any network address translation, \ntunneling, or proxy. The same IP address is used internally and externally in a \npod, which also means that the standard naming and discovery mechanisms, \nsuch as DNS, can be directly used. This type of communications also requires \nKubernetes network plug-ins (for example, flannel) to configure a layer network \nfabric, routed network, and more.\n\u2022\nCommunication between pods: Pods can communicate with each other through \nIP addresses only when pods know the IP addresses of each other. In a cluster, \npods may be frequently deleted and created. That is, the IP addresses of pods are \nnot fixed. To solve this problem, a Service provides an abstraction layer for \naccessing pods. No matter how the backend pod changes, the Service functions as \na stable frontend to enable external access. In addition, a Service supports HA \nand load balancing, forwarding requests to the correct pod.\n\u2022\nFlannel is a network plannin"}, {"id": 225, "text": "s, the Service functions as \na stable frontend to enable external access. In addition, a Service supports HA \nand load balancing, forwarding requests to the correct pod.\n\u2022\nFlannel is a network planning service designed by the CoreOS team for \nKubernetes. It enables containers created on different nodes in a cluster to have a \nunique virtual IP address in the cluster.\n\u2022\nCalico is famous for its performance and flexibility compared to Flannel's \nsimplicity. Calico provides more comprehensive functions, not only network \nconnections between hosts and pods, but also network security and management.\n"}, {"id": 226, "text": "\u2022\nAfter a pod is created, the following problems may occur when you directly \naccess a pod:\n\u25ab\nThe pod can be deleted and recreated at any time by a controller such as a \nDeployment, and the result of accessing the pod becomes unpredictable.\n\u25ab\nThe IP address of the pod is allocated only after the pod is started. Before \nthe pod is started, the IP address of the pod is unknown.\n\u25ab\nAn application is usually composed of multiple pods that run the same \nimage. Accessing pods one by one is not efficient.\n\u2022\nReplicationControllers, ReplicaSets, and Deployments only ensure the number of \nmicroservice pods that support services, but do not solve the problem of how to \naccess these services. A pod is only an instance that runs services. It may be \nstopped on a node at any time and recreated on another node using a new IP \naddress. Therefore, services cannot be provided using a fixed IP address and port \nnumber. To provide services stably, service discovery and load balancing are \nrequired. Service discovery finds the target backend service requested by the \nclient. In a Kubernetes cluster, the service that the client needs to access is the \nService object. Each Service corresponds to a valid virtual IP address in the cluster. \nThe cluster uses the virtual IP address to access a Service.\n\u2022\nIn Kubernetes, a Service is an abstraction which defines a logical set of pods and \na policy by which to access them, usually this pattern is called a microservice. The \nset of pods targeted by a Servic"}, {"id": 227, "text": " Kubernetes, a Service is an abstraction which defines a logical set of pods and \na policy by which to access them, usually this pattern is called a microservice. The \nset of pods targeted by a Service is usually determined by a selector.\n\u2022\nThe implementation types of Service are as follows:\n\u25ab\nClusterIP: provides an internal virtual IP address for pods to access (default \nmode).\n\u25ab\nNodePort: enables a port on the node for external access.\n\u25ab\nLoadBalancer: allows access through an external load balancer.\n"}, {"id": 228, "text": "\u2022\nIngress provides load balancing, SSL termination, and name-based virtual hosting. \nIngress exposes HTTP and HTTPS routes from outside the cluster to Services \nwithin the cluster. Traffic routing is controlled by rules defined on the ingress \nresource.\n\u2022\nTo use an Ingress, you must install Ingress Controller on your Kubernetes cluster. \nIngress Controller can be implemented in multiple modes. The most common one \nis NGINX Ingress Controller maintained by Kubernetes. In Huawei Cloud, Cloud \nContainer Engine (CCE) works with Elastic Load Balance (ELB) to implement \nlayer-7 load balancing (via ingresses).\n"}, {"id": 229, "text": "\u2022\nA volume will no longer exist if the pod to which it is mounted does not exist. \nHowever, files in the volume may outlive the volume, depending on the volume \ntype. All containers in a pod can access its volumes, but the volumes must have \nbeen mounted. Volumes can be mounted to any directory in a container.\n\u25ab\nPV: defines a directory for persistent storage on a host machine, for example, \na mount directory of a file system.\n\u25ab\nPVC: describes the attributes of the PV that a pod wants to use, such as the \nvolume capacity and read/write permissions.\n\u2022\nAlthough PVs and PVCs allow you to consume abstract storage resources, you \nmay need to configure multiple files to create PVs and PVCs. Therefore, they are \ngenerally managed by the cluster administrator. To resolve this issue, Kubernetes \nsupports dynamic PV provisioning to create PVs automatically. The cluster \nadministrator can deploy a PV provisioner and define the corresponding \nStorageClass. In this way, developers can select the storage class to be created \nwhen creating a PVC. The PVC transfers the StorageClass to the PV provisioner, \nand the provisioner automatically creates a PV.\n\u2022\nStorageClass describes the storage class used in the cluster. You need to specify \nStorageClass when creating a PVC or PV.\n\u2022\nTo allow a pod to use PVs, a Kubernetes cluster administrator needs to set the \nnetwork storage class and provides the corresponding PV descriptors to \nKubernetes. You only need to create a PVC and bind the PVC with the"}, {"id": 230, "text": "se PVs, a Kubernetes cluster administrator needs to set the \nnetwork storage class and provides the corresponding PV descriptors to \nKubernetes. You only need to create a PVC and bind the PVC with the volumes in \nthe pod so that you can store data.\n"}, {"id": 231, "text": "\u2022\nThe cluster migration process is as follows:\n\u25ab\nPlan resources for the target cluster. For details about the differences \nbetween CCE clusters and on-premise clusters, see \"Key Performance \nParameter\" in \"Planning Resources for the Target Cluster\". Plan resources as \nrequired and ensure the performance configuration of the target cluster is \nthe same as that of the source cluster.\n\u25ab\nMigrate resources outside a cluster. Huawei Cloud provides migration \nsolutions to migrate resources outside the cluster. These solutions involve \nthe migration of container images, databases, and storage.\n\u25ab\nInstall the migration tool. After resources outside the cluster are migrated, \nyou can use the migration tool to back up and restore application \nconfigurations in the source and target clusters.\n\u25ab\nMigrate resources in the cluster. You can use open source DR software, such \nas Velero, to back up resources in the source cluster to the object storage \nand restore the resources in the target cluster.\n\u25aa\nNo need to configure, update, or manage servers. Managing servers, \nVMs, and containers involves personnel, tools, training, and time.\n\u25aa\nFaaS and BaaS products can be scaled flexibly and precisely to process \neach request. For developers, a serverless platform does not need \ncapacity planning or auto scaling triggers or rules.\n\u25ab\nUpdate resources accordingly. After the migration, cluster resources may fail \nto be deployed. You need to update the faulty resources. The possible \nadaptation problems l"}, {"id": 232, "text": "scaling triggers or rules.\n\u25ab\nUpdate resources accordingly. After the migration, cluster resources may fail \nto be deployed. You need to update the faulty resources. The possible \nadaptation problems lie in images, Services and ingresses, StorageClasses, \nand databases.\n\u25ab\nPerform additional tasks. After cluster resources are properly deployed, \n"}, {"id": 233, "text": "verify application functions after the migration and switch service traffic to \nthe target cluster. After confirming that all services are running properly, \nbring the source cluster offline.\n"}, {"id": 234, "text": "\u2022\nCCE is deeply integrated with high-performance HUAWEI CLOUD computing \n(ECS/BMS), network (VPC/EIP/ELB), and storage (EVS/OBS/SFS) services, and \nsupports heterogeneous computing architectures such as GPU and Arm. You can \nbuild high-availability Kubernetes clusters secured by multi-AZ, cross-region \ndisaster recovery (DR) and auto scaling.\n"}, {"id": 235, "text": "\u2022\nHuawei is amongst the first developers of the Kubernetes community in China. \nHuawei is a major contributor to the open source community and a leader in the \ncontainer ecosystem. Huawei Cloud CCE is the earliest commercial Kubernetes \nservice in China, and is also one of the first products that passed the CNCF \nCertified Kubernetes Conformance Program. CCE features benefits such as access \nto open ecosystems, enhanced commercial features, and adaptation to \nheterogeneous infrastructure.\n\u2022\nVolcano: Native Kubernetes has weak support for batch computing services. \nVolcano provides two enhanced batch computing capabilities. One is advanced \njob management, such as task queuing, priority setting, eviction, backfilling, and \nstarvation prevention. The other is intelligent scheduling, such as topology-aware \naffinity-based scheduling and dynamic driver-executor ratio adjustment. In \naddition, scheduling and distributed frameworks such as gang scheduling and PS-\nWorker are supported.\n\u2022\nYou can use CCE via the CCE console, kubectl, or Kubernetes APIs.\n"}, {"id": 236, "text": "\u2022\nA node is a basic element of a container cluster. CCE uses high-performance \nElastic Cloud Servers (ECSs) or Bare Metal Servers (BMSs) as nodes to build \nhighly available Kubernetes clusters.\n\u2022\nKata containers are distinguished from common containers in a few aspects. The \nmost important difference is that each Kata container (pod) runs on an \nindependent micro-VM, has an independent OS kernel, and is securely isolated at \nthe virtualization layer. CCE provides container isolation that is more secure than \nindependent private Kubernetes clusters. With Kata containers, kernels, \ncomputing resources, and networks are isolated between different containers to \nprotect pod resources and data from being preempted and stolen by other pods.\n\u2022\nA workload is an application running on Kubernetes. No matter how many \ncomponents are there in your workload, you can run it in a group of Kubernetes \npods.\n\u2022\nCCE supports Kubernetes-native deployment and lifecycle management of \ncontainer workloads, including creation, configuration, monitoring, auto scaling, \nupgrade, uninstall, service discovery, and load balancing.\n"}, {"id": 237, "text": "\u2022\nRecommendations on CIDR block planning:\n\u25ab\nCIDR blocks cannot overlap. Otherwise, a conflict occurs. All subnets \n(including those created from the secondary CIDR block) in the VPC where \nthe cluster resides cannot conflict with the container and Service CIDR blocks.\n\u25ab\nEnsure that each CIDR block has sufficient IP addresses. The IP addresses in \nthe node CIDR block must match the cluster scale. Otherwise, nodes cannot \nbe created due to insufficient IP addresses. The IP addresses in the container \nCIDR block must match the service scale. Otherwise, pods cannot be created \ndue to insufficient IP addresses.\n\u2022\nIn the Cloud Native Network 2.0 model, the container CIDR block and node CIDR \nblock share the IP addresses in the same VPC. Therefore, you are advised not to \nset the container subnet and node subnet to the same. Otherwise, containers or \nnodes may fail to be created due to insufficient IP resources.\n\u2022\nCCE supports the following container network models: container tunnel network, \nVPC network, and Cloud Native Network 2.0.\n\u2022\nThe container tunnel network is constructed on but independent of the node \nnetwork through tunnel encapsulation. This network model uses VXLAN to \nencapsulate Ethernet packets into UDP packets and transmits them in tunnels. \nOpen vSwitch serves as the backend virtual switch. Though at some costs of \nperformance, packet encapsulation and tunnel transmission enable higher \ninteroperability and compatibility for most scenarios that do not require high "}, {"id": 238, "text": "ackend virtual switch. Though at some costs of \nperformance, packet encapsulation and tunnel transmission enable higher \ninteroperability and compatibility for most scenarios that do not require high \nperformance.\n"}, {"id": 239, "text": "\u2022\nAdvantages: The container network directly uses the VPC, making it easy to locate \nnetwork problems and improve the networking performance. Requests from \nexternal networks in a VPC can be directly routed to a container IP address. Load \nbalancing, security groups, and EIPs provided by the VPC can be directly used.\n\u2022\nDisadvantages: The container network consumes the IP addresses in the VPC. You \nneed to plan the container CIDR block before creating a cluster.\n\u2022\nThis network model is available only to CCE Turbo clusters.\n"}, {"id": 240, "text": "\u2022\nIn CCE, container storage is backed both by Kubernetes-native objects, such as \nemptyDir, hostPath, secret, and ConfigMap, and by cloud storage services. These \ncloud storage services can be accessed via Container Storage Interface (CSI).\n\u2022\nCSI enables Kubernetes to support various classes of storage. For example, CCE \ncan easily interconnect with Huawei Cloud block storage (EVS), file storage (SFS), \nand object storage (OBS).\n\u2022\nCCE provides an add-on named everest to serve as CSI. Everest is a cloud native \ncontainer storage system. Based on CSI, clusters can interconnect with Huawei \nCloud storage services such as EVS, OBS, SFS, and SFS Turbo. everest is a system \nresource add-on. It is installed by default when a cluster of Kubernetes v1.15 or \nlater is created.\n"}, {"id": 241, "text": "\u2022\nEase of use:\n\u25ab\nYou can directly push and pull container images without platform build or \nO&M.\n\u25ab\nSWR provides an easy-to-use management console for full lifecycle \nmanagement over container images.\n\u2022\nSecurity and reliability:\n\u25ab\nSWR supports HTTPS to ensure secure image transmission, and provides \nmultiple security isolation mechanisms between and inside accounts.\n\u25ab\nSWR leverages professional storage services of Huawei to ensure reliable \nimage storage.\n\u2022\nFaster image pull and build:\n\u25ab\nP2P acceleration technology developed by Huawei brings faster image pull \nfor CCE clusters during high concurrency.\n\u25ab\nIntelligent node scheduling around the globe ensures that your image build \ntasks can be automatically assigned to the idle nodes nearest to the image \nrepository.\n"}, {"id": 242, "text": "\u2022\nFrom the practices of customers and partners, there are four typical scenarios of \nusing CCE:\n\u25ab\nFirst, progressive IT architecture upgrade. With CCE, complex applications in \ntraditional architectures are decoupled into multiple lightweight modules. \nEach module is run as a Kubernetes workload. For example, stateless \napplications run as Deployments and stateful applications run as \nStatefulSets. In this way, modules can be flexibly upgraded and scaled to \nmeet changing market demands.\n\u25ab\nSecond, faster service rollout. The same container image can be used \nthrough each phase from R&D to O&M to ensure the consistency of service \nrunning environments. Services can be used out of the box and rolled out \nfaster.\n\u25ab\nThird, auto scaling upon service traffic fluctuation. Containers can be quickly \nscaled within seconds to ensure service performance.\n\u25ab\nFourth, fewer resources and reduced cost. With containers, host resources \ncan be divided at a finer granularity to improve resource utilization.\n"}, {"id": 243, "text": "\u2022\nIn the serverless model, a cloud provider runs servers and dynamically allocates \nresources so that you can build and run applications without having to create, \nmanage, or maintain servers. This model helps you improve development \nefficiency and reduce IT costs.\n\u2022\nCCE provides semi-hosted clusters, while CCI provides fully-hosted clusters that do \nnot need manual management.\n\u2022\nFunctions:\n\u25ab\nCCI provides one-stop container lifecycle management, allowing you to run \ncontainers without creating or managing server clusters.\n\u25ab\nCCI supports multiple types of compute resources, including CPUs, GPUs, and \nAscend chips, to run containers.\n\u25ab\nVarious network access modes and layer-4 and layer-7 load balancing are \navailable to meet scenario-specific needs.\n\u25ab\nCCI can store data on various Huawei Cloud storage volumes, including EVS, \nSFS, and OBS.\n\u25ab\nCCI supports fast auto scaling. Users can customize scaling policies and \ncombine multiple scaling policies to cope with traffic surge during peak \nhours.\n\u25ab\nThe comprehensive container status monitoring of CCI monitors the \nresources consumed by containers, including the CPU, memory, GPU, and \nGPU memory usage.\n\u25ab\nCCI provides dedicated container instances, which run Kata containers on \nhigh-performance physical servers, enabling VM-level security isolation \nwithout performance deterioration.\n"}, {"id": 244, "text": "\u2022\nWith CCI, you can stay focused on your own services, instead of underlying \nhardware and resources. CCI is billed by the second for convenient use anytime.\n\u2022\nDedicated container instances allow you to exclusively use physical servers and \nsupport service isolation among departments. They run Kata Containers on high-\nperformance physical servers, enabling VM-level security isolation without \nperformance loss. Huawei Cloud performs O&M, allowing you to completely \nfocus on your services.\n"}, {"id": 245, "text": "\u2022\nCCI provides VM-level isolation without compromising the startup speed, offering \nyou better container experience. It has the following features:\n\u25ab\nNative support for Kata containers\n\u25ab\nKata-based kernel virtualization, providing comprehensive security isolation \nand protection\n\u25ab\nHuawei-developed virtualization acceleration technologies for higher \nperformance and security\n"}, {"id": 246, "text": "\u2022\nCurrently, most big data and AI training and inference applications (such as \nTensorFlow and Caffe) run in containers. These applications are GPU intensive \nand require high-performance network and storage. As these applications are \ntask-based, resources must be quickly allocated upon task creation and released \nupon task completion, and powerful compute and network resources as well as \nhigh I/O storage are required for high-density computing.\n\u2022\nCCI resources are billed on demand by second, reducing costs.\n\u2022\nVolcano is a batch processing platform based on Kubernetes. It provides a series \nof features required by machine learning, deep learning, bioinformatics, genomics, \nand other big data applications, as a powerful supplement to Kubernetes \ncapabilities. Volcano provides general-purpose, high-performance computing \ncapabilities, such as job scheduling, heterogeneous chip management, and job \nrunning management, serving end users through computing frameworks for \ndifferent industries, such as AI, big data, gene sequencing, and rendering. \n(Volcano has been open-sourced in GitHub.)\n"}, {"id": 247, "text": "\u2022\nNo O&M is required for clusters and servers, which greatly reduces costs.\n"}, {"id": 248, "text": "\u2022\nCCI is tailored for task-based scenarios.\n\u25ab\nThese scenarios include heterogeneous hardware-based AI training and \ninference, training tasks can be hosted on CCI.\n\u25ab\nIt also works in HPC scenarios, such as gene sequencing.\n\u25ab\nThird, burst scale-out in a long-term stable running environment, such as e-\ncommerce flash sales and hot topic-based marketing.\n\u2022\nThe main advantages of CCI are on-demand use for lower costs, and full hosting \nfor O&M-free. It also enables consistency and scalability based on standard \nimages.\n\u2022\nCCI supports pay-per-use or package-based billing. A core-hour indicates the \nnumber of cores multiplied by time. For example, 730 core-hours indicate that \nyou can use 730 cores for one hour or one core for 730 hours.\n\u25ab\nIn pay-per-use mode, you will be charged by second for each instance and \nthe billing statistics are presented by hour.\n\u25ab\nIn package-based billing mode, if your resource usage exceeds the quota of \nthe package within the package validity period, you will be billed for the \nexcess usage on a pay-per-use basis. If you buy multiple packages, resources \nin the package with the earliest expiration time will be used first.\n"}, {"id": 249, "text": "\u2022\nTo work with AOS, you only need to create a template describing the applications \nand the required cloud resources, including their dependencies and references. \nAOS will then set up these applications and provision the resources as specified in \nthe template. For example, when creating an ECS, together with a VPC and a \nsubnet on which the ECS runs, you only need to create a template defining an \nECS, VPC, subnet, and their dependencies. AOS will then create a stack, namely, a \ncollection of resources you specified in the template. After the stack has been \nsuccessfully created, the ECS, VPC, and subnet are available to use.\n\u2022\nProduct functions:\n\u25ab\nAOS provides automatic orchestration of mainstream Huawei Cloud services. \nFor details, see Cloud Services and Resources that Can Be Orchestrated in \nAOS. AOS also provides lifecycle management including resource scheduling, \napplication design, deployment, and modification, to reduce O&M costs \nthrough automation.\n\u25ab\nStandard languages (YAML and JSON) can be used to describe required \nbasic resources, application systems, upper-layer services, and their \nrelationships. Automatic resource provision, application deployment, and \nservice loading can be implemented in a few clicks based on uniform \ndescription and defined dependency relationships. You can manage deployed \nresources and applications in a unified manner.\n\u25ab\nAOS Template Market provides abundant templates for free, including basic \nresource templates, service combination"}, {"id": 250, "text": "tionships. You can manage deployed \nresources and applications in a unified manner.\n\u25ab\nAOS Template Market provides abundant templates for free, including basic \nresource templates, service combination templates, and industry templates, \ncovering common application scenarios. You can use public templates \ndirectly to deploy services in the cloud in a few clicks.\n"}, {"id": 251, "text": "\u2022\nKarmada is a multi-cluster management system built on Kubernetes native APIs. \nIt provides automated multi-cluster management capabilities in a pluggable \nmanner for multi-cloud and hybrid cloud applications. Karmada enables \ncentralized management, high availability, fault recovery, and traffic scheduling.\n\u2022\nMCP leverages cluster federation to implement unified management of clusters \nof different cloud service providers. As a unified entry for multiple clusters, MCP \nsupports dynamic cluster access and global cluster monitoring dashboard.\n\u2022\nBased on the multi-cluster and federation technologies, MCP manages \nKubernetes clusters across regions or clouds and supports full lifecycle \nmanagement of applications across clusters, including deployment, deletion, and \nupgrade, by using standard cluster federation APIs in Kubernetes.\n\u2022\nMCP supports cross-cluster auto scaling policies to balance the pod distribution in \neach cluster and implement global load balancing.\n\u2022\nYou can create federated Services for cross-cluster service discovery. MCP enables \nservice region affinity based on the proximity access principle, reducing network \nlatency.\n\u2022\nMCP is compatible with the latest Kubernetes-community federation architectures, \nKubernetes native APIs and Karmada APIs.\n\u2022\nMCP supports application federation, which allows you to deploy an application \nfrom only one cluster to multiple clusters across clouds in just a few clicks. In this \nway, cross-cloud DR and traffic sharing are imple"}, {"id": 252, "text": "ication federation, which allows you to deploy an application \nfrom only one cluster to multiple clusters across clouds in just a few clicks. In this \nway, cross-cloud DR and traffic sharing are implemented.\n\u2022\nYou can clone or migrate your applications to other clusters or across \nclouds/regions in just a few clicks without re-writing or modifying your service \ncode.\n"}, {"id": 253, "text": "\u2022\nService release: Service providers upload a service package, verify the lifecycle and \nfeatures of the service in the OSC, and release the service as an offering for other \ntenants to subscribe to.\n\u2022\nService subscription: OSC contains Huawei-developed services, services published \nby ecosystem partners, and open source services. All services can be subscribed to \nby users. Instances can be deployed only after successful subscription.\n\u2022\nService unsubscription: Users can unsubscribe from a service at any time. Upon \nunsubscription, the system automatically deletes the deployed services and \ninstances.\n\u2022\nPrivate service uploading: Users can upload services developed based on Helm, \nOperator Framework, or OSC service specifications to OSC as private services for \nmanagement. \n\u2022\nService upgrade: When a provider publishes the updated version of a service, the \nsubscribers will receive an upgrade notification and can decide whether to \nupgrade the service to the latest version.\n\u2022\nInstance deployment: After subscribing to a service, users can deploy an instance, \nspecifying the region, container cluster, and running parameters.\n\u2022\nInstance O&M: OSC provides the O&M view of instances. Users can view the \nmonitoring and logs of instances and switch from the O&M view to the \ncorresponding cloud service for in-depth data analysis.\n\u2022\nInstance update: Users can modify the running configurations of an instance.\n\u2022\nInstance deletion: When the lifecycle of a service running in an instance end"}, {"id": 254, "text": "cloud service for in-depth data analysis.\n\u2022\nInstance update: Users can modify the running configurations of an instance.\n\u2022\nInstance deletion: When the lifecycle of a service running in an instance ends, \nusers can delete the instance to reclaim related resources.\n"}, {"id": 255, "text": "\u2022\nServerless computing does not mean that we no longer use servers to host and \nrun code, nor does it mean that O&M engineers are no longer needed. \nConversely, it means that consumers no longer need to spend time and resources \non configuring, maintaining, updating, or expanding servers, or planning capacity. \nAll these are handled by a serverless platform, enabling developers to focus on \nservice logic and O&M engineers to process key service tasks.\n\u2022\nThere are two serverless architectures:\n\u25ab\nFunctions-as-a-service (FaaS): provides event-driven computing. Developers \nuse functions triggered by events or HTTP requests to run and manage \napplication code. They deploy small units of code to FaaS, where the code is \nexecuted as discrete actions on request, and can be expanded without \nmanaging servers or any other underlying infrastructure.\n\u25ab\nBackend-as-a-service (BaaS): an API-based third-party service that can \nreplace the core function subset in applications. Because these APIs are \nprovided as services that can be automatically expanded and transparently \noperated, they are serverless for developers.\n\u2022\nFaaS executes function code, and BaaS only uses APIs to provide backend services \non which applications depend.\n"}, {"id": 256, "text": "\u2022\nGenerally, serverless is recommended for workloads in the following \nscenarios:Asynchronous, concurrent, easy to be parallelized into independent \nunits.Infrequent requests with huge and unpredictable expansion \nrequirements.Stateless and transient, without instant cold start \nrequirements.Highly dynamic service requirement changes.\n\u2022\nServerless products or platforms have the following benefits:\n\u2022\nNo server O&M: Serverless has significantly changed the application cost model \nby eliminating the overhead involved in maintaining server resources.\n\u25ab\nNo need to configure, update, or manage servers. Managing servers, VMs, \nand containers involves personnel, tools, training, and time.\n\u25ab\nFaaS and BaaS products can be scaled flexibly and precisely to process each \nrequest. For developers, a serverless platform does not need capacity \nplanning or auto scaling triggers or rules.\n\u2022\nNo cost for idle resources: For consumers, a major benefit of serverless products is \nthat idle resources do not incur any cost. For example, idle VMs and containers \nwill not be charged. However, the costs for stateful storage, functions, and \nfeature sets will be charged.\n"}, {"id": 257, "text": "\u2022\nWhen using FunctionGraph, you do not need to apply for or pre-configure any \ncompute, storage, or network services. You only need to upload and run code in \nsupported runtimes. FunctionGraph provides and manages underlying compute \nresources, including CPUs, memory, and networks. It also supports configuration \nand resource maintenance, code deployment, automatic scaling, load balancing, \nsecure upgrade, and resource monitoring.\n\u2022\nFunctionGraph supports Node.js, Java, Python, Go, and C#, allowing you to edit \ncode inline, import OBS files, and upload ZIP and JAR packages. It uses SMN, \nAPIG, and OBS triggers. It collects and displays real-time metrics and logs, and \nenables you to query logs online, making it easy to view function status and \nlocate problems. Function flows orchestrate and coordinate multiple distributed \nfunctions. FunctionGraph provides unified plug-ins for on-/off-cloud development \nand debugging. HTTP functions can be triggered for web service optimization by \nsending HTTP requests to specific URLs. In addition, you can enable tracing on the \nfunction configuration page so that you can view Java virtual machine (JVM) and \ntracing information on the APM console. Currently, this feature is only available \nfor Java functions. You can package and upload container images to \nFunctionGraph for running.\n\u2022\nFunctionGraph 2.0 is a next-generation function computing and orchestration \nservice. It has the following features:\n\u25ab\nDeep integration with CloudIDE, concur"}, {"id": 258, "text": "er images to \nFunctionGraph for running.\n\u2022\nFunctionGraph 2.0 is a next-generation function computing and orchestration \nservice. It has the following features:\n\u25ab\nDeep integration with CloudIDE, concurrent function debugging, tracing, \nwizard-based building, and full lifecycle management\n\u25ab\nSix programming languages and custom runtime, cold startup and auto \nscaling in 100 milliseconds\n\u25ab\nFirst to support stateful functions in China, visualized function orchestration\n\u25ab\nServerless web applications with zero reconstruction\n"}, {"id": 259, "text": "\u2022\nApplication development: out-of-the-box CloudIDE, debugging and tracing of \nclustered serverless applications, code breakpoints, stack viewing, call topologies, \nand hot code replace (HCR)\n\u2022\nCI/CD: deep integration with serverless runtimes; lightweight DevOps with O&M \ntools\n\u2022\nApplication hosting: lifecycle management with unified specifications; templates \nand marketplace for experience and reuse\n\u2022\nCloud application engine (CAE): a one-stop serverless application hosting service \nthat enables ultra-fast deployment at low cost with simple O&M. It releases \napplications from source code, software packages, and image packages, with \nseconds of auto scaling, pay-per-use billing, no infrastructure O&M, and multiple \nobservable metrics.\n"}, {"id": 260, "text": "\u2022\n(On-cloud) CloudIDE: Create a function using a template, view the function and \ndownload it to the cloud, debug it using CloudIDE, and push it to the cloud.\n\u2022\n(Off-cloud) VSCode plug-in: Create a function using a template, view the function \non the cloud, download it to a local host, debug it using VSCode plug-in, and \npush it to the cloud.\n"}, {"id": 261, "text": "\u2022\nHTTP functions are better for optimizing web services and can be triggered by \nsending HTTP requests to specific URLs. You can specify this type when creating a \nfunction. HTTP functions only support APIG and APIC triggers.\n"}, {"id": 262, "text": "\u2022\nThe following challenges may exist when you shift from the traditional \ndevelopment mode to the serverless mode:\n\u25ab\nDifferent runtimes and deliverable formats: The runtime provided by \nserverless function vendors may be Docker or microVM. The deliverable \nformats and function signatures are different. You have to make adaptations.\n\u25ab\nImmature ecosystem: Popular open-source tools (such as CI/CD pipeline) are \nnot supported.\n\u2022\nThe container ecosystem is mature and does not have portability and agile \ndelivery issues. Container images are standard deliverables in the cloud native \nera. However, containers still involve O&M and idle resource costs.\n\u2022\nYou can create custom images for both event and HTTP functions.\n"}, {"id": 263, "text": "\u2022\nAnswer 1: False\n\u25ab\nLabel instead of ConfigMaps\n\u2022\nAnswer 2: ABC\n\u25ab\nSupports multiple languages, such as Node.js, Java, Python, Go, and C#\n"}, {"id": 264, "text": "\u2022\nDiscussion 1:\n\u25ab\nConstruction cost, including equipment, site, and prices\n\u25ab\nO&M costs, including manpower, power, and network costs\n\u25ab\nSecurity\n\u25ab\nConvenience\n\u2022\nDiscussion 2:\n\u25ab\nResponse speed\n\u25ab\nPerformance\n\u25ab\nSecurity\n\u25ab\nMaintainability\n\u25ab\nCost\n\u25ab\nConvenience\n"}, {"id": 265, "text": "\u2022\nEnterprises need to make a trade-off between rapid service development and \nexquisite application architecture. Microservice architecture is the future trend. The \nmicroservice architecture has abundant features, including fault tolerance, quick \nrollout, more complex functions, high availability, requirement response, \nmanageability, and independent module release.\n\u2022\nOn the monolithic architecture, all functions are integrated in one project. The \narchitecture is simple, the development cost in the early phase is low, and the \ndevelopment period is short. Therefore, this architecture is ideal for small-scale \nprojects. However, as the small projects grow larger, it is difficult to develop, \nexpand, and maintain the monolithic architecture.\n\u2022\nProjects using the monolithic architecture are vertically divided, so small projects \ncannot become too large.\n\u2022\nOn the SOA, repeated common functions are extracted as components to provide \nservices for each system. Projects (or systems) communicate with services through \nWebService or remote procedure call (RPC). SOA improves the development \nefficiency and system reusability and maintainability. \n\u2022\nBut the SOA has disadvantages. The boundary between systems and services is \nblurred, which is not conducive to development and maintenance. The granularity \nof the extracted services is too large, and systems are highly coupled with the \nservices.\n\u2022\nThe microservice architecture is an approach to developing a single application as \na sui"}, {"id": 266, "text": " The granularity \nof the extracted services is too large, and systems are highly coupled with the \nservices.\n\u2022\nThe microservice architecture is an approach to developing a single application as \na suite of small services, each running in its own process, and communicating with \nlightweight mechanisms, often an HTTP resource API. Services are split at a finer \ngranularity, which facilitates resource reuse and improves development efficiency. \nIn this way, optimization solutions for each service can be formulated more \naccurately, improving the system maintainability. \n"}, {"id": 267, "text": "\u2022\nThe monolithic architecture is an archive package. The package contains \napplications with all functions. In the early stage of software development, the \nmonolithic architecture is popular because it is easy to deploy, the technologies \nare simple, and the labor cost is low. However, in the Internet era, the complexity \nof service requirements and the delivery frequency increase. The traditional \nmonolithic architecture cannot meet the requirements of developers:\n\u25ab\nThe monolithic architecture is complex as all modules are coupled. They \nhave blurred boundaries and complex dependencies. Function adjustment \nmay bring unknown impacts and potential bugs.\n\u25ab\nWhen a monolithic system encounters a performance bottleneck, the \nsystem can only scale out horizontally and add service instances to balance \nthe load. Vertical expansion and module decoupling are not supported.\n\u25ab\nThe monolithic architecture has poor scalability. A monolithic application \ncan be scaled only as a whole. Scaling of a single module cannot be \nperformed.\n\u25ab\nThe monolithic architecture cannot isolate faults. The entire system may \nbreak down even when a small module is faulty (for example, a request is \nblocked) as all function modules are aggregated.\n\u25ab\nOn the monolithic architecture, the release impact is large. The entire \nsystem is released each time and the system restarts upon each release. \nThis poses a great challenge to a large-scale integrated system. If we \ndecouple each module, only the modified modu"}, {"id": 268, "text": "he entire \nsystem is released each time and the system restarts upon each release. \nThis poses a great challenge to a large-scale integrated system. If we \ndecouple each module, only the modified module needs to be released.\n\u25ab\nThe deployment slows down. The build and deployment duration increases \nas the code size increases.\n\u25ab\nTechnological innovation is hindered. A monolithic application solves all \nproblems using a unified technical platform or solution. Each team member \nmust use the same development language and architecture. \n"}, {"id": 269, "text": "\u2022\nSOA decouples applications, modularizes them, and builds functions into \nindependent units to provide services.\n\u2022\nSOA contains multiple services. The services communicate with each other \nthrough mutual dependency or communication mechanisms to provide a series \nof functions. A service independently exists in an OS process. Services invoke each \nother through networks.\n\u2022\nSOA solves the following problems:\n\u25ab\nFirst, system integration. SOA sorts out the mesh structure between \nscattered and unplanned systems into a regular and governable star \nstructure. Some products, such as the ESB, technical specifications, and \nservice management specifications, need to be introduced.\n\u25ab\nSecond, system as a service. SOA abstracts service logic into reusable and \nassemblable services and orchestrates the services to quickly regenerate \nservices. This transforms inherent functions into common services to quickly \nreuse business logic.\n\u25ab\nThird, business as a service. SOA abstracts enterprise functions into reusable \nand assemblable services. It transforms the enterprise architecture into a \nservice-oriented one to provide better services. SOA solves the problems of \nsystem invoking and system function reuse from the technical perspective.\n"}, {"id": 270, "text": "\u2022\nThe term bus is an extension of a physical bus that transports bits between \ndifferent devices of a computer. An ESB provides similar functions at a higher \nabstraction level. In an enterprise architecture that uses an ESB, applications \ninteract with each other through the bus, and the bus schedules information \nbetween applications. ESB reduces the number of point-to-point connections \nrequired for interaction between applications, making it easier and more intuitive \nto analyze the impact of major software changes. Reconstruction of a component \nin the system also becomes simple.\n\u2022\nAn ESB provides reliable message transmission, service access, protocol \nconversion, data format conversion, and content-based routing regardless of \nphysical locations, protocols, and data formats.\n\u2022\nIn the future, the enterprise integration architecture will integrate application \nAPIs, messages, devices, data, and multiple clouds, and connect all applications, \nbig data, cloud services, devices, and partners of enterprises. The traditional \n\"integration factory\" mode controlled by IT teams will be transformed to the \nself-service integration mode that is supported by business lines, subsidiaries, \napplication development teams, and end users, that is, the \"unified hybrid \nintegration platform.\"\n"}, {"id": 271, "text": "\u2022\nThe origin of microservices was Micro-Web-Service proposed by Dr. Peter \nRodgers at the 2005 Cloud Computing Expo. Juval Lowy had a similar idea, that \nis, to turn categories into granular services. The core was that services are used \nby Unix-like pipelines. In 2014, Martin Fowler and James Lewis jointly proposed \nthe concept of microservices. The concept defines microservices as small services \ncomposed of single applications and has its own schedule and lightweight \nprocessing. Services are designed according to functions and automatically \ndeployed. They communicate with other services using HTTP APIs. In addition, \nservices are managed at the minimum scale (such as Docker), and implemented \nusing different programming languages and components such as libraries.\n\u2022\nMicroservice is an architecture and organization method for developing software. \nSoftware consists of small independent services that communicate with each \nother through clearly defined APIs.\n\u2022\nThe microservice architecture emerged because any small change in a monolithic \narchitecture affects all other modules. Any small change on the cloud needs to \nbe compiled and released in a unified manner. If a module needs to be extended, \nthe whole architecture needs to be extended. Therefore, a series of microservices \nare used to build applications. Microservices can be independently deployed and \nexpanded, and can be developed using different languages. Moreover, modular \nboundaries are provided. \n"}, {"id": 272, "text": "\u2022\nIn a microservice architecture, the entire web application is organized into a \nseries of small web services. These small web services can be compiled and \ndeployed independently and communicate with each other through their exposed \nAPIs. They cooperate with each other to provide functions for users as a whole, \nbut can be expanded independently.\n\u2022\nMicroservice is a software architecture style based on small building blocks that \nfocus on single responsibilities and functions. It combines complex, large-scale \napplications as modules. Functional blocks communicate with each other using \nlanguage-independent or language-agnostic APIs.\n\u2022\nMicroservices follow the design concept that focus on functions. During \napplication design, an application can be divided based on functions or processes. \nEach function is independently implemented as a service that can be \nindependently executed. Then, the same protocol is used to combine all the \nservices to form an application. If a specific function needs to be extended, you \nonly need to operate that function, not the entire application. \n\u2022\nThe API gateway is generally located on the execution path of each API request. \nIt belongs to the data plane, receives requests from clients, reversely proxies the \nrequests to underlying APIs and execute traffic control and user policies before \nthat. Before proxying the request back to the original client, it can also respond \nto the instructions of the underlying API and execute the correspondi"}, {"id": 273, "text": "cute traffic control and user policies before \nthat. Before proxying the request back to the original client, it can also respond \nto the instructions of the underlying API and execute the corresponding policy \nagain.\n\u2022\nRESTful APIs are REST-styled. RESTful is a development and design style of \nnetwork applications and can be defined in XML or JSON format.\n"}, {"id": 274, "text": "\u2022\nComplexity is solved by decomposing huge monolithic applications into multiple \nservices. An application is divided into multiple manageable branches or services \nwithout changing functions. Each service is defined through APIs.\n\u2022\nThe microservice architecture provides a modular solution for functions that are \nimpossible in monolithic encoding. A single service is easy to develop, understand, \nand maintain.\n\u2022\nMicroservices are independently implemented and deployed, that is, they run in \nindependent processes. Therefore, they can be independently monitored and \nexpanded.\n\u2022\nIn the microservice architecture, each microservice is independently deployed. \nDevelopers do not need to worry about whether other services will impact the \nmicroservices.\n\u2022\nThe microservice architecture enables each service to be developed by a \ndedicated development team. Developers can freely choose development \ntechnologies to provide API services.\n"}, {"id": 275, "text": "\u2022\nIn the early stage, communication between computers required a physical layer \nto transmit bytecodes and electronic signals at the bottom layer. In addition to \nservice logic, services also needed to handle a series of network transmission \nproblems such as packet loss, disorder, and retry.\n\u2022\nIn the 1980s, TCP was published, solving the common traffic control problems in \nnetwork transmission. The technology stack was moved downwards and \nextracted from services to become a part of the network layer in the OS.\n\u2022\nIn the 1990s, network communication between computers was no longer a \nproblem. Distributed systems represented by GFS, BigTable, and MapReduce \ndeveloped rapidly. Communication semantics specific to distributed systems \nemerged, such as circuit breaker policies, load balancing, service discovery, \nauthentication and authorization, quota limit, and monitoring. Each service \nneeded to implement the required semantics.\n\u2022\nTo address this issue, microservice-oriented development frameworks with \ncommon semantic functions are developed, including Finagle from Twitter, \nProxygen from Facebook, and Spring Cloud.\n\u2022\nHowever, developers still need to handle the complex frameworks, and track and \nsolve framework problems. In addition, the frameworks usually support only one \nor several languages. Services that are not written with the framework-supported \nlanguages are difficult to integrate into the frameworks. Therefore, the proxy \n(sidecar) mode represented by Linkerd and E"}, {"id": 276, "text": "veral languages. Services that are not written with the framework-supported \nlanguages are difficult to integrate into the frameworks. Therefore, the proxy \n(sidecar) mode represented by Linkerd and Envoy emerges. This is the first-\ngeneration Service Mesh.\n\u2022\nTo provide a unified upper-layer O&M portal, a centralized control panel is \nintroduced. All single-node agent components interact with the control panel to \nupdate network topology policies and report data. In this model, each service is \npaired with a sidecar proxy. Services communicate with each other only through \nsidecars. This is the second-generation Service Mesh, represented by Istio, a joint \n"}, {"id": 277, "text": "project launched by Google, IBM, and Lyft.\n"}, {"id": 278, "text": "\u2022\nSpring Cloud is an ordered set of microservice solutions or frameworks for \nbuilding distributed microservice systems. Based on Spring Boot, it encapsulates \nmature and verified microservice frameworks in the market to shield complex \nconfigurations and implementation principles.\n\u2022\nSpring Cloud sub-projects can be classified into two types. Most sub-project are \nfor encapsulating and abstracting mature frameworks by using Spring Boot. The \nother type is for implementing infrastructure with certain distributed systems \ndeveloped. For example, Spring Cloud Stream works similarly to Kafka or \nActiveMQ.\n\u2022\nSpring Boot is a new framework provided by the Pivotal team. It simplifies the \ninitial setup and development process of Spring applications. The framework is \nconfigured in a specific way so that developers no longer need to define a \ntemplated configuration. In this way, Spring Boot is committed to becoming a \nleader in the rapid application development field.\n\u2022\nSpring Cloud has the following features:\n\u25ab\nIt has strong support from companies such as Netflix, and active \ncontributions from the Spring open source Community.\n\u25ab\nBy combining mature microservice products and frameworks in a \nstandardized manner, Spring Cloud delivers a complete set of microservice \nsolutions to reduce development costs and risks.\n\u25ab\nThanks to Spring Boot, Spring Cloud features simple configuration, quick \ndevelopment, easy deployment, and convenient testing.\n\u25ab\nSpring Cloud can call REST services. Co"}, {"id": 279, "text": "development costs and risks.\n\u25ab\nThanks to Spring Boot, Spring Cloud features simple configuration, quick \ndevelopment, easy deployment, and convenient testing.\n\u25ab\nSpring Cloud can call REST services. Compared with RPC, REST is more \nlightweight and flexible. REST services depend on a contract rather than \ncode, facilitating cross-language implementation, release and deployment.\n\u25ab\nSpring Cloud is compatible with Docker and Kubernetes microservice \norchestration.\n"}, {"id": 280, "text": "\u2022\nThe term service mesh was first proposed by Buoyant and first publicly used in \n2016. In 2017, Buoyant released its first Service Mesh product, Linkerd, and an \narticle What's a service mesh? And why do I need one? The article provides an \nauthoritative definition of service mesh.\n\u2022\nWithout a service mesh layer, the logic governing communication can be coded \ninto each service. However, as the communication between microservices and for \nlogical governing becomes more complex, service meshes are required to \nintegrate a large number of discrete services into one functional application.\n\u2022\nSimilar to a TCP/IP layer between applications or microservices, a service mesh is \nresponsible for network invoking, rate limiting, outlier detection, and monitoring \nbetween services. Developers usually do not need to pay attention to the TCP/IP \nlayer when developing applications. Similarly, service meshes free developers \nfrom what service frameworks like Spring Cloud and Netflix OSS can implement.\n\u2022\nWithout a service mesh, each microservice is coded with logic to govern service-\nto-service communication, which means developers are less focused on service \ndevelopment. It is also more difficult to diagnose communication failures because \nthe logic that governs inter-service communication is hidden within each service.\n\u2022\nService Mesh describes the network of microservices that make up applications \nand the interactions between applications. As a service mesh grows in size and \ncomplexity"}, {"id": 281, "text": " hidden within each service.\n\u2022\nService Mesh describes the network of microservices that make up applications \nand the interactions between applications. As a service mesh grows in size and \ncomplexity, it can become harder to understand and manage. You need to take \ncare of basic operations, such as service discovery, load balancing, failure \nrecovery, metrics, and monitoring. Advanced O&M includes blue-green \ndeployment, canary release, rate limiting, access control, and end-to-end \nauthentication.\n"}, {"id": 282, "text": "\u2022\nRules must be defined in the logical governing layer of each service for \ncommunication between microservices. A service mesh extracts the rules from \neach service and abstracts them as an infrastructure layer. The service mesh does \nnot add new functions to the runtime environment of each microservice. When \nmicroservices communicate with each other, requests are routed through the \nproxies of the service mesh. The proxies are also called sidecars. They run \nindependently alongside services and form a mesh network. Sidecar proxies work \nwith microservices to route requests to other proxies.\n\u2022\nA sidecar is a design pattern which separates certain functionality or a set of \nfunctionalities from the application into a separate process. It can add \nfunctionality non-intrusively to the application without adding additional code to \nmeet third-party requirements. In software architecture, a sidecar is loosely \ncoupled with a main or parent application to extend and enhance functionality.\n\u2022\nIn a service mesh, services and their sidecar proxies constitute a data plane for \ndata management, request processing and response. A service mesh also includes \na control plane for managing interactions between services that are coordinated \nby sidecar proxies.\n\u2022\nIn the service mesh workflow, the control plane pushes service configurations in \nthe entire mesh to the sidecar proxy of each node. The routing information can \nbe dynamically configured for all or certain services. After confirmin"}, {"id": 283, "text": "e control plane pushes service configurations in \nthe entire mesh to the sidecar proxy of each node. The routing information can \nbe dynamically configured for all or certain services. After confirming the \ndestination address, the sidecar sends the traffic to the corresponding service \ndiscovery endpoint.\n"}, {"id": 284, "text": "\u2022\nServiceComb, the top microservice project, was contributed by Huawei and \nincubated by Apache in 2017. It is the first Apache-incubated microservice \nproject.\n"}, {"id": 285, "text": "\u2022\nIstio is an open source service mesh layered transparently onto existing \ndistributed applications. It is also a platform that has APIs for connecting log \nrecord platform, telemetry, or policy system. Istio provides a uniform and more \nefficient way to run the distributed microservice architecture and secure, connect, \nand monitor microservices.\n\u2022\nEarlier, the data plane SideCar proxy is unstable and traffic-intensive as Service \nMesh puts too many functions, including the inter-service communications and \nrelated governance into it. As a solution to these problems, the second-\ngeneration Service Mesh emerges and separates the configuration policy and \ndecision logic from the proxy servers to form an independent control plane.\n\u2022\nIstio has two components: the data plane and the control plane.\n\u25ab\nThe data plane is the communication between services. Without a service \nmesh, the network cannot figure out the type, source, and destination of \nthe traffic.\n\u25ab\nThe control plane takes your desired configuration, and its view of the \nservices, and dynamically programs the proxy servers, updating them as the \nrules or environments change.\n"}, {"id": 286, "text": "\u2022\nIstio service mesh has two components: the data plane (Envoy) and the control \nplane (Istiod).\n\u25ab\nEnvoy is a high-performance proxy developed in C++ to mediate all \ninbound and outbound traffic for all services in the service mesh. Envoy \nproxies are deployed as sidecars to services and are the only Istio\ncomponents that interact with data plane traffic. In addition to load \nbalancing, circuit breakers, and fault injection, Envoy also supports a \npluggable extension model built on WebAssembly (Wasm) that allows for \ncustom policy enforcement and telemetry generation for mesh traffic.\n\u25ab\nIstiod is a control plane component that provides service discovery, \nconfiguration, and certificate management. Istiod converts advanced rules \nwritten in YAML into Envoy-specific configurations and propagates them to \nthe sidecars. Pilot abstracts platform-specific service discovery mechanisms \nand synthesizes them into a standard format that sidecars can consume. \nCitadel enables strong service-to-service and end-user authentication with \nbuilt-in identity and credential management. You can also use Istio's\nauthorization feature to control who can access your services.\n\u2022\nAs a core component of Istio, Pilot manages and configures all sidecar proxies \ndeployed in a specific Istio service mesh. As a component responsible for \nconfiguration management, Galley verifies the format and content of the \nconfiguration information and provides it for the Pilot on the control plane. \nCitadel consists o"}, {"id": 287, "text": " a component responsible for \nconfiguration management, Galley verifies the format and content of the \nconfiguration information and provides it for the Pilot on the control plane. \nCitadel consists of the CA server, security discovery server, and certificate key \ncontroller.\n\u2022\nCore concepts of Istio:\n\u25ab\nData plane components are injected as non-intrusive sidecars into service \ncontainers, with transparent traffic hijacking.\n\u25ab\nUpper-level APIs are implemented based on Kubernetes CRDs, fully \ndeclarative and standardized.\n\u25ab\nThe data plane and control plane communicate with each other through \nstandard protocols, allowing pub/sub messaging.\n"}, {"id": 288, "text": "\u2022\nIstio extends Kubernetes to establish a programmable, application-aware \nnetwork using the powerful Envoy service proxy. Working with both Kubernetes \nand traditional workloads, Istio simplifies deployment with standard, universal \ntraffic management, telemetry, and security.\n\u2022\nIstio aims to achieve scalability and meet various deployment requirements. Istio\ncontrol plane runs on Kubernetes. In this way, applications deployed in a cluster \ncan be added to your mesh. In addition, the mesh can be extended to other \nclusters, and even connected with VMs or other endpoints running outside \nKubernetes.\n\u2022\nTo enable Istio, you only need to deploy a special sidecar proxy in the \nenvironment and use the Istio control plane to configure and manage the proxy \nto intercept all network communication between microservices. You can use Istio\nto achieve:\n\u25ab\nAutomatic load balancing for HTTP, gRPC, WebSocket, and TCP traffic\n\u25ab\nFine-grained control of traffic behavior with rich routing rules, retries, \nfailovers, and fault injection\n\u25ab\nA pluggable policy layer and configuration API supporting access control, \nrate limits and quotas\n\u25ab\nAutomatic metrics, logs, and traces for all traffic within a cluster, including \ncluster ingress and egress\n\u25ab\nSecure service-to-service communication in a cluster with strong identity-\nbased authentication and authorization\n\u2022\nGoogle Remote Procedure Call (gRPC) is a high-performance open source RPC \nsoftware framework built on the HTTP 2.0 transport layer protocol"}, {"id": 289, "text": "h strong identity-\nbased authentication and authorization\n\u2022\nGoogle Remote Procedure Call (gRPC) is a high-performance open source RPC \nsoftware framework built on the HTTP 2.0 transport layer protocol. It provides an \nAPI design method for managing and configuring network devices. gRPC\nsupports multiple programming languages, such as C, Java, Golong, and Python.\n"}, {"id": 290, "text": "\u2022\nASM supports smooth access and unified governance of multiple applications, \nsuch as containers, traditional microservices, and third-party services. It enables \nhybrid management of cross-cluster traffic under various network conditions in \nmulti-cloud and hybrid cloud scenarios. Large-scale meshes are provided for \nintelligent O&M and scaling to help you automatically and transparently manage \napplication access.\n\u2022\nASM provides a high-performance, low-loss, lightweight, and multi-form mesh \ndata plane and supports uninstallation by pod and node, accelerating sidecar \nforwarding. Flexible topology learning optimizes configurations and resources on \nthe mesh control plane.\n\u2022\nASM can well resolve application network governance issues such as challenges \nin cloud native application management, network connection, and security \nmanagement.\n\u2022\nASM is deeply integrated with CCE to manage application traffic and lifecycle in a \nnon-intrusive manner. ASM enhances the full-stack capabilities of Huawei Cloud \ncontainer services with better usability, reliability, and visualization.\n"}, {"id": 291, "text": "\u2022\nHybrid deployment: Unified governance of hybrid deployment of VM applications \nand containerized applications\n\u2022\nObservability: Out-of-the-box usability and end-to-end intelligent monitoring, \nlogs, topologies, and tracing\n\u2022\nUnified service governance in the multi-cloud and hybrid cloud scenarios, unified \nservice governance of multiple infrastructure resources (multi-container \ncluster/container-VM/VM-PM), and cross-cluster grayscale release, topology, and \ntracing\n\u2022\nProtocol extension: Solution of integrating with microservice SDKs for Spring \nCloud\n\u2022\nCommunity and open source: No. 3 in the world by contribution to Istio\ncommunity; quick response to community version issues and requirements\n"}, {"id": 292, "text": "\u2022\nGrayscale release policies:\n\u25ab\nGrayscale policies based on request content: You can set criteria based on \nrequest content, such as header and cookie. Only requests meeting the \ncriteria will be distributed to the grayscale version.\n\u25ab\nGrayscale policies based on traffic ratio: You can set specific ratio for the \ntraffic to be distributed to the grayscale version.\n\u25ab\nCanary release: Guidance will be provided to help you perform canary \nrelease on a service, including rolling out a grayscale version, observing the \nrunning and traffic of the grayscale version, configuring grayscale release \npolicies, and diverging the traffic.\n\u25ab\nBlue-green deployment: Guidance will be provided to help you perform \nblue-green deployment on a service, including rolling out a grayscale \nversion, observing the running and traffic of the grayscale version, and \nswitching the traffic.\n"}, {"id": 293, "text": "\u2022\nAn O&M-free hosting control plane is provided. Unified service governance, \ngrayscale release, security, and service running monitoring capabilities for \nmultiple clouds and clusters are supported. Unified service discovery and \nmanagement of multiple infrastructure resources such as containers and VMs are \nprovided.\n\u2022\nThe meshes of multiple clusters share a set of root certificates. They distribute \nkeys and certificate pairs to service pods in the data plane, and periodically \nchange key certificates. Key certificates can be revoked as required. When a \nservice calls another service, the mesh data plane envoy performs two-way \nauthentication and channel encryption. These two services can come from two \ndifferent clusters. Transparent end-to-end two-way authentication across clusters \nis supported.\n\u2022\nLoad balancing, service routing, fault injection, outlier detection, and fault \ntolerance policies can be intuitively configured using an application topology. \nMicroservice traffic management can be real-time, visualized, intelligent, and \nautomated, requiring no modifications on your applications.\n\u25ab\nRouting rules based on weight, content, and TCP/IP implements flexible \ngrayscale release of applications.\n\u25ab\nHTTP sticky session achieves service processing continuity.\n\u25ab\nRate limiting and outlier detection ensure stable and reliable links \nbetween services.\n\u25ab\nNetwork persistent connection management saves resources and improves \nnetwork throughput.\n\u25ab\nService security certificati"}, {"id": 294, "text": "ng and outlier detection ensure stable and reliable links \nbetween services.\n\u25ab\nNetwork persistent connection management saves resources and improves \nnetwork throughput.\n\u25ab\nService security certification, authentication, and audit lay a solid \nfoundation for service security assurance.\n"}, {"id": 295, "text": "\u2022\nLoad balancing, service routing, fault injection, outlier detection, and fault \ntolerance policies can be intuitively configured using an application topology. \nMicroservice traffic management can be real-time, visualized, intelligent, and \nautomated, requiring no modifications on your applications.\n\u25ab\nRouting rules based on weight, content, and TCP/IP implements flexible \ngrayscale release of applications.\n\u25ab\nHTTP sticky session achieves service processing continuity.\n\u25ab\nRate limiting and outlier detection ensure stable and reliable links between \nservices.\n\u25ab\nNetwork persistent connection management saves resources and improves \nnetwork throughput.\n\u25ab\nService security certification, authentication, and audit lay a solid \nfoundation for service security assurance.\n\u2022\nRequests can be distributed based on the request content (browsers or OSs).\n\u2022\nRequests can be distributed based on traffic ratio.\n"}, {"id": 296, "text": "\u2022\nContainer-based infrastructure brings a series of new challenges. It is necessary \nto evaluate and enhance the performance of API endpoints and identify potential \nrisks of infrastructure. ASM enables you to enhance API performance with no \ncode refactoring and service delay.\n\u2022\nIn traditional iterations, a new service version is directly released to all users at a \ntime. This is risky, because once an online accident or a bug occurs, the impact on \nusers is great. It could take a long time to fix the issue. Sometimes, the version \nhas to be rolled back, which severely affects user experience. Grayscale release is \na smooth iteration mode for version upgrade. During the upgrade, some users \nuse the new version, while other users continue to use the old version. After the \nnew version is stable and ready, it gradually takes over all the live traffic.\n"}, {"id": 297, "text": "\u2022\nMain features:\n\u25ab\nEase of use: Instances created in minutes; out of the box with visual \noperations and real-time monitoring\n\u25ab\nReliability: Cross-AZ deployment, automatic fault detection, alarms, and \nfailover; fixes for open-source availability issues (split brains or multiple \ncontrollers)\n\u25ab\nProven success: Widely deployed in customer cloud; major e-commerce \nevents (VMALL 11.11 Shopping Festival); open-source community links; \ncustomer trusted choice\n"}, {"id": 298, "text": "\u2022\nZooKeeper: a distributed coordination application that stores Kafka metadata.\n\u2022\nClients:\n\u25ab\nProducer: a client application that continuously publishes messages to one \nor more topics.\n\u25ab\nConsumer: a client that subscribes to one or more topics.\n\u2022\nServer: consists of service processes called brokers. A Kafka cluster consists of \nmultiple brokers.\n\u2022\nKafka: distributed message stream processing middleware.\n\u2022\nBroker: receives and processes requests from clients and persists messages.\n\u2022\nTopic: a publish/subscription object in Kafka. Dedicated topics can be created for \neach service, application, or even each category of data. Topics are divided into \npartitions.\n\u2022\nHigh availability mechanism of Kafka:\n\u25ab\nDifferent brokers run on different machines. If one broker is down, other \nbrokers can still provide services for external systems.\n\u25ab\nThe same data is replicated to multiple machines.\n"}, {"id": 299, "text": "\u2022\nImmediate use: DMS for RabbitMQ provides single-node and cluster instances \nwith a range of specifications for you to choose from. Instances can be created \nwith just a few clicks on the console, without requiring you to prepare servers.\n\u2022\nRich features: DMS for RabbitMQ supports Advanced Message Queuing Protocol \n(AMQP) and a variety of messaging features such as message broadcast, delayed \ndelivery, and dead letter queues.\n\u2022\nFlexible routing: In RabbitMQ, an exchange receives messages from producers \nand pushes the messages to queues. RabbitMQ provides direct, topic, headers, \nand fanout exchanges. You can also bind and customize exchanges.\n\u2022\nHigh availability: In a RabbitMQ cluster, data is replicated to all nodes through \nmirrored queues, preventing service interruption and data loss in case of a node \nbreakdown.\n\u2022\nMonitoring and alarm: RabbitMQ cluster metrics are monitored and reported, \nincluding broker memory, CPU usage, and network flow. If an exception is \ndetected, an alarm will be triggered.\n\u2022\nAMQP is an advanced message queue protocol at the application layer of the \nunified messaging service. It is an open standard application layer protocol for \nmessage-oriented middleware. A client and message middleware developed \nbased on this protocol can exchange messages without product or programming \nlanguage barriers.\n"}, {"id": 300, "text": "\u2022\nSupported message types:\n\u25ab\nNormal messages: Messages that do no have any features of delayed \nmessages, ordered messages, or transactional messages.\n\u25ab\nDelayed/Scheduled messages: Messages that are delivered to consumers \nafter a specific period after being sent from producers to DMS for \nRocketMQ.\n\u25ab\nOrdered messages: Messages that are retrieved in the exact order that they \nare created.\n\u25ab\nTransactional messages: Messages that achieve eventual consistency, \ndelivering distributed transaction processing similar to X/Open XA.\n\u2022\nProducer: a program that delivers messages.\n\u2022\nConsumer: a program that receives messages.\n\u2022\nnamesrv: stores topic routing information. Clients must access namesrv to obtain \ntopic routing information before production and consumption.\n\u2022\nMaster: receives production and consumption requests from clients.\n\u2022\nSlave: functions as a replica node and receives replicated data from master.\n\u2022\nRaft consensus algorithm ensures data consistency between the master and slave \nnodes. Automatic failover is performed between these nodes in the same group.\n\u2022\nBroker: receives and processes client requests and persists messages. The three \nnodes in a broker work in master/slave mode.\n"}, {"id": 301, "text": "\u2022\nRabbitMQ supports persistence with the firehose feature or the rabbitmq_tracing \nplugin. However, rabbitmq_tracing reduces performance and should be used only \nfor troubleshooting.\n\u2022\nThe performance of message-oriented middleware is measured by throughput. \nWhile RabbitMQ provides tens of thousands of QPS, Kafka provides millions. \nHowever, if idempotency and transactions are enabled for Kafka, its performance \nwill be compromised.\n"}, {"id": 302, "text": "\u2022\nThe microservice architecture includes remote procedure call (RPC) \ncommunication between microservices, distributed microservice instances and \nservice discovery, external and dynamic configurations, centralized configuration \nmanagement, microservice governance capabilities (such as circuit breaker, \nisolation,and load balancing), tracing, and log collection and retrieval.\n\u2022\nThe microservice architecture consists of the following:\n\u25ab\nRPC communication between microservices. Using RPC for communication \nreduces coupling between microservices and makes the system more open \nwith fewer technological restrictions.\n\u25ab\nDistributed microservice instances and service discovery. The microservice \narchitecture focuses on resilience and the microservice design is generally \nstateless. Increasing stateless microservice instances lets you improve \nprocessing performance. When there are a large number of instances, a \nmiddleware that supports service registry and discovery is required for \nmicroservice calling and addressing.\n\u25ab\nDynamic and centralized configuration management. Configuration \nmanagement is increasingly complex as the number of microservices and \ninstances increases. The configuration management middleware provides a \nunified view for all microservices, simplifying their configuration \nmanagement. These governance capabilities can mitigate the impact of \nsome common faults of the microservice architecture on the services.\n\u25ab\nTracing and centralized log collection and retrie"}, {"id": 303, "text": "guration \nmanagement. These governance capabilities can mitigate the impact of \nsome common faults of the microservice architecture on the services.\n\u25ab\nTracing and centralized log collection and retrieval. Viewing logs remains \nthe most commonly used method for analyzing system faults. Tracing \ninformation helps locate faults and analyze performance bottlenecks.\n"}, {"id": 304, "text": "\u2022\nThe purpose of planning the development environment is to ensure that \ndevelopers can better work in parallel, reduce dependencies, reduce the workload \nof environment setup, and reduce the risks of bringing the production \nenvironment online.\n\u25ab\nSet up a local development environment on the intranet. The advantage of \nthe local development environment is that each service domain or \ndeveloper can set up a minimum function set environment that meets their \nrequirements to facilitate log viewing and code debugging. The \ndisadvantage of local development environment is the low integration. \nWhen the integration and joint commissioning are required, it is difficult to \nensure environment stability.\n\u25ab\nThe cloud-based test environment is a relatively stable integration test \nenvironment. After the local development and test are complete, each \nservice domain deploys their own services in the cloud test environment \nand can invoke services in other domains for integration tests. These test \nenvironments are integrated in ascending order. \n\u25ab\nThe production environment is a formal service environment. It needs to \nsupport dark launch upgrades, online joint commissioning, and traffic \ndiversion to minimize the impact of upgrade faults on services.\n\u25ab\nIn the cloud-based test environment, the public IP addresses of CSE and \nmiddleware can be opened, or network interconnection can be \nimplemented. In this way, the middleware on the cloud can be used to \nreplace the local environment, red"}, {"id": 305, "text": "he public IP addresses of CSE and \nmiddleware can be opened, or network interconnection can be \nimplemented. In this way, the middleware on the cloud can be used to \nreplace the local environment, reducing the time for developers to install \nthe environment. \n"}, {"id": 306, "text": "\u2022\nMicroservices and components provide a technical basis for large-scale \ncollaborative development and a unified framework for internal sharing. By the \nbeginning of 2021, AppGallery already had more than 300 microservices\navailable, with more than 10,000 instances deployed on the live network. More \nthan 500 dynamic layout cards have been developed on the client, and more \nthan 100 components have been built.\n\u2022\nAppGallery Connect: provides developers with full-lifecycle mobile app services, \ncovering all devices and scenarios, reducing development costs, improving \noperation efficiency, and facilitating business success.\n"}, {"id": 307, "text": "\u2022\nYou can open your services and data by directly providing open APIs to API callers \nor releasing them on KooGallery for monetization.\n\u2022\nYou can also obtain and call open APIs from APIG to reduce your development \ntime and costs.\n\u2022\nBy using APIG, you can monetize services while reducing R&D investment for \nmore business focus and higher operational efficiency. For example, enterprise A \nhas created a mobile number location lookup API in APIG and released it on \nKooGallery. Enterprise B obtains and calls the API from KooGallery and pays for \nthe fee incurred. In this way, enterprise A monetizes its services and enterprise B \nreduces its development time and costs, achieving shared success.\n"}, {"id": 308, "text": "\u2022\nSwagger is a standard, complete framework for generating, describing, invoking, \nand visualizing RESTful web services. It aims to define standard, language-\nindependent RESTful APIs. It enables people and computers to discover and \nunderstand services without accessing source code or documentation or \nmonitoring network traffic.\n"}, {"id": 309, "text": "\u2022\nDevCloud consists of the following services:\n\u25ab\nProjectMan: provides agile project management and collaboration, supports \nmanagement of sprints, milestones, and requirements across projects, \ntracks bugs, and provides multi-dimensional statistics reports.\n\u25ab\nCodeHub: a Git-based online code hosting service for software developers. \nIt is also a code repository for security management, member and \npermission management, branch protection and merging, online editing, \nand statistics. The service addresses issues such as cross-region \ncollaboration, multi-branch concurrent development, and code version \nmanagement.\n\u25ab\nCloudPipeline: provides visualized, customizable pipelines to shorten the \ndelivery period and improve efficiency.\n\u25ab\nCodeCheck: manages code quality in the cloud. You can easily perform \nstatic checks and security checks on code in multiple programming \nlanguages and obtain comprehensive quality reports. CodeCheck also \nallows you to view grouped defects with fix suggestions provided, \neffectively controlling quality.\n\u25ab\nCloudBuild: provides an easy-to-use hybrid language build platform to \nimplement cloud-based build, and supports continuous and efficient \ndelivery. With CloudBuild, you can create, configure, and execute build tasks \nwith a few clicks to obtain, build, and package code automatically and \nmonitor build status in real time.\n\u25ab\nCloudDeploy: provides visualized, one-click deployment. It supports \ndeployment on VMs or containers by using Tomcat, Spring B"}, {"id": 310, "text": " and package code automatically and \nmonitor build status in real time.\n\u25ab\nCloudDeploy: provides visualized, one-click deployment. It supports \ndeployment on VMs or containers by using Tomcat, Spring Boot, and other \ntemplates or by flexibly orchestrating atomic actions. It also supports \nparallel deployment and seamless integration with CloudPipeline, providing \nstandard deployment environments and implementing automatic \n"}, {"id": 311, "text": "deployment.\n"}, {"id": 312, "text": "\u2022\nE2E process: One platform covers common functions in software development. \nThese functions are embedded and integrated for governance and O&M.\n\u2022\nOver 20 mainstream programming languages, development frameworks, and \nrunning environments, for seamless application migration.\n\u2022\nSecure and trustworthy: DevCloud provides security testing, trustworthiness \nbuilding, high security standards, and 7,000+ code check rules.\n"}, {"id": 313, "text": "\u2022\nEvolving from waterfall, agile, to DevOps is the technical route for modern \ndevelopers to build excellent products. New CI/CD methods rise with DevOps. \nTraditional software development and delivery methods are rapidly becoming \noutdated. In the agile era, most companies released software every month, every \nquarter, or even every year. In the DevOps era, it is normal that software is \nreleased every week, every day, or even multiple times a day. This is especially \ntrue when SaaS becomes popular in the industry. Applications can be updated \ndynamically without forcing users to download updated components. Many times, \nusers are not even aware of changes.\n\u2022\nCI focuses on integrating the work of each developer into a code repository, \nwhich is performed several times a day. The main purpose is to detect integration \nerrors as early as possible, so that the team can collaborate better. Continuous \ndelivery (CD) aims to minimize the inherent team friction during deployment or \nrelease. It automates each build and deployment step so that code release can be \nsecurely completed at any time (ideally). Continuous deployment (CD) is more \nautomated. Whenever the code is greatly changed, the build/deployment is \nautomatically performed.\n"}, {"id": 314, "text": "\u2022\nA project consists of a series of coordinated and controlled activities in a certain \nprocess. The objective of a project is to meet specific requirements and is \nrestricted by time and resources. Project management covers the project process \nand results to achieve project objectives. Kanban project is a special type. Kanban \ndepends on projects. It displays work items, their levels, and their types.\n\u2022\nProfessional agile project management: agile-based project set management, \nsingle-project Scrum, and lean Kanban\n\u2022\nProfessional product planning: Gantt charts, mind maps, and overall product \nplans\n\u2022\nMulti-dimensional and professional reports: multi-project Kanban, dashboard, \nand reports\n\u2022\nR&D knowledge management: Structured knowledge and accumulated \ninnovations.\n\u2022\nTrusted audit logs: 1000+ audit events, comprehensive tracing, and high security \nand reliability\n\u2022\nTypical scenario:\n\u25ab\nCollaborative operations of product, development, and test personnel\n\u25ab\nRequirement management\n\u25ab\nProject health (progress, quality, risk, and personnel) management\n\u25ab\nDefect management\n"}, {"id": 315, "text": "\u2022\nAccess security control: CodeHub provide authentication tools such as branch \nprotection and IP address whitelist to ensure that only accounts with specific \npermissions and IP addresses can access code repositories.\n\u2022\nRemote backup: Authorized users can back up repositories to other regions, \nphysical hosts, and cloud hosts on Huawei Cloud by one click.\n\u2022\nRepository locking: You can manually lock a repository to disable any changes or \ncommits, preventing the stable version to be released from being compromised.\n\u2022\nSSH deployment key: Use the SSH key to control read and write permissions of a \nrepository. Use the deployment key to enable the read-only permission of a \nrepository.\n\u2022\nMisoperation tracing and recovery: Code and branches that are deleted by \nmistake can be accurately rolled back or retrieved. For deleted repositories, \nbackups are kept in the physical storage for a specific retention period.\n\u2022\nOperation logs: All operations have tokens. Key operations are audited and \nrecorded.\n\u2022\nRule setting: CodeHub allows you to configure commit rules, merge requests, and \ngates to ensure that the code quality is controllable.\n\u2022\nNotification setting: When an important change occurs in a repository, a \nnotification such as an email or SMS message can be sent to the preset role.\n"}, {"id": 316, "text": "\u2022\nIn-house development\n\u25ab\nHuawei-developed cross-process code check engine based on the syntax\ntree and context-free grammar (CFG), supporting code check in 10 \nlanguages, such as C, C++, Java, Python, and Go.\n\u2022\nHigh-quality code check rule set based on Huawei's 30-year R&D experience\n\u25ab\n3000+ code check rules and 20+ scenarios, covering programming\nstyles/coding security/memory management/input verification/unsafe\nfunctions/thread synchronization/code repetition rate.\n\u25ab\nCompatible with more than 5 secure coding standards, such as \nCWE/OWASP TOP 10/SANS TOP 25/MISRA/CERT.\n\u2022\nAutomatic auxiliary defect fixing\n\u25ab\nCodeHub offers intelligent fix suggestions and fixes defects by automatic\ncode changes, improving fix efficiency.\n\u25ab\nProvides Java and C/C++ programming guidelines for defect fixing. Provides\nautomatic fixing of Go code.\n"}, {"id": 317, "text": "\u2022\nRecommended: ProjectMan, CodeHub, CodeCheck, CloudBuild, CloudDeploy, \nCloudTest, CloudArtifact\n"}, {"id": 318, "text": "\u2022\nServiceStage provides application hosting, monitoring, alarms, and log analysis \nfor enterprise developers, test personnel, O&M personnel, and project managers. \nThe platform is compatible with mainstream application technology stacks, \nincluding multiple languages, microservice frameworks, and running \nenvironments in the industry. It helps enterprises improve the management and \nO&M efficiency of traditional, web, and microservice applications, focus on \nindustry-oriented application innovation, and improve enterprise competitiveness.\n\u2022\nSpring Cloud: mainstream open-source microservice development framework in \nthe industry.\n\u2022\nspring-cloud-huawei: Spring Cloud applications can be hosted on Huawei Cloud \nusing spring-cloud-huawei.\n\u2022\nServiceComb: open-source microservice framework contributed by Huawei to \nApache.\n"}, {"id": 319, "text": "\u2022\nServiceStage combines basic resources (such as CCE and ECS) and optional \nresources (such as ELB, RDS, and DCS) in the same VPC into an environment, \nsuch as a development environment, testing environment, pre-production \nenvironment, or production environment. The resources within an environment \ncan be networked together. Managing resources and deploying services by \nenvironment simplifies O&M.\n\u2022\nDubbo is an open-source, high-performance, and lightweight Java RPC service \nframework developed by Alibaba. It can be seamlessly integrated with the Spring \nframework.\n"}, {"id": 320, "text": "\u2022\nServiceStage:\n\u25ab\nGraphically displays application monitoring metrics in real time, including \nCPU usage, alarms, node exceptions, run logs, and key events.\n\u25ab\nSupports microservice API-level SLA metrics (throughput, latency, and \nsuccess rate) monitoring and governance in real time (in seconds), ensuring \ncontinuous service running.\n"}, {"id": 321, "text": "\u2022\nSolution value:\n\u25ab\nApplication hosting: Full-lifecycle hosting of traditional, web, and \nmicroservice applications is supported, supporting dark launch and scaling \nof applications.\n\u25ab\nApplication monitoring: Application running status can be observed, \nmonitored, and controlled, ensuring easy O&M.\n\u25ab\nApplication alarms: Alarm information is delivered through multiple \nchannels in real time so enterprises can respond to system faults as quickly \nas possible.\n\u25ab\nApplication logs: A massive number of logs are stored, supporting second-\nlevel search and facilitating fault locating and operations analysis.\n"}, {"id": 322, "text": "\u2022\nServiceStage:\n\u2022\nInterconnects with source code repositories, such as DevCloud, GitHub, Gitee, \nGitLab, and Bitbucket. After it is bound, you can directly pull up the source code \nfrom source code repositories for building.\n\u2022\nIntegrates the software center and archives the built software packages (or \nimage packages) to the corresponding repositories and organizations.\n\u2022\nIntegrates related infrastructure, such as VPC, CCE, ECS, EIP, and ELB. When \ndeploying applications, you can directly use existing or new infrastructures.\n\u2022\nIntegrates the Cloud Service Engine (CSE). You can perform operations related to \nmicroservice governance on the ServiceStage console.\n\u2022\nIntegrates Application Operations Management (AOM) and Application \nPerformance Management (APM) services. You can perform operations related to \napplication O&M and performance monitoring.\n\u2022\nIntegrates storage, database, and cache services and implements persistent data \nstorage through simple configuration.\n"}, {"id": 323, "text": "\u2022\nEver growing services may encounter various unexpected situations, such as \ninstantaneous and large-scale concurrent access, service errors, and intrusion. The \nmicroservice architecture implements fine-grained service management and \ncontrol to meet service requirements.\n\u2022\nServiceStage provides superior microservice application solutions and has the \nfollowing advantages:\n\u25ab\nSupports multiple microservice frameworks, such as native ServiceComb, \nSpring Cloud, Dubbo, and Service Mesh, and supports the dual-stack mode \n(SDK and Service Mesh interconnection). The service code can be directly \nmanaged on the cloud without modification.\n\u25ab\nSupports API management based on Swagger.\n\u25ab\nSupports multiple languages, such as Java, Go, .Node.js, PHP, and Python.\n\u25ab\nProvides functions such as service center, configuration center, dashboard, \nand dark launch.\n\u25ab\nProvides complete microservice governance policies, including fault \ntolerance, rate limiting, service degradation, circuit breaker, fault injection, \nand blacklist and whitelist. GUI-based operations can be performed in \ndifferent service scenarios, greatly improving the availability of service \ngovernance.\n"}, {"id": 324, "text": "\u2022\nAnswer 1: False. The microservice architecture features decoupling and DevOps.\n\u2022\nAnswer 2: ABCD\n"}, {"id": 325, "text": "\u2022\nDiscussion 1: Discuss the architecture, development, release, and O&M.\n\u2022\nDiscussion 2: Discuss the advantages of microservices, precautions for \ncloudification, and O&M management.\n"}, {"id": 326, "text": "\u2022\nO&M personnel have to master professional skills, make complicated \nconfigurations, and maintain multiple systems.\n\u2022\nMetrics cannot be associated for analysis. Therefore, O&M personnel need to \ncheck metrics one by one based on their experience.\n\u2022\nDistributed tracing systems are complicated, expensive, and unstable.\n"}, {"id": 327, "text": "\u2022\nThe IT architecture becomes more and more complex, and there are obvious \ndifferences between cloud O&M and traditional IT O&M. O&M personnel face \nmany challenges.\n\u2022\nMany enterprises opt to have development and O&M departments with different \ngoals. However, department miscommunication may hinder projects and lower \nefficiency. Therefore, the entire system architecture needs to evolve continuously, \nmoving from traditional O&M to automated O&M. This will help break down the \nbarriers between O&M engineers, development engineers, and quality assurance \nengineers, and form an efficient work system.\n"}, {"id": 328, "text": "\u2022\nTo help users focus on service O&M and reduce the workload in routine platform \nmaintenance, Huawei is responsible for platform O&M and provides users with a \nstable and reliable cloud platform.\n\u2022\nConsole is a visualized entry for cloud resource users to manage and provision \nresources.\n\u2022\nCloud Eye, Application Operations Management (AOM), and Application \nPerformance Management (APM) are multi-dimensional monitoring platforms \nthat allow users to monitor cloud resource usage and service status, set alarm \nrules, quickly respond to exceptions, thereby ensuring smooth service running.\n\u2022\nUsers can the cloud O&M service console and tools to support service O&M.\n"}, {"id": 329, "text": "\u2022\nWith the popularization of microservices, the relationship between applications is \nincreasingly complex. O&M personnel cannot handle it anymore. Professional \ntools are required to comprehensively monitor application calls, and display \nservice execution traces and statuses, thereby helping users quickly demarcate \nperformance bottlenecks and faults.\n\u2022\nAfter applications are migrated to the cloud, users still want microservice \ndependency visualization, better end user experience, fast problem tracing, \nassociation analysis on scattered logs. To meet these requirements, Huawei Cloud \nprovides diverse O&M services to improve O&M efficiency.\n"}, {"id": 330, "text": "\u2022\nHuawei Cloud launched a dimensional cloud application O&M solution that \nintegrates AOM and APM. This solution monitors infrastructure, applications, and \nservices in real time, and supports association analysis of application and \nresource alarms, log analysis, intelligent threshold, distributed tracing, and \nmobile app exception analysis, enabling users to quickly diagnose and rectify \nfaults within minutes, and ensure stable application running.\n\u25ab\nResource monitoring: AOM monitors applications and cloud resources in \nreal time, collects metrics, logs, and events to analyze application health \nstatus, and supports alarm reporting and data visualization.\n\u25ab\nLog management: LTS provides log collection, real-time query, and storage, \nhelping users easily cope with routine O&M.\n\u25ab\nLocating of performance problems: APM provides professional distributed \napplication performance analysis capabilities, enabling O&M personnel to \nquickly locate problems and resolve performance bottlenecks in a \ndistributed architecture.\n"}, {"id": 331, "text": "\u2022\nPrometheus is an open source monitoring tool. It is derived from Google's \nborgmon monitoring system, which was created by former Google employees \nworking at SoundCloud in 2012. Prometheus was developed as an open source \ncommunity project and officially released in 2015. In 2016, Prometheus officially \njoined the Cloud Native Computing Foundation, after Kubernetes.\n\u2022\nAs a key part of observability practices (monitoring, logging, and tracing), \nmonitoring has changed a lot in the cloud native era compared with previous \nsystem monitoring. Microservice and containerization lead to the exponential \nincrease of monitoring objects and metrics. Short lifecycles of monitoring objects \ngreatly increase monitoring data volumes and complexity.\n\u2022\nTherefore, Prometheus is developed to unify monitoring metrics and data query \nlanguages. Prometheus can be easily integrated with many open source tools to \nmonitor systems and services. It also analyzes vast volumes of data, facilitating \nsystem optimization and decision-making. It can be used in any scenarios where \nmetrics need to be collected.\n\u2022\nPromQL is a query language for labled time series data. It is totally different \nfrom the SQL query statements for relational databases.\n\u2022\nPrometheus is not only a time series database. It provides functions of integrated \ntools in the entire ecosystem.\n\u2022\nPrometheus is mainly used to monitor infrastructures, including servers (such as \nCPU and memory), databases (such as MySQL and PostgreSQL), "}, {"id": 332, "text": " functions of integrated \ntools in the entire ecosystem.\n\u2022\nPrometheus is mainly used to monitor infrastructures, including servers (such as \nCPU and memory), databases (such as MySQL and PostgreSQL), and web \nservices. It pulls data based on the configuration and connection with data \nsources.\n"}, {"id": 333, "text": "\u2022\nPrometheus is designed for reliability and allows users to quickly diagnose \nproblems. Each Prometheus server is standalone, not depending on network \nstorage or other remote services.\n\u2022\nPrometheus pulls data from exporters or through a gateway. (If it is deployed in \nKubernetes, service discovery can be used). It stores scraped data locally, runs \nrules to cleanse and sort data, and stores processed data in new time series.\n\u2022\nPrometheus components:\n\u25ab\nThe Prometheus server periodically scrapes data from targets via service \ndiscovery or static configuration.\n\u25ab\nWhen the size of the newly scraped data is larger than the configured \ncache, the data is persisted to disks. (If remote storage is used, the data will \nbe persisted to the cloud).\n\u25ab\nPrometheus periodically queries data. When conditions are met, \nPrometheus pushes alerts to the configured Alertmanager.\n\u25ab\nWhen receiving an alert, the Alertmanager performs aggregation, \ndeduplication, and noise reduction based on the configuration, and then \nsends the alert.\n\u25ab\nAPIs, the Prometheus console, or Grafana can be used to query and \naggregate data.\n\u2022\nData can be pulled by and pushed to Prometheus.\n\u25ab\nPull: Existing exporters are installed on the client and run as a daemon \nprocess. Exporters collect data, respond to HTTP requests, and return \nmetrics. \n\u25ab\nPush: The client (or server) with the official pushgateway plug-in installed \ncan organize monitoring data into metrics and send them to the \npushgateway using a script. Then, "}, {"id": 334, "text": "and return \nmetrics. \n\u25ab\nPush: The client (or server) with the official pushgateway plug-in installed \ncan organize monitoring data into metrics and send them to the \npushgateway using a script. Then, the pushgateway pushes the metrics to \nPrometheus as an intermediary forwarding medium.\n"}, {"id": 335, "text": "\u2022\nIt has the following six features:\n\u25ab\nDisplay mode: It provides fast and flexible visualization and supports \nextensive dashboard plug-ins, such as heatmaps and line charts.\n\u25ab\nData sources: It supports diverse data sources, such as Graphite, InfluxDB, \nOpenTSDB, Prometheus, Elasticsearch, CloudWatch, and KairosDB.\n\u25ab\nNotifications: Rules are defined based on different metrics to determine \nwhether to trigger an alarm and send a notification.\n\u25ab\nTransformation: Different data sources can be used in the same chart. Data \nsources can be specified based on each query or even customized.\n\u25ab\nAnnotations: Users can annotate graphs with rich events from different \ndata sources and hover over events to show full event metadata and tags.\n\u25ab\nFilters: Ad hoc filters allow users to add key/value filters that are \nautomatically added to all metric queries that use the specified data source.\n\u2022\nA TSDB is a database optimized for time-stamped or time series data. It is built \nspecifically for handling measurements and events that are time-stamped. Time \nseries data can be measurements or events that are tracked, monitored, \ndownsampled, and aggregated over time. It includes server metrics, application \nperformance, network data, sensor data, and many other types of analytics data.\n\u2022\nGrafana components:\n\u25ab\nfilebeat: collects Fault Tracing & Diagnosing System (FTDS) data. \nmetricbeat: collects system resource data. logstash: cleanses logs. influxdb: \ndistributed time series database. grafana: displ"}, {"id": 336, "text": "ts:\n\u25ab\nfilebeat: collects Fault Tracing & Diagnosing System (FTDS) data. \nmetricbeat: collects system resource data. logstash: cleanses logs. influxdb: \ndistributed time series database. grafana: displays data.\n"}, {"id": 337, "text": "\u2022\nFluentd_exporter collects and transfers logs.\n\u2022\nNode_exporter collects host data.\n"}, {"id": 338, "text": "\u2022\nPrometheus is used to monitor Kubernetes clusters, including:\n\u25ab\nNode metrics, such as CPUs, load, fdisk, and memory.\n\u25ab\nStatus of internal components, such as kube-scheduler, kube-controller-\nmanager, and kubedns or coredns.\n\u25ab\nApplication metrics, such as the Deployment status, resource requests, \nscheduling, and API latency.\n"}, {"id": 339, "text": "\u2022\nCloud Eye provides the following functions:\n\u25ab\nAutomatic monitoring: Cloud Eye automatically starts after resources such \nas ECSs are created. On Cloud Eye console, Users can view the resource \nstatus and create alarm rules.\n\u25ab\nServer monitoring: After installing the Agent on an ECS or Bare Metal \nServer (BMS), users can collect minute-level ECS or BMS monitoring data in \nreal time.\n\u25ab\nFlexible alarm rule configuration: Users can create alarm rules for multiple \nresources at the same time. After an alarm rule is created, users can flexibly \nmanage it, for example at any time users can modify, enable, disable, or \ndelete it.\n\u25ab\nReal-time notification: Users can enable Alarm Notification when creating \nalarm rules. When the cloud service status changes and the monitoring \ndata of the metric reaches the threshold specified in an alarm rule, Cloud \nEye notifies users by sending messages, emails, or HTTP or HTTPS requests \nto server IP addresses. In this way, users can monitor the cloud resource \nstatus and changes in real time.\n\u25ab\nMonitoring panel: allows users to view cross-service and cross-dimension \nmonitoring data on a monitoring panel and centrally displays metrics of key \nservices that users care about. This not only provides an overview of the \nstatus of cloud services, but also allows users to view monitoring details \nduring troubleshooting.\n\u25ab\nMonitoring data transfer to OBS: The retention period of raw data of each \nmetric is two days. After the retention period expires, t"}, {"id": 340, "text": " allows users to view monitoring details \nduring troubleshooting.\n\u25ab\nMonitoring data transfer to OBS: The retention period of raw data of each \nmetric is two days. After the retention period expires, the raw data will not \nbe saved. Users can dump raw data to OBS buckets for longer storage.\n"}, {"id": 341, "text": "Server monitoring:\n\u25ab\nServer monitoring provides more than 40 metrics, such as metrics for CPU, \nmemory, disk, and network, to meet the basic monitoring and O&M \nrequirements for servers.\n\u25ab\nAfter the Agent is installed, data of Agent-related metrics is reported once \na minute.\n\u25ab\nCPU usage, memory usage, and the number of opened files used by active \nprocesses give users a better understanding of the ECS or BMS usages.\n\u2022\nBasic monitoring covers metrics automatically reported by ECSs. The data is \ncollected every 5 minutes.\n\u2022\nOS monitoring provides proactive and fine-grained OS monitoring for ECSs or \nBMSs, and it requires the Agent to be installed on all servers that will be \nmonitored. The data is collected every minute. OS monitoring supports metrics \nsuch as CPU usage and memory usage (Linux).\n\u2022\nProcess monitoring is used to monitor active processes on servers. By default, \nCloud Eye collects the CPU usage, memory usage, and number of opened files of \nactive processes.\n"}, {"id": 342, "text": "\u2022\nThe differences between custom event monitoring and custom monitoring are as \nfollows:\n\u25ab\nMonitoring of custom events is used to report and query monitoring data \nfor non-consecutive events, and generate alarms in these scenarios.\n\u25ab\nCustom monitoring is used to report and query periodically and \ncontinuously collected monitoring data, and generate alarms in these \nscenarios.\n"}, {"id": 343, "text": "\u2022\nAlarm rules can be created for all monitoring items of Cloud Eye.\n\u2022\nUsers can configure the effective time of alarm rules.\n\u2022\nNotifications can be reported by multiple methods, such as email, SMS, HTTP, or \nHTTPS.\n\u2022\nService invoking based on alarm rules are supported. For example, when a \ncertain type of alarm is triggered, other cloud services (such as FunctionGraph) \ncan be triggered to perform configured operations.\n"}, {"id": 344, "text": "\u2022\nDashboards allow users to compare performance data of different services from \ndifferent dimensions. Users must create a dashboard before adding graphs.\n"}, {"id": 345, "text": "\u2022\nE-commerce services feature large data volume and large data access, which \nrequires large memory, fast data exchange and processing, and extremely strict \nmonitoring.\n\u2022\nECS is a core service in e-commerce scenarios. Therefore, a comprehensive and \nthree-dimensional ECS monitoring system plays an important role in service \nstability. Proactive fine-grained server monitoring of Cloud Eye helps ensure that \ne-commerce services run smoothly.\n\u2022\nPeople access the websites of e-commerce platforms and make transactions. \nDuring grand annual shopping festivals, the websites are often hit by various \nproblems like slow page loading and long network latency when people access \nfrom different networks. Website monitoring can perform continuous dialing \ntests on websites or ECS elastic IP addresses (EIPs) to monitor the availability and \nresponse time of the websites.\n\u2022\nFor services used by an e-commerce platform, such as Relational Database \nService (RDS), Elastic Load Balance (ELB), and Virtual Private Cloud (VPC), cloud \nservice monitoring allows users to track the status of each cloud service and \nusage of each metric. After setting alarm rules for cloud service metrics, users can \nget a more accurate picture of the health of cloud services.\n\u2022\nAn e-commerce platform involves many Huawei Cloud services, such as ECS, \nContent Delivery Network (CDN), AS, Web Application Firewall (WAF), RDS, ELB, \nand Object Storage Service (OBS). With resource groups, users can view resource \nusages, "}, {"id": 346, "text": "i Cloud services, such as ECS, \nContent Delivery Network (CDN), AS, Web Application Firewall (WAF), RDS, ELB, \nand Object Storage Service (OBS). With resource groups, users can view resource \nusages, alarms, and health status and manage alarm rules, relating to a specific \nservice. This greatly reduces O&M complexity and improves O&M efficiency.\n"}, {"id": 347, "text": "\u2022\nLog auditing is the core of information security audit. They are essential for the \nsecurity risk control of information systems in both private and public sectors.\n\u2022\nCTS directly connects to other Huawei Cloud services, records operations on cloud \nresources and the results, and transfers these records in the form of trace files to \nOBS buckets in real time.\n\u2022\nCTS provides the following functions:\n\u25ab\nTrace recording of operations performed, including system-triggered \noperations, operations on the management console, and API-calling \noperations. \n\u25ab\nTrace query on the CTS console from the last seven days by multiple \ndimensions: trace type, trace source, resource type, filter, operator, trace \nstatus. \n\u25ab\nTrace transfer to OBS buckets periodically for later query to meet \ncompliance and persistent storage requirements.\n\u25ab\nTrace file encryption using keys provided by the Data Encryption Workshop \n(DEW) during the transfer.\n"}, {"id": 348, "text": "\u2022\nA trace file is a collection of traces. CTS generates trace files based on services \nand transfer cycle and send these files to the specified OBS bucket in real time. In \nmost cases, all traces of a service generated in a transfer cycle are compressed \ninto one trace file. However, if there are a large number of traces, CTS will adjust \nthe number of traces contained in each trace file. Trace files are in JSON format.\n"}, {"id": 349, "text": "\u2022\nManagement trackers record operations on all cloud resources, such as creation, \nlogin, and deletion.\n\u2022\nData trackers record operations on data, such as upload and download.\n"}, {"id": 350, "text": "\u2022\nCompliance audit:\n\u25ab\nUsers need to ensure the compliance of their own service systems, and the \ncloud vendors they choose need to ensure the compliance of users' service \nsystems and resources.\n\u2022\nKey event notifications:\n\u25ab\nUsers can configure HTTP or HTTPS notifications targeted at their \nindependent audit systems and synchronize CTS logs to these systems for \nauditing.\n\u25ab\nUsers can also select a certain type of logs (such as file upload) as a trigger \nfor a preset workflow (for example, file format conversion) in \nFunctionGraph, simplifying service deployment and O&M as well as \npreventing risks.\n\u2022\nData mining:\n\u25ab\nA trace contains up to 24 fields, recording when an operation was \nperformed by a specific user on a specific resource and the IP address from \nwhich the operation was performed.\n\u2022\nFault locating and analysis:\n\u25ab\nCTS provides the following search dimensions: trace type, trace source, \nresource type, filter, operator and trace status. Each trace contains the \nrequest and response of an operation. Querying traces is one of the most \nefficient methods for locating a fault.\n"}, {"id": 351, "text": "\u2022\nReal-time log collection: LTS collects logs from hosts and cloud services in real \ntime and displays them on the LTS console in an intuitive and orderly manner. \nUsers can query logs or transfer logs for long-term storage.\n\u2022\nLog query and real-time analysis: Collected logs can be quickly queried by \nkeyword or fuzzy match. Users can analyze logs in real time to perform security \ndiagnosis and analysis, or obtain operations statistics, such as cloud service visits \nand clicks.\n\u2022\nLog monitoring and alarm reporting: LTS works with Application Operations \nManagement (AOM) to count the frequency of specified keywords in logs \nretained in LTS. For example, if the keyword ERROR occurs frequently, it can \nindicate that services are not running normally.\n\u2022\nLog transfer: Logs of hosts and cloud services are retained in LTS for seven days \nby default. Users can also set the retention duration to a value ranging from 1 to \n30 days. Retained logs are deleted once the duration is over. For long-term \nstorage, users can transfer logs to OBS and Data Ingestion Service (DIS).\n\u2022\nA dashboard is composed of multiple charts and allows users to view SQL \nanalysis results of logs in real time.\n"}, {"id": 352, "text": "\u2022\nLog groups can be created in two ways. They are either automatically created \nwhen other Huawei Cloud services are connected to LTS, or manually created by \nusers on the LTS console.\n\u2022\nUsers can configure logs of different types, such as operation logs and access \nlogs, to be written into different log streams. ICAgent will package and send the \ncollected logs to LTS by log stream. In this way, users can quickly find the target \nlogs in the corresponding log streams. The use of log streams greatly reduces the \nnumber of log reads and writes and improves efficiency.\n\u2022\nIf ICAgent has been installed on the host for other cloud services, skip the \ninstallation. The time and time zone of the local browser must be consistent with \nthose of the host before the installation. Users can install ICAgent on the Host \nManagement page of the LTS console. When ICAgent is installed, users need to \nconfigure log collection paths, which are paths of the host logs to be collected.\n\u2022\nDuring log structuring, logs with fixed or similar formats are extracted from a log \nstream based on the defined structuring method and irrelevant logs are filtered \nout. Users can then use SQL syntax to query and analyze the structured logs.\n"}, {"id": 353, "text": "\u2022\nCollected logs can be quickly queried by keyword or fuzzy match. Users can \nanalyze logs in real time to perform security diagnosis and analysis, or obtain \noperations statistics, such as cloud service visits and clicks.\n"}, {"id": 354, "text": "\u2022\nLog transfer:\n\u25ab\nLogs can only be transferred to OBS buckets that are deployed in the same \nregion as LTS.\n\u25ab\nLogs cannot be written to an encrypted OBS bucket.\n"}, {"id": 355, "text": "\u2022\nLog collection and analysis:\n\u25ab\nLogs of hosts and cloud services are difficult to query and will be cleared \nregularly. LTS collects logs for unified management and displays them on \nthe LTS console in an intuitive and orderly manner for fast query. LTS also \nsupports long-term log storage. Collected logs can be quickly queried by \nkeyword or fuzzy match. Users can analyze logs in real time to perform \nsecurity diagnosis and analysis, or obtain operations statistics, such as cloud \nservice visits and clicks.\n\u2022\nService optimization:\n\u25ab\nThe performance and quality of website services play an important role in \ncustomer satisfaction. By analyzing the network congestion logs, users can \nidentify performance bottlenecks, and take measures such as improving \nwebsite caching policies or network transmission policies to improve \nperformance.\n\u2022\nNetwork troubleshooting:\n\u25ab\nNetwork quality is the cornerstone of service stability. LTS centralizes logs \nfrom different sources, helping users quickly detect and locate faults, \nbacktrack easily. For example, users can quickly locate a problematic ECS \nthat is using too much bandwidth. Users can also judge whether there are \nongoing attacks, unauthorized hot-linking, or malicious access requests by \nanalyzing access logs, and locate and rectify faults as soon as possible.\n"}, {"id": 356, "text": "\u2022\nWith the popularization of container technologies, more and more enterprises \ndevelop applications using microservice frameworks. As the number of cloud \nservices increases, enterprises gradually turn to cloud O&M. However, they face \nthe following O&M challenges:\n\u2022\nO&M personnel have to master professional skills, make complicated \nconfiguration, and maintain multiple systems at the same time. Distributed \ntracing systems are complicated, expensive, and unstable.\n\u2022\nDistributed applications face analysis difficulties such as how to visualize the \ndependency between microservices, improve user experience, associate scattered \nlogs for analysis, and quickly trace problems.\n\u2022\nAdvantages of AOM:\n\u25ab\nManagement of massive quantities of logs: High-performance search and \nservice analysis are supported. Logs are automatically associated and can \nbe filtered by application, host, file, or instance.\n\u25ab\nAssociation analysis: AOM finds correlations between metrics and alarm \ndata from applications, components, instances, hosts, and transactions, \nallowing users to quickly locate faults.\n\u25ab\nOpen ecosystem: AOM opens O&M data query APIs and collection \nstandards, and supports independent development.\n"}, {"id": 357, "text": "\u2022\nData collection and access layer:\n\u25ab\nICAgent-based data collection: ICAgent plug-ins are installed on hosts to \nreport O&M data.\n\u25ab\nAPI-based data collection: Custom metrics can be connected to AOM by \nusing open APIs or Exporter APIs.\n\u2022\nTransmission and storage layer:\n\u25ab\nData transmission: AOM Access is a proxy for receiving O&M data. Received \ndata will be placed in a Kafka queue. Kafka then transmits the data to the \nservice computing layer in real time using its high-throughput capability.\n\u25ab\nData storage: After being processed by the AOM backend, O&M data is \nwritten into a database. Cassandra stores time series data, Redis is used for \ncache query, etcd stores AOM configuration data, and Elasticsearch stores \nresources, logs, alarms, and events.\n\u2022\nService computing layer:\n\u25ab\nAOM provides basic O&M services such as alarm reporting, logging, and \nmetric monitoring, and AI services such as exception detection and analysis.\n"}, {"id": 358, "text": "\u2022\nAs cloud migration becomes popular, enterprises are facing the challenge of \nmanaging diverse resources from different cloud vendors. Configuration \nmanagement database (CMDB) is a DevOps-based resource management \nplatform for the entire application lifecycle. As a fundamental service for \nautomated O&M, it centrally manages the relationships between applications \nand resource objects of Huawei Cloud as well as other cloud vendors.\n\u2022\nCMDB functions:\n\u25ab\nResource search: Users can search for resources (such as applications and \nhosts) by ID, keyword, or name.\n\u25ab\nApplication management: CMDB manages the relationships between cloud \nservices and applications (especially those running on ECS, CCE, and RDS).\n\u25ab\nResource management: CMDB manages all cloud services of users in a \nunified manner. Users can view the relationships between applications and \nall cloud service resource objects (including those that have not been bound \nto applications) for resource analysis and management.\n\u25ab\nEnvironment tags: Users can add tags to application environments to filter \nenvironments with the same attribute.\n"}, {"id": 359, "text": "\u2022\nApplication monitoring adopts the hierarchical drill-down design. The hierarchy is \nas follows: Application list > Application details > Component details > Instance \ndetails > Container details > Process details. That is, applications, components, \ninstances, containers, and processes are associated and their relationships are \ndirectly displayed on the console.\n"}, {"id": 360, "text": "\u2022\nThe alarm center enables users to manage alarms and events. It supports custom \nnotification actions, allowing users to obtain alarm information by email or SMS \nmessage. In this way, users can detect and handle exceptions at the earliest time. \nBefore using the alarm management function, ensure that the ICAgent has been \ninstalled on the host.\n\u2022\nWith a dashboard, different graphs can be displayed on the same screen. Various \ngraphs, such as line graphs, digital graphs, and top N resource graphs allow users \nto comprehensively monitor resource data.\n\u2022\nLog search enables users to quickly search for required logs from massive \nquantities of logs. Log dump enables users to store logs for a long period of time. \nAfter users create statistical rules, AOM can periodically count keywords in logs \nand generate metric data, so that users can monitor system performance and \nservices in real time. By configuring delimiters, users can divide log content into \nmultiple words and use these words to search for logs.\n"}, {"id": 361, "text": "\u2022\nIn the cloud era, more and more applications are deployed in the distributed \nmicroservice architecture. As the number of users increases rapidly, many \napplication exceptions occur. In traditional O&M, metrics cannot be associated \nfor analysis, so they need manual and subjective processing. This results in low \nefficiency, high maintenance costs, and non-ideal performance.\n\u2022\nWhen there are massive quantities of services, O&M personnel face two major \nchallenges:\n\u25ab\nLarge distributed applications have complex relationships, making it \ndifficult to analyze and locate problems. O&M personnel face problems \nsuch as how to ensure normal application running, and quickly locate faults \nand performance bottlenecks.\n\u25ab\nUsers choose to leave due to poor experience. O&M personnel fail to detect \nand track services with poor experience in real time, and cannot quickly \ndiagnose application exceptions, greatly affecting user experience.\n\u2022\nAPM helps O&M personnel quickly identify application performance bottlenecks \nand locate root causes of faults, ensuring experience.\n"}, {"id": 362, "text": "\u2022\nData collection: APM can collect data about applications, basic resources, and \nuser experience from Java probes and Istio mesh in non-intrusive mode.\n\u2022\nThere are two types of application topologies:\n\u25ab\nSingle-component topology: topology of a single component under an \nenvironment. Users can also view the call relationships of direct and indirect \nupstream and downstream components.\n\u25ab\nGlobal application topology: topology of some or all components under an \napplication.\n"}, {"id": 363, "text": "\u2022\nAPM probes inject the trace code into distributed transactions and performance \ninformation during class loading.\n\u2022\nAPM transactions are HTTP transactions. When a user purchases a mobile phone \nfrom VMALL, the user's PC sends an HTTP request to the VMALL backend. This HTTP \nrequest is an HTTP transaction. As the HTTP request URL is unique, it is used as the \ntransaction name. After a service (Java application) with a probe (pinpoint) deployed \nreceives an HTTP transaction, APM extracts the transaction information and displays \nit on the console.\n"}, {"id": 364, "text": "\u2022\nFull-link topology:\n\u25ab\nVisible topology: APM displays application call and dependency relationships \nin topologies. Application Performance Index (Apdex) is used to quantify \nuser satisfaction with application performance. Different colors indicate \ndifferent Apdex value ranges, helping users quickly detect and locate \nperformance problems.\n\u25ab\nInter-application calling: APM can display call relationships between \napplication services on the topology. When services are called across \napplications, APM can collect inter-application call relationships and display \napplication performance data.\n\u25ab\nSQL analysis: APM can count and display key metrics about databases or \nSQL statements on the topology. \n\u25ab\nJVM metric monitoring: APM can count and display JVM metric data of \ninstances on the topology. APM monitors the memory and thread metrics in \nthe JVM running environment in real time, enabling users to quickly detect \nmemory leakage and thread exceptions.\n\u2022\nTracing: APM comprehensively monitors calls and displays service execution traces \nand statuses, helping users quickly demarcate performance bottlenecks and \nfaults.\n\u25ab\nIn the displayed trace list, click the target trace to view its basic information.\n\u25ab\nOn the trace details page, users can view the trace's complete information, \nincluding the local method stack and remote call relationships.\n\u2022\nTransaction analysis: APM analyzes service flows on servers in real time, displays \nkey metrics (such as throughput, error rate, and laten"}, {"id": 365, "text": "uding the local method stack and remote call relationships.\n\u2022\nTransaction analysis: APM analyzes service flows on servers in real time, displays \nkey metrics (such as throughput, error rate, and latency) of transactions, and uses \nApdex to evaluate users' satisfaction with applications. If transactions are \nabnormal, alarms are reported. For transactions with poor user experience, locate \nproblems through topologies and tracing.\n"}, {"id": 366, "text": "\u2022\nAPM traces each service transaction in real time, quickly analyzes the transaction \nstatus, and diagnoses problems.\n\u25ab\nTransaction customization: Users can define transaction names based on \nURLs for better understanding.\n\u25ab\nHealth rule configuration: A health rule can be configured for each \ntransaction. If the threshold is exceeded, an error message is displayed.\n\u25ab\nPerformance tracing: APM accurately collects abnormal performance data \nand compares current data with historical baseline data to find application \nexception methods and improve O&M efficiency.\n"}, {"id": 367, "text": "\u2022\nApplication discovery and dependency: APM monitors application metric data in a \nnon-intrusive way and automatically generates dependencies through APIs \nbetween services.\n\u2022\nApplication metric aggregation: Key metrics of microservice instances are \nautomatically aggregated to applications.\n"}, {"id": 368, "text": "\u2022\nMulti-protocol and high-concurrency performance tests:\n\u25ab\nUsers can quickly define standard HTTP, HTTPS, TCP, or UDP packet \ncontents and simply adjust loads for different tested applications. CPTS \nallows users to define any fields in HTTP, HTTPS, TCP, or UDP packets \nbased on their requirements.\n\u25ab\nDifferent behaviors of virtual users defined for different test scenarios: The \nnumber of requests initiated by each user per second can be set by think \ntime, which is the interval for the same user to send, or by defining multiple \nrequest packets in a transaction.\n\u25ab\nCustomizing the response result verification provides more accurate \nstandards of successful requests. CPTS allows users to configure check \npoints for requests. After obtaining response packets, CPTS verifies their \nresponse code and header. Only response packets meeting the specified \nconditions are regarded as normal.\n\u2022\nTest task model customized for complex scenarios:\n\u25ab\nWith multiple flexible combinations of transaction elements and test task \nphases, CPTS helps users test application performance in scenarios with \ndifferent operation types and concurrent operations.\n\u25ab\nA transaction can be used by multiple test tasks, and multiple test phases \ncan be defined for a transaction. In each test phase, users can define the \ntest duration, number of concurrent users and tests, as well as simulate \ncomplex scenarios with different traffic peaks and troughs.\n\u2022\nTwo types of costs will be generated when users run performa"}, {"id": 369, "text": "he \ntest duration, number of concurrent users and tests, as well as simulate \ncomplex scenarios with different traffic peaks and troughs.\n\u2022\nTwo types of costs will be generated when users run performance tests in CPTS: \nthe cost for using CPTS and the cost for using resources of other cloud services, \nsuch as ECS. CPTS is billed by package on a pay-per-use or yearly/monthly basis.\n"}, {"id": 370, "text": "\u2022\nEngines for millions of concurrent users and capability of full-link bottleneck \nanalysis accelerate testing from weeks to just hours.\n"}, {"id": 371, "text": "\u2022\n1. False\n\u25ab\nLTS allows user to transfer logs to OBS and DIS.\n\u2022\n2. C\n\u25ab\nCloud Eye does not support performance tests.\n"}, {"id": 372, "text": "\u2022\nDiscussion 1: The differences lie in the sites, equipment, routine inspections, \ntroubleshooting, and software tools.\n\u2022\nDiscussion 2: Basic monitoring can be performed without Agents. Data is \ncollected every 5 minutes. With Agents installed, Cloud Eye can provide advanced \nmonitoring. For example, system-level, proactive, and fine-grained monitoring is \nprovided, and data is collected every minute. Host processes cannot be monitored \nunless Agents are installed.\n"}, {"id": 373, "text": "\u2022\nAs companies' digital construction gradually enters the intelligent upgrade phase, \ncompanies need to fully enjoy the dividends brought by cloud computing. The \nvalue of cloud to services is no longer simple resource provision, but also \napplication-centric for service enablement.\n\u2022\nDigital twins: Fully utilize the simulation process and completes mapping in the \nvirtual space to reflect the entire lifecycle of the corresponding physical \nequipment, effectively reducing the actual production cost.\n"}, {"id": 374, "text": "\u2022\nHuawei Cloud EI consists of big data and AI solutions.\n"}, {"id": 375, "text": "\u2022\nThe content in red will be further learned.\n\u2022\nHuawei Cloud DAYU:\n\u25ab\nIt is dedicated to transforming enterprise data from resources to assets. By \nimporting data from all domains into a lake, data can be transmitted across \nisolated systems, service awareness can be implemented, and data \nresources can be intelligently managed. Data value is mined from multiple \nperspectives, layers, and granularities to implement data-driven digital \ntransformation.\n\u2022\nHuawei Cloud AI-enabled ModelArts:\n\u25ab\nModelArts: a one-stop AI development platform for developers\n\u2022\nTraining framework:\n\u25ab\nMindSpore is an open-source AI framework developed by Huawei. It is a \ndeep learning training and inference framework that supports all-scenario \ndevice-edge-cloud scenarios and is mainly applied to AI fields such as \ncomputer vision and natural language processing.\n\u2022\nComputing power:\n\u25ab\nNPU: a new type of processor based on neural network algorithms and \nacceleration.\n"}, {"id": 376, "text": "\u2022\nMRS helps customers build a unified big data platform for data access, data \nstorage, data analysis, and value mining. Furthermore, it interconnects with \nHuawei Cloud IoT, ROMA platform, DLF, and DLV to help customers easily \nresolve difficulties in data channel cloudification, big data job development and \nscheduling, and data display.\n\u2022\nMRS provides different big data analysis and processing components for different \nscenarios. You can select stream computing components such as Flink for real-\ntime processing, and Spark or MapReduce for offline batch computing.\n\u2022\nCarbonData is a new local file format of Apache Hadoop. It uses advanced \ncolumn-based storage, indexing, compression, and encoding technologies to \nimprove computing efficiency and accelerate PB-level data query. Therefore, it \ncan be used for faster interaction query. CarbonData is also a high-performance \nanalysis engine that integrates data sources with Spark.\n"}, {"id": 377, "text": "\u2022\nAdvantages of MRS in massive data analysis scenarios (environmental protection \nindustry):\n\u25ab\nLow costs: Enjoy the cost-effective storage of OBS.\n\u25ab\nAnalysis of mass data: Analyze TB or PB of data with Hive.\n\u25ab\nVisualized data import and export tool: Use Loader to export data to Data \nWarehouse Service (DWS) for business intelligence (BI) analysis.\n\u2022\nAdvantages of MRS in massive data storage scenarios (IoV industry):\n\u25ab\nReal-time: With Kafka, you can access massive amounts of vehicle \nmessages in real time.\n\u25ab\nStorage of mass data: With HBase, you can store a large volume of data \nand query data in milliseconds.\n\u25ab\nDistributed data query: With Spark, you can analyze and query a large \nvolume of data.\n\u2022\nAdvantages of MRS in real-time data processing scenarios (elevator industry):\n\u25ab\nReal-time data ingestion: With Flume, you can achieve real-time data \ningestion and enjoy various data collection and storage access methods.\n\u25ab\nData source access: Use Kafka to access the data of tens of thousands of \nelevators and escalators in real time.\n"}, {"id": 378, "text": "\u2022\nWeather data can be stored in OBS and periodically dumped to HDFS for batch \nanalysis.\n"}, {"id": 379, "text": "\u2022\nDLI frees you from managing any servers. DLI supports standard SQL and is \ncompatible with standard SQL and Spark and Flink SQL. It also supports multiple \naccess modes and mainstream data formats. You can use SQL applications to \nquery mainstream data formats without data ETL. DLI supports SQL statements \nfor heterogeneous data sources, including CloudTable, RDS, DWS, CSS, OBS, \ncustom databases on ECSs, and offline databases.\n\u2022\nDLI is applicable to large-scale log analysis, federated analysis of heterogeneous \ndata sources, and big data ETL processing.\n"}, {"id": 380, "text": "\u2022\nDWS is a cloud-native service based on Huawei converged data warehouse \nGaussDB. Based on the shared-nothing distributed architecture, GaussDB(DWS) \nuses a massively parallel processing (MPP) engine and consists of multiple \nindependent logical nodes that do not share system resources, such as CPUs, \nmemory, and storage. In such an architecture, data is distributed on multiple \nnodes. Data analytics tasks can be quickly executed in parallel on the nodes \nwhere data is stored.\n\u2022\nDWS provides a web-based service management platform, that is, the \nmanagement console. You can also manage DWS clusters using HTTPS-based \nAPIs.\n\u2022\nDWS is often used together with Cloud Data Migration (CDM) and Data \nIngestion Service (DIS). CDM is used for batch data migration, and DIS is used for \nstream data ingestion.\n"}, {"id": 381, "text": "\u2022\nDataArts Migration: Based on the big data cloud migration and intelligent data \nlake solution, DataArts Migration provides easy-to-use migration capabilities and \ncan integrate a broad set of data sources into the data lake more easily and \nefficiently.\n\u2022\nDataArts Architecture can be used to create entity-relationship (ER) models and \ndimensional models to standardize and visualize data development and output \ndata governance methods that can guide development personnel to work with \nease.\n\u2022\nDataArts Factory is a one-stop collaborative big data development platform that \nprovides fully managed big data scheduling capabilities.\n\u2022\nDataArts Quality can monitor metrics and data quality, and screen out \nunqualified data in a timely manner.\n\u2022\nDataArts Catalog provides enterprise-class metadata management to clarify \ninformation assets. It uses a data map to display a data lineage and panorama of \ndata assets for intelligent data search, operations, and monitoring.\n\u2022\nDataArts DataService enables you to manage APIs centrally and control the \naccess to subjects, profiles, and metrics. It improves data access, query, and \nretrieval efficiency and data consumption experience, and monetizes data assets. \nIt also allows you to quickly generate new APIs based on data tables, register \nyour legacy APIs, and centrally manage and publish them.\n\u2022\nDataArts Security provides all-round security assurance to safeguard network \nsecurity and control user permissions. It provides a review mechanism"}, {"id": 382, "text": "egacy APIs, and centrally manage and publish them.\n\u2022\nDataArts Security provides all-round security assurance to safeguard network \nsecurity and control user permissions. It provides a review mechanism for key \nprocesses in DataArts Architecture and DataArts DataService. Data is managed \nby level and category throughout the lifecycle, ensuring data privacy compliance \nand traceability.\n"}, {"id": 383, "text": "\u2022\nThe long tail is a business strategy that allows companies to realize significant \nprofits by selling low volumes of hard-to-find items to many customers, instead \nof only selling large volumes of a reduced number of popular items.\n\u2022\nTo achieve digital transformation, medium- and long-tail enterprises need \nadvanced data technologies, professionals, and large amounts of capital \ninvestment. Therefore, they urgently need a universal model offered by a leader \nin the big data industry to reduce digitalization costs and lower the barrier to \ndata use.\n\u2022\nBased on Huawei's IT process data governance methodology, Huawei Cloud \nlaunched a lightweight big data solution. This Serverless solution uses Huawei \nCloud assets to enable quick data governance, requiring less resources and \ndevelopment, deployment, and O&M workloads. It frees medium- and long-tail \nenterprises from worrying about technology stacks and cloud resources, and \nallows them to use resources on demand, reducing operational costs.\n\u2022\nHuawei Cloud big data services provide one-stop management and development \nthroughout the entire data lifecycle and significantly simplify the data \ngovernance process for medium- and long-tail enterprises. With these services, \nmedium- and long-tail enterprises can analyze a large amount of data more \nquickly and efficiently, use data more easily, monetize data in a shorter time, and \ndigitize their business smoothly.\n"}, {"id": 384, "text": "\u2022\nAs big data has grown, there has been a corresponding growth in the power of \nAI. AI has been constantly changing methods of production and how we live.\n"}, {"id": 385, "text": "\u2022\nAI engineers face many challenges when they are installing and configuring \nvarious AI tools, preparing data, and training models. ModelArts, a one-stop AI \ndevelopment platform is designed to address these challenges. ModelArts\nintegrates data preparation, algorithm development, model training, and model \ndeployment into the production environment, allowing AI engineers to perform \none-stop AI development.\n\u2022\nModelArts supports the entire development process, including data processing, \nand model training, management, and deployment. It also includes AI Gallery, a \nplace where models can be shared.\n\u2022\nData processing: All data formats are supported, as well as team labeling.\n\u2022\nTraining: Pre-trained models accelerate the implementation of AI applications. \nHuawei-developed inference frameworks hide underlying hardware and software \ndifferences from the upper layer software to improve performance. Multi-vendor, \nmulti-framework, and multi-function models are centrally managed.\n\u2022\nDeployment: High-concurrency model deployment, low-latency access, auto \nscaling, grayscale release, and rolling upgrade are provided. Models can be \ndeployed in different production environments, for example, deployed as in-cloud \nreal-time or batch inference services, or on devices and edge devices.\n\u2022\nAI Gallery: Common algorithms are preconfigured in AI Gallery, and models can \nbe shared publicly or within an enterprise.\n"}, {"id": 386, "text": "\u2022\nARPU = Total revenue/Number of active users\n"}, {"id": 387, "text": "\u2022\nHuawei algorithm experts provide resources such as algorithm engines, SDKs, \nand APIs for gaming companies to call for training. Furthermore, models can be \ncustomized on Huawei Cloud to significantly improve algorithm development \nefficiency.\n"}, {"id": 388, "text": "\u2022\nAI Gallery is a developer ecosystem community built on ModelArts. In this \ncommunity, scientific research institutions, AI application developers, solution \nintegrators, enterprises, and individual developers can share and purchase \nalgorithms, models, and datasets. This accelerates the development and \nimplementation of AI assets and enables every participant to create business \nvalue in the AI development ecosystem.\n\u2022\nPangu models: There are multiple foundation models, including the NLP, CV, \nmulti-modal, and scientific computing models. Through model generalization, the \nPangu models enable large-scale industrialized AI that could not be supported in \ntraditional AI development. This enables brand-new industrial AI development.\n\u2022\nOptVerse AI solver: integrates AI with operations research to break through the \noptimization limit of operations research in the industry and find the optimal \nsolution for linear and integer models, helping enterprises make quantitative \ndecisions and refine their operations.\n"}, {"id": 389, "text": "\u2022\nGovernment Intelligent Twins, Traffic Intelligent Twins, EIHealth, GeoGenius, \nCampus Intelligent Twins, Water Intelligent Twins, Heating Intelligent Twins, \nIndustrial Intelligent Twins, Network Intelligent Twins\n"}, {"id": 390, "text": "\u2022\nDigital factory: By connecting production line devices to an IoT platform for real-\ntime monitoring, analysis, and alarm management, efficiency is improved and \npower saved.\n\u2022\nProduct design optimization: Enterprises can connect their products to the \nHuawei Cloud IoT platform to improve product design and provide personalized \nservices based on collected user and product data.\n"}, {"id": 391, "text": "\u2022\nWe need to connect everything, monetize data, and build an ecosystem to \ndevelop the IoT industry.\n"}, {"id": 392, "text": "\u2022\nAs there are mappings between boxes and RFID tags and mappings between \nboxes and warehouse gates, a large amount of RFID data is generated during the \ninbound and outbound processes. By leveraging the stream computing capability \nof Flink, IoTA can detect inbound and outbound goods under a gate in seconds. \nThen, the system checks goods against the goods list and informs warehouse \nstaff of the goods status in real time.\n"}, {"id": 393, "text": "\u2022\nFDI: If an enterprise and its partners use different data sources, information \ntransmission will be ineffective. FDI can convert multiple mainstream data \nformats, such as MySQL, Kafka, and APIs. It can also work with other services, \nsuch as Gauss200, to store, convert, and analyze big data.\n\u2022\nAPIC: If a corporate group integrates its IT system with those of its branches in \ndifferent regions, direct access to each other's databases can be very complex \nand cause information leaks. Open access through APIs and enhanced API call \nsecurity ensure collaboration across networks and regions.\n\u2022\nMQS: If an enterprise and its partners use different message systems, \ninterconnection between their message systems is costly, and message \ntransmission may not be reliable or secure. To address these issues, the Kafka \nprotocol can be used for communication between the enterprise and its \npartners, while MQS functions as a message transfer station to provide secure \nand reliable message transmission. The enterprise can create multiple topics, \nauthorize each partner to subscribe to these topics, and publish messages to \nthe topics. Then, partners can subscribe to these topics to obtain messages.\n\u2022\nLINK: In industrial scenarios, device information and production parameters are \nscattered. If a fault occurs in a production line, it requires a long time to \nmanually collect information and parameters from each device. LINK connects \ndevices to IT systems or big data platforms, and uploads"}, {"id": 394, "text": "f a fault occurs in a production line, it requires a long time to \nmanually collect information and parameters from each device. LINK connects \ndevices to IT systems or big data platforms, and uploads information such as \ndevice running status to these platforms so that enterprises can view \ninformation about all devices graphically and therefore quickly locate faults.\n"}, {"id": 395, "text": "\u2022\nUsing ROMA Connect, connections can be secure, reliable, and efficient for \nsafe cross-organization collaboration of APIs, data, and messages.\n\u2022\nROMA Connect provides hybrid integration capabilities to connect service \nsystems, devices, and heterogeneous data sources. Beyond that, developing \nnew applications costs half of the time by connecting IT and OT data through \nROMA Connect.\n\u2022\nROMA Connect provides API gateways and custom backends for simplified and \nquick API openness. Various data tables can be directly opened as RESTful APIs \nfor service systems to call.\n"}, {"id": 396, "text": "\u2022\n1. C\n\u2022\n2. ABC. ModelArts focuses on model deployment rather than application \ndeployment.\n"}]}